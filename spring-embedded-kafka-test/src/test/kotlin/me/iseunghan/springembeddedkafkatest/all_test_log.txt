> Task :checkKotlinGradlePluginConfigurationErrors
> Task :compileKotlin UP-TO-DATE
> Task :compileJava NO-SOURCE
> Task :processResources UP-TO-DATE
> Task :classes UP-TO-DATE
> Task :compileTestKotlin UP-TO-DATE
> Task :compileTestJava NO-SOURCE
> Task :processTestResources NO-SOURCE
> Task :testClasses UP-TO-DATE
4월 03, 2025 11:43:01 오후 org.junit.platform.launcher.core.LauncherConfigurationParameters loadClasspathResource
경고: Discovered 3 'junit-platform.properties' configuration files on the classpath (see below); only the first (*) will be used.
- jar:file:/Users/shlee/.gradle/caches/modules-2/files-2.1/org.apache.kafka/kafka_2.13/3.8.1/6278b0f00854c36ed12c8a016152332bc9d1a609/kafka_2.13-3.8.1-test.jar!/junit-platform.properties (*)
- jar:file:/Users/shlee/.gradle/caches/modules-2/files-2.1/org.apache.kafka/kafka-server-common/3.8.1/4ad5b80346a131afa11708238441f16026f0ec5c/kafka-server-common-3.8.1-test.jar!/junit-platform.properties
- jar:file:/Users/shlee/.gradle/caches/modules-2/files-2.1/org.apache.kafka/kafka-clients/3.8.1/11d3ffefbc452fc4c5d45f4f6ec368bcab290a95/kafka-clients-3.8.1-test.jar!/junit-platform.properties
4월 03, 2025 11:43:01 오후 org.junit.platform.launcher.core.LauncherConfigurationParameters loadClasspathResource
경고: Discovered 3 'junit-platform.properties' configuration files on the classpath (see below); only the first (*) will be used.
- jar:file:/Users/shlee/.gradle/caches/modules-2/files-2.1/org.apache.kafka/kafka_2.13/3.8.1/6278b0f00854c36ed12c8a016152332bc9d1a609/kafka_2.13-3.8.1-test.jar!/junit-platform.properties (*)
- jar:file:/Users/shlee/.gradle/caches/modules-2/files-2.1/org.apache.kafka/kafka-server-common/3.8.1/4ad5b80346a131afa11708238441f16026f0ec5c/kafka-server-common-3.8.1-test.jar!/junit-platform.properties
- jar:file:/Users/shlee/.gradle/caches/modules-2/files-2.1/org.apache.kafka/kafka-clients/3.8.1/11d3ffefbc452fc4c5d45f4f6ec368bcab290a95/kafka-clients-3.8.1-test.jar!/junit-platform.properties
23:43:01.855 [Test worker] INFO org.springframework.test.context.support.AnnotationConfigContextLoaderUtils -- Could not detect default configuration classes for test class [me.iseunghan.springembeddedkafkatest.test1.KafkaTest1]: KafkaTest1 does not declare any static, non-private, non-final, nested classes annotated with @Configuration.
23:43:01.901 [Test worker] INFO org.springframework.boot.test.context.SpringBootTestContextBootstrapper -- Found @SpringBootConfiguration me.iseunghan.springembeddedkafkatest.SpringEmbeddedKafkaTestApplication for test class me.iseunghan.springembeddedkafkatest.test1.KafkaTest1

  .   ____          _            __ _ _
 /\\ / ___'_ __ _ _(_)_ __  __ _ \ \ \ \
( ( )\___ | '_ | '_| | '_ \/ _` | \ \ \ \
 \\/  ___)| |_)| | | | | || (_| |  ) ) ) )
  '  |____| .__|_| |_|_| |_\__, | / / / /
 =========|_|==============|___/=/_/_/_/

 :: Spring Boot ::                (v3.4.4)

2025-04-03T23:43:02.084+09:00  INFO 45581 --- [    Test worker] m.i.s.test1.KafkaTest1                   : Starting KafkaTest1 using Java 17.0.12 with PID 45581 (started by shlee in /Users/shlee/workspaces/study/iseunghan-Lab/spring-embedded-kafka-test)
2025-04-03T23:43:02.084+09:00  INFO 45581 --- [    Test worker] m.i.s.test1.KafkaTest1                   : No active profile set, falling back to 1 default profile: "default"
2025-04-03T23:43:02.584+09:00  INFO 45581 --- [    Test worker] o.a.k.clients.consumer.ConsumerConfig    : ConsumerConfig values:
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-my-group-1
	client.rack =
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = my-group
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2025-04-03T23:43:02.604+09:00  INFO 45581 --- [    Test worker] o.a.k.c.t.i.KafkaMetricsCollector        : initializing Kafka metrics collector
2025-04-03T23:43:02.658+09:00  INFO 45581 --- [    Test worker] o.a.kafka.common.utils.AppInfoParser     : Kafka version: 3.8.1
2025-04-03T23:43:02.658+09:00  INFO 45581 --- [    Test worker] o.a.kafka.common.utils.AppInfoParser     : Kafka commitId: 70d6ff42debf7e17
2025-04-03T23:43:02.658+09:00  INFO 45581 --- [    Test worker] o.a.kafka.common.utils.AppInfoParser     : Kafka startTimeMs: 1743691382657
2025-04-03T23:43:02.660+09:00  INFO 45581 --- [    Test worker] o.a.k.c.c.internals.LegacyKafkaConsumer  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Subscribed to topic(s): my-topic
2025-04-03T23:43:02.668+09:00  INFO 45581 --- [    Test worker] m.i.s.test1.KafkaTest1                   : Started KafkaTest1 in 0.712 seconds (process running for 1.267)
2025-04-03T23:43:02.805+09:00  INFO 45581 --- [ntainer#0-0-C-1] org.apache.kafka.clients.Metadata        : [Consumer clientId=consumer-my-group-1, groupId=my-group] Cluster ID: eAJxFseLSyS13kVa5_dBNg
2025-04-03T23:43:02.806+09:00  INFO 45581 --- [ntainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Discovered group coordinator logs-service-kafka-1:9092 (id: 2147483646 rack: null)
2025-04-03T23:43:02.810+09:00  WARN 45581 --- [ntainer#0-0-C-1] org.apache.kafka.clients.NetworkClient   : [Consumer clientId=consumer-my-group-1, groupId=my-group] Error connecting to node logs-service-kafka-1:9092 (id: 2147483646 rack: null)

java.net.UnknownHostException: logs-service-kafka-1: nodename nor servname provided, or not known
	at java.base/java.net.Inet6AddressImpl.lookupAllHostAddr(Native Method) ~[na:na]
	at java.base/java.net.InetAddress$PlatformNameService.lookupAllHostAddr(InetAddress.java:934) ~[na:na]
	at java.base/java.net.InetAddress.getAddressesFromNameService(InetAddress.java:1543) ~[na:na]
	at java.base/java.net.InetAddress$NameServiceAddresses.get(InetAddress.java:852) ~[na:na]
	at java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1533) ~[na:na]
	at java.base/java.net.InetAddress.getAllByName(InetAddress.java:1385) ~[na:na]
	at java.base/java.net.InetAddress.getAllByName(InetAddress.java:1306) ~[na:na]
	at org.apache.kafka.clients.DefaultHostResolver.resolve(DefaultHostResolver.java:27) ~[kafka-clients-3.8.1.jar:na]
	at org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:124) ~[kafka-clients-3.8.1.jar:na]
	at org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.resolveAddresses(ClusterConnectionStates.java:536) ~[kafka-clients-3.8.1.jar:na]
	at org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:511) ~[kafka-clients-3.8.1.jar:na]
	at org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:466) ~[kafka-clients-3.8.1.jar:na]
	at org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:173) ~[kafka-clients-3.8.1.jar:na]
	at org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:1070) ~[kafka-clients-3.8.1.jar:na]
	at org.apache.kafka.clients.NetworkClient.ready(NetworkClient.java:320) ~[kafka-clients-3.8.1.jar:na]
	at org.apache.kafka.clients.NetworkClientUtils.tryConnect(NetworkClientUtils.java:141) ~[kafka-clients-3.8.1.jar:na]
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.tryConnect(ConsumerNetworkClient.java:589) ~[kafka-clients-3.8.1.jar:na]
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$FindCoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:937) ~[kafka-clients-3.8.1.jar:na]
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$FindCoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:913) ~[kafka-clients-3.8.1.jar:na]
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:206) ~[kafka-clients-3.8.1.jar:na]
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:169) ~[kafka-clients-3.8.1.jar:na]
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:129) ~[kafka-clients-3.8.1.jar:na]
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:616) ~[kafka-clients-3.8.1.jar:na]
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:428) ~[kafka-clients-3.8.1.jar:na]
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:313) ~[kafka-clients-3.8.1.jar:na]
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:231) ~[kafka-clients-3.8.1.jar:na]
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:289) ~[kafka-clients-3.8.1.jar:na]
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:263) ~[kafka-clients-3.8.1.jar:na]
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.coordinatorUnknownAndUnreadySync(ConsumerCoordinator.java:450) ~[kafka-clients-3.8.1.jar:na]
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:482) ~[kafka-clients-3.8.1.jar:na]
	at org.apache.kafka.clients.consumer.internals.LegacyKafkaConsumer.updateAssignmentMetadataIfNeeded(LegacyKafkaConsumer.java:652) ~[kafka-clients-3.8.1.jar:na]
	at org.apache.kafka.clients.consumer.internals.LegacyKafkaConsumer.poll(LegacyKafkaConsumer.java:611) ~[kafka-clients-3.8.1.jar:na]
	at org.apache.kafka.clients.consumer.internals.LegacyKafkaConsumer.poll(LegacyKafkaConsumer.java:591) ~[kafka-clients-3.8.1.jar:na]
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:874) ~[kafka-clients-3.8.1.jar:na]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.pollConsumer(KafkaMessageListenerContainer.java:1692) ~[spring-kafka-3.3.4.jar:3.3.4]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doPoll(KafkaMessageListenerContainer.java:1667) ~[spring-kafka-3.3.4.jar:3.3.4]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.pollAndInvoke(KafkaMessageListenerContainer.java:1445) ~[spring-kafka-3.3.4.jar:3.3.4]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.run(KafkaMessageListenerContainer.java:1335) ~[spring-kafka-3.3.4.jar:3.3.4]
	at java.base/java.util.concurrent.CompletableFuture$AsyncRun.run(CompletableFuture.java:1804) ~[na:na]
	at java.base/java.lang.Thread.run(Thread.java:840) ~[na:na]

2025-04-03T23:43:02.813+09:00  INFO 45581 --- [ntainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Group coordinator logs-service-kafka-1:9092 (id: 2147483646 rack: null) is unavailable or invalid due to cause: coordinator unavailable. isDisconnected: false. Rediscovery will be attempted.
2025-04-03T23:43:02.814+09:00  INFO 45581 --- [ntainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Requesting disconnect from last known coordinator logs-service-kafka-1:9092 (id: 2147483646 rack: null)
2025-04-03T23:43:02.924+09:00  WARN 45581 --- [ntainer#0-0-C-1] org.apache.kafka.clients.NetworkClient   : [Consumer clientId=consumer-my-group-1, groupId=my-group] Error connecting to node logs-service-kafka-1:9092 (id: 1 rack: null)

java.net.UnknownHostException: logs-service-kafka-1
	at java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801) ~[na:na]
	at java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1533) ~[na:na]
	at java.base/java.net.InetAddress.getAllByName(InetAddress.java:1385) ~[na:na]
	at java.base/java.net.InetAddress.getAllByName(InetAddress.java:1306) ~[na:na]
	at org.apache.kafka.clients.DefaultHostResolver.resolve(DefaultHostResolver.java:27) ~[kafka-clients-3.8.1.jar:na]
	at org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:124) ~[kafka-clients-3.8.1.jar:na]
	at org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.resolveAddresses(ClusterConnectionStates.java:536) ~[kafka-clients-3.8.1.jar:na]
	at org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:511) ~[kafka-clients-3.8.1.jar:na]
	at org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:466) ~[kafka-clients-3.8.1.jar:na]
	at org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:173) ~[kafka-clients-3.8.1.jar:na]
	at org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:1070) ~[kafka-clients-3.8.1.jar:na]
	at org.apache.kafka.clients.NetworkClient.ready(NetworkClient.java:320) ~[kafka-clients-3.8.1.jar:na]
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.trySend(ConsumerNetworkClient.java:514) ~[kafka-clients-3.8.1.jar:na]
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:271) ~[kafka-clients-3.8.1.jar:na]
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:231) ~[kafka-clients-3.8.1.jar:na]
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:289) ~[kafka-clients-3.8.1.jar:na]
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:263) ~[kafka-clients-3.8.1.jar:na]
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.coordinatorUnknownAndUnreadySync(ConsumerCoordinator.java:450) ~[kafka-clients-3.8.1.jar:na]
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:482) ~[kafka-clients-3.8.1.jar:na]
	at org.apache.kafka.clients.consumer.internals.LegacyKafkaConsumer.updateAssignmentMetadataIfNeeded(LegacyKafkaConsumer.java:652) ~[kafka-clients-3.8.1.jar:na]
	at org.apache.kafka.clients.consumer.internals.LegacyKafkaConsumer.poll(LegacyKafkaConsumer.java:611) ~[kafka-clients-3.8.1.jar:na]
	at org.apache.kafka.clients.consumer.internals.LegacyKafkaConsumer.poll(LegacyKafkaConsumer.java:591) ~[kafka-clients-3.8.1.jar:na]
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:874) ~[kafka-clients-3.8.1.jar:na]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.pollConsumer(KafkaMessageListenerContainer.java:1692) ~[spring-kafka-3.3.4.jar:3.3.4]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doPoll(KafkaMessageListenerContainer.java:1667) ~[spring-kafka-3.3.4.jar:3.3.4]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.pollAndInvoke(KafkaMessageListenerContainer.java:1445) ~[spring-kafka-3.3.4.jar:3.3.4]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.run(KafkaMessageListenerContainer.java:1335) ~[spring-kafka-3.3.4.jar:3.3.4]
	at java.base/java.util.concurrent.CompletableFuture$AsyncRun.run(CompletableFuture.java:1804) ~[na:na]
	at java.base/java.lang.Thread.run(Thread.java:840) ~[na:na]

2025-04-03T23:43:03.121+09:00  WARN 45581 --- [ntainer#0-0-C-1] org.apache.kafka.clients.NetworkClient   : [Consumer clientId=consumer-my-group-1, groupId=my-group] Error connecting to node logs-service-kafka-1:9092 (id: 1 rack: null)

java.net.UnknownHostException: logs-service-kafka-1
	at java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801) ~[na:na]
	at java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1533) ~[na:na]
	at java.base/java.net.InetAddress.getAllByName(InetAddress.java:1385) ~[na:na]
	at java.base/java.net.InetAddress.getAllByName(InetAddress.java:1306) ~[na:na]
	at org.apache.kafka.clients.DefaultHostResolver.resolve(DefaultHostResolver.java:27) ~[kafka-clients-3.8.1.jar:na]
	at org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:124) ~[kafka-clients-3.8.1.jar:na]
	at org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.resolveAddresses(ClusterConnectionStates.java:536) ~[kafka-clients-3.8.1.jar:na]
	at org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:511) ~[kafka-clients-3.8.1.jar:na]
	at org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:466) ~[kafka-clients-3.8.1.jar:na]
	at org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:173) ~[kafka-clients-3.8.1.jar:na]
	at org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:1070) ~[kafka-clients-3.8.1.jar:na]
	at org.apache.kafka.clients.NetworkClient.access$800(NetworkClient.java:76) ~[kafka-clients-3.8.1.jar:na]
	at org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1259) ~[kafka-clients-3.8.1.jar:na]
	at org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1159) ~[kafka-clients-3.8.1.jar:na]
	at org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:592) ~[kafka-clients-3.8.1.jar:na]
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:281) ~[kafka-clients-3.8.1.jar:na]
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:252) ~[kafka-clients-3.8.1.jar:na]
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:243) ~[kafka-clients-3.8.1.jar:na]
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:165) ~[kafka-clients-3.8.1.jar:na]
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:302) ~[kafka-clients-3.8.1.jar:na]
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:263) ~[kafka-clients-3.8.1.jar:na]
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.coordinatorUnknownAndUnreadySync(ConsumerCoordinator.java:450) ~[kafka-clients-3.8.1.jar:na]
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:482) ~[kafka-clients-3.8.1.jar:na]
	at org.apache.kafka.clients.consumer.internals.LegacyKafkaConsumer.updateAssignmentMetadataIfNeeded(LegacyKafkaConsumer.java:652) ~[kafka-clients-3.8.1.jar:na]
	at org.apache.kafka.clients.consumer.internals.LegacyKafkaConsumer.poll(LegacyKafkaConsumer.java:611) ~[kafka-clients-3.8.1.jar:na]
	at org.apache.kafka.clients.consumer.internals.LegacyKafkaConsumer.poll(LegacyKafkaConsumer.java:591) ~[kafka-clients-3.8.1.jar:na]
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:874) ~[kafka-clients-3.8.1.jar:na]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.pollConsumer(KafkaMessageListenerContainer.java:1692) ~[spring-kafka-3.3.4.jar:3.3.4]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doPoll(KafkaMessageListenerContainer.java:1667) ~[spring-kafka-3.3.4.jar:3.3.4]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.pollAndInvoke(KafkaMessageListenerContainer.java:1445) ~[spring-kafka-3.3.4.jar:3.3.4]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.run(KafkaMessageListenerContainer.java:1335) ~[spring-kafka-3.3.4.jar:3.3.4]
	at java.base/java.util.concurrent.CompletableFuture$AsyncRun.run(CompletableFuture.java:1804) ~[na:na]
	at java.base/java.lang.Thread.run(Thread.java:840) ~[na:na]

2025-04-03T23:43:03.129+09:00  INFO 45581 --- [    Test worker] t.c.s.AnnotationConfigContextLoaderUtils : Could not detect default configuration classes for test class [me.iseunghan.springembeddedkafkatest.test2.KafkaTest2]: KafkaTest2 does not declare any static, non-private, non-final, nested classes annotated with @Configuration.
2025-04-03T23:43:03.135+09:00  INFO 45581 --- [    Test worker] .b.t.c.SpringBootTestContextBootstrapper : Found @SpringBootConfiguration me.iseunghan.springembeddedkafkatest.SpringEmbeddedKafkaTestApplication for test class me.iseunghan.springembeddedkafkatest.test2.KafkaTest2

  .   ____          _            __ _ _
 /\\ / ___'_ __ _ _(_)_ __  __ _ \ \ \ \
( ( )\___ | '_ | '_| | '_ \/ _` | \ \ \ \
 \\/  ___)| |_)| | | | | || (_| |  ) ) ) )
  '  |____| .__|_| |_|_| |_\__, | / / / /
 =========|_|==============|___/=/_/_/_/

 :: Spring Boot ::                (v3.4.4)

2025-04-03T23:43:03.179+09:00  INFO 45581 --- [    Test worker] k.utils.Log4jControllerRegistration$     : Registered kafka:type=kafka.Log4jController MBean
2025-04-03T23:43:03.184+09:00  INFO 45581 --- [    Test worker] o.a.zookeeper.server.ZooKeeperServer     :
2025-04-03T23:43:03.185+09:00  INFO 45581 --- [    Test worker] o.a.zookeeper.server.ZooKeeperServer     :   ______                  _
2025-04-03T23:43:03.185+09:00  INFO 45581 --- [    Test worker] o.a.zookeeper.server.ZooKeeperServer     :  |___  /                 | |
2025-04-03T23:43:03.185+09:00  INFO 45581 --- [    Test worker] o.a.zookeeper.server.ZooKeeperServer     :     / /    ___     ___   | | __   ___    ___   _ __     ___   _ __
2025-04-03T23:43:03.185+09:00  INFO 45581 --- [    Test worker] o.a.zookeeper.server.ZooKeeperServer     :    / /    / _ \   / _ \  | |/ /  / _ \  / _ \ | '_ \   / _ \ | '__|
2025-04-03T23:43:03.185+09:00  INFO 45581 --- [    Test worker] o.a.zookeeper.server.ZooKeeperServer     :   / /__  | (_) | | (_) | |   <  |  __/ |  __/ | |_) | |  __/ | |
2025-04-03T23:43:03.185+09:00  INFO 45581 --- [    Test worker] o.a.zookeeper.server.ZooKeeperServer     :  /_____|  \___/   \___/  |_|\_\  \___|  \___| | .__/   \___| |_|
2025-04-03T23:43:03.185+09:00  INFO 45581 --- [    Test worker] o.a.zookeeper.server.ZooKeeperServer     :                                               | |
2025-04-03T23:43:03.185+09:00  INFO 45581 --- [    Test worker] o.a.zookeeper.server.ZooKeeperServer     :                                               |_|
2025-04-03T23:43:03.185+09:00  INFO 45581 --- [    Test worker] o.a.zookeeper.server.ZooKeeperServer     :
2025-04-03T23:43:03.188+09:00  INFO 45581 --- [    Test worker] o.a.zookeeper.server.ZooKeeperServer     : Server environment:zookeeper.version=3.8.4-9316c2a7a97e1666d8f4593f34dd6fc36ecc436c, built on 2024-02-12 22:16 UTC
2025-04-03T23:43:03.188+09:00  INFO 45581 --- [    Test worker] o.a.zookeeper.server.ZooKeeperServer     : Server environment:host.name=localhost
2025-04-03T23:43:03.188+09:00  INFO 45581 --- [    Test worker] o.a.zookeeper.server.ZooKeeperServer     : Server environment:java.version=17.0.12
2025-04-03T23:43:03.188+09:00  INFO 45581 --- [    Test worker] o.a.zookeeper.server.ZooKeeperServer     : Server environment:java.vendor=Eclipse Adoptium
2025-04-03T23:43:03.188+09:00  INFO 45581 --- [    Test worker] o.a.zookeeper.server.ZooKeeperServer     : Server environment:java.home=/Library/Java/JavaVirtualMachines/temurin-17.jdk/Contents/Home
2025-04-03T23:43:03.188+09:00  INFO 45581 --- [    Test worker] o.a.zookeeper.server.ZooKeeperServer     : Server environment:java.class.path=/Users/shlee/.gradle/caches/8.13/workerMain/gradle-worker.jar:/Users/shlee/workspaces/study/iseunghan-Lab/spring-embedded-kafka-test/build/classes/kotlin/test:/Users/shlee/workspaces/study/iseunghan-Lab/spring-embedded-kafka-test/build/classes/kotlin/main:/Users/shlee/workspaces/study/iseunghan-Lab/spring-embedded-kafka-test/build/resources/main:/Users/shlee/.gradle/caches/modules-2/files-2.1/org.springframework.boot/spring-boot-starter-web/3.4.4/441c0c71f38783c1d34b8bac5f06f1fa6b103274/spring-boot-starter-web-3.4.4.jar:/Users/shlee/.gradle/caches/modules-2/files-2.1/org.springframework.boot/spring-boot-starter-json/3.4.4/7f7ca72abcab50661b7b5d23f63b3bc2375bb2b3/spring-boot-starter-json-3.4.4.jar:/Users/shlee/.gradle/caches/modules-2/files-2.1/org.springframework.kafka/spring-kafka-test/3.3.4/d355af0a1960bca450aef05c73646edd869a839b/spring-kafka-test-3.3.4.jar:/Users/shlee/.gradle/caches/modules-2/files-2.1/org.apache.kafka/kafka_2.13/3.8.1/832245e638abf7ec3996b76a42944960abe39e23/kafka_2.13-3.8.1.jar:/Users/shlee/.gradle/caches/modules-2/files-2.1/org.apache.kafka/kafka_2.13/3.8.1/6278b0f00854c36ed12c8a016152332bc9d1a609/kafka_2.13-3.8.1-test.jar:/Users/shlee/.gradle/caches/modules-2/files-2.1/org.apache.kafka/kafka-server/3.8.1/9b339e97deeecaf1e7897b4c731d77392b4745de/kafka-server-3.8.1.jar:/Users/shlee/.gradle/caches/modules-2/files-2.1/org.apache.kafka/kafka-group-coordinator/3.8.1/d372d5bfee64939af353d4246cab795884cb477c/kafka-group-coordinator-3.8.1.jar:/Users/shlee/.gradle/caches/modules-2/files-2.1/org.apache.kafka/kafka-metadata/3.8.1/3d6662afa30ed98985422d35485962819253040b/kafka-metadata-3.8.1.jar:/Users/shlee/.gradle/caches/modules-2/files-2.1/org.apache.kafka/kafka-raft/3.8.1/8f44b9f0da4fa1670d8506d33c06ba7662b37045/kafka-raft-3.8.1.jar:/Users/shlee/.gradle/caches/modules-2/files-2.1/org.apache.kafka/kafka-storage/3.8.1/daf35a6f6a1b8da8d2baa754856b12d5ebec7be3/kafka-storage-3.8.1.jar:/Users/shlee/.gradle/caches/modules-2/files-2.1/org.apache.kafka/kafka-storage-api/3.8.1/a522b224b598f30b9637ad2eb3a5db64c14db4a4/kafka-storage-api-3.8.1.jar:/Users/shlee/.gradle/caches/modules-2/files-2.1/org.apache.kafka/kafka-server-common/3.8.1/483d5f635aa0a751946a6bd80b0b3acbdc6b7169/kafka-server-common-3.8.1.jar:/Users/shlee/.gradle/caches/modules-2/files-2.1/org.apache.kafka/kafka-server-common/3.8.1/4ad5b80346a131afa11708238441f16026f0ec5c/kafka-server-common-3.8.1-test.jar:/Users/shlee/.gradle/caches/modules-2/files-2.1/com.fasterxml.jackson.datatype/jackson-datatype-jdk8/2.18.3/621558295660935134b171ce2b0d9ad6842ec2ff/jackson-datatype-jdk8-2.18.3.jar:/Users/shlee/.gradle/caches/modules-2/files-2.1/com.fasterxml.jackson.datatype/jackson-datatype-jsr310/2.18.3/cc57924cccf42fc852081c36215272f84ffcd991/jackson-datatype-jsr310-2.18.3.jar:/Users/shlee/.gradle/caches/modules-2/files-2.1/com.fasterxml.jackson.module/jackson-module-parameter-names/2.18.3/fa63ae5eb3956b1a6d7a7ead2b513af89dea22e7/jackson-module-parameter-names-2.18.3.jar:/Users/shlee/.gradle/caches/modules-2/files-2.1/org.apache.kafka/kafka-streams-test-utils/3.8.1/edc3d933c78754476fc5564f4fc2cb3da060a25a/kafka-streams-test-utils-3.8.1.jar:/Users/shlee/.gradle/caches/modules-2/files-2.1/org.apache.kafka/kafka-streams/3.8.1/9e054a4e7b88bc80f069e22045826013b7b697ec/kafka-streams-3.8.1.jar:/Users/shlee/.gradle/caches/modules-2/files-2.1/com.fasterxml.jackson.module/jackson-module-scala_2.13/2.18.3/654e30ed3c2774c41fb285ded3076b9ff139e6fa/jackson-module-scala_2.13-2.18.3.jar:/Users/shlee/.gradle/caches/modules-2/files-2.1/com.fasterxml.jackson.dataformat/jackson-dataformat-csv/2.18.3/f560c1fca2340a9bdb0d6d5ebd903f9932962613/jackson-dataformat-csv-2.18.3.jar:/Users/shlee/.gradle/caches/modules-2/files-2.1/com.fasterxml.jackson.core/jackson-databind/2.18.3/537e3886263e3b3464385040453e92567fd509e2/jackson-databind-2.18.3.jar:/Users/shlee/.gradle/caches/modules-2/files-2.1/com.fasterxml.jackson.core/jackson-annotations/2.18.3/7fa21cf7da4598f8240e4ebd9779249622af1acd/jackson-annotations-2.18.3.jar:/Users/shlee/.gradle/caches/modules-2/files-2.1/com.fasterxml.jackson.core/jackson-core/2.18.3/78f80c259268200e588aa204dd97ecf09b76916e/jackson-core-2.18.3.jar:/Users/shlee/.gradle/caches/modules-2/files-2.1/com.fasterxml.jackson.module/jackson-module-kotlin/2.18.3/d79c117605fc157ec32ec9774690bb3e493938ce/jackson-module-kotlin-2.18.3.jar:/Users/shlee/.gradle/caches/modules-2/files-2.1/org.jetbrains.kotlin/kotlin-reflect/1.9.25/73023c38b7b20430232893cf9b556dc8486e07a4/kotlin-reflect-1.9.25.jar:/Users/shlee/.gradle/caches/modules-2/files-2.1/org.jetbrains.kotlin/kotlin-test-junit5/1.9.25/787558f7e75d14bac4b6e60421d434e66bd20f3f/kotlin-test-junit5-1.9.25.jar:/Users/shlee/.gradle/caches/modules-2/files-2.1/org.jetbrains.kotlin/kotlin-test/1.9.25/c76017b12cb5cfb74983979149cef7d6c3953e6e/kotlin-test-1.9.25.jar:/Users/shlee/.gradle/caches/modules-2/files-2.1/org.jetbrains.kotlin/kotlin-stdlib/1.9.25/f700a2f2b8f0d6d0fde48f56d894dc722fb029d7/kotlin-stdlib-1.9.25.jar:/Users/shlee/.gradle/caches/modules-2/files-2.1/org.springframework.kafka/spring-kafka/3.3.4/e9a6e9ef5e01379643e612bfde97bea06822341b/spring-kafka-3.3.4.jar:/Users/shlee/.gradle/caches/modules-2/files-2.1/org.springframework.boot/spring-boot-starter-test/3.4.4/4cb1fd0bdc34459bf389df17620e339897d1439c/spring-boot-starter-test-3.4.4.jar:/Users/shlee/.gradle/caches/modules-2/files-2.1/org.junit.jupiter/junit-jupiter/5.11.4/a699f024a4a4706b36bddbeb42d499aff9e09379/junit-jupiter-5.11.4.jar:/Users/shlee/.gradle/caches/modules-2/files-2.1/org.junit.jupiter/junit-jupiter-engine/5.11.4/dc10ec209623986a68ea07f67cdc7d2a65a60355/junit-jupiter-engine-5.11.4.jar:/Users/shlee/.gradle/caches/modules-2/files-2.1/org.mockito/mockito-junit-jupiter/5.14.2/3cfc377d4bb9fe729f3dd9098d9a9b27da58324a/mockito-junit-jupiter-5.14.2.jar:/Users/shlee/.gradle/caches/modules-2/files-2.1/org.junit.jupiter/junit-jupiter-params/5.11.4/e4c86fbe2a39c60c6b87260ef7f7e7c1a1906481/junit-jupiter-params-5.11.4.jar:/Users/shlee/.gradle/caches/modules-2/files-2.1/org.junit.jupiter/junit-jupiter-api/5.11.4/308315b28e667db4091b2ba1f7aa220d1ddadb97/junit-jupiter-api-5.11.4.jar:/Users/shlee/.gradle/caches/modules-2/files-2.1/org.junit.platform/junit-platform-engine/1.11.4/21f61b123ad6ac8f7e73971bff3a096c8d8e1cd0/junit-platform-engine-1.11.4.jar:/Users/shlee/.gradle/caches/modules-2/files-2.1/org.junit.platform/junit-platform-commons/1.11.4/8898eea3ed0da2641548d602c3e308804f166303/junit-platform-commons-1.11.4.jar:/Users/shlee/.gradle/caches/modules-2/files-2.1/org.junit.platform/junit-platform-launcher/1.11.4/3d83c201899d8c5e74e1a5d628eab900342a0e48/junit-platform-launcher-1.11.4.jar:/Users/shlee/.gradle/caches/modules-2/files-2.1/org.jetbrains/annotations/13.0/919f0dfe192fb4e063e7dacadee7f8bb9a2672a9/annotations-13.0.jar:/Users/shlee/.gradle/caches/modules-2/files-2.1/org.springframework.boot/spring-boot-starter/3.4.4/6ad00ebe69a28a5c1c97f80f81920d65e0e4250b/spring-boot-starter-3.4.4.jar:/Users/shlee/.gradle/caches/modules-2/files-2.1/org.springframework.boot/spring-boot-starter-tomcat/3.4.4/4b18ac49cf13ef2c9a98418ebd8fc212a5259da9/spring-boot-starter-tomcat-3.4.4.jar:/Users/shlee/.gradle/caches/modules-2/files-2.1/org.springframework/spring-webmvc/6.2.5/db8f1171041d7091f3de80cffdfb9d6c5fbf3015/spring-webmvc-6.2.5.jar:/Users/shlee/.gradle/caches/modules-2/files-2.1/org.springframework/spring-web/6.2.5/b42d2c0acbe05bad4c849883aa8816c25b6c1d94/spring-web-6.2.5.jar:/Users/shlee/.gradle/caches/modules-2/files-2.1/org.springframework.boot/spring-boot-test-autoconfigure/3.4.4/3d799c8cd92eed0d0624de08fec52ab01a638e51/spring-boot-test-autoconfigure-3.4.4.jar:/Users/shlee/.gradle/caches/modules-2/files-2.1/org.springframework.boot/spring-boot-test/3.4.4/cc93446b14f050793c57ecd74a066ecf9ebac96b/spring-boot-test-3.4.4.jar:/Users/shlee/.gradle/caches/modules-2/files-2.1/org.springframework.boot/spring-boot-autoconfigure/3.4.4/5ebc8c0682374768ee6eac9acf12f41e76762207/spring-boot-autoconfigure-3.4.4.jar:/Users/shlee/.gradle/caches/modules-2/files-2.1/org.springframework.boot/spring-boot/3.4.4/f9dbe14c2e5e35a2cd27156802ea6b7c42ab34fd/spring-boot-3.4.4.jar:/Users/shlee/.gradle/caches/modules-2/files-2.1/org.springframework/spring-context/6.2.5/237de0c3afca2099ab497cc7464726c02b8ab5c5/spring-context-6.2.5.jar:/Users/shlee/.gradle/caches/modules-2/files-2.1/org.springframework/spring-messaging/6.2.5/977fa11f45ce6391b4e1e76830f08b0647fe44a9/spring-messaging-6.2.5.jar:/Users/shlee/.gradle/caches/modules-2/files-2.1/org.springframework/spring-tx/6.2.5/99ecc1c976f58ad63df7205e18eac628f2f1fa9a/spring-tx-6.2.5.jar:/Users/shlee/.gradle/caches/modules-2/files-2.1/org.springframework.retry/spring-retry/2.0.11/bd4fae67445baf330b69b6b786748a308ab31f6/spring-retry-2.0.11.jar:/Users/shlee/.gradle/caches/modules-2/files-2.1/org.apache.kafka/kafka-group-coordinator-api/3.8.1/9f99f11956404ae64a2b210adaf6d1f3850ab26f/kafka-group-coordinator-api-3.8.1.jar:/Users/shlee/.gradle/caches/modules-2/files-2.1/org.apache.kafka/kafka-tools-api/3.8.1/43f69819f286340b5738e562122256b1b2470e99/kafka-tools-api-3.8.1.jar:/Users/shlee/.gradle/caches/modules-2/files-2.1/org.apache.kafka/kafka-clients/3.8.1/fd79e3aa252c6d818334e9c0bac8166b426e498c/kafka-clients-3.8.1.jar:/Users/shlee/.gradle/caches/modules-2/files-2.1/org.apache.kafka/kafka-clients/3.8.1/11d3ffefbc452fc4c5d45f4f6ec368bcab290a95/kafka-clients-3.8.1-test.jar:/Users/shlee/.gradle/caches/modules-2/files-2.1/io.micrometer/micrometer-observation/1.14.5/b23dff6bf07a29f67fdae8f3f3f8f1c78fa7b126/micrometer-observation-1.14.5.jar:/Users/shlee/.gradle/caches/modules-2/files-2.1/com.jayway.jsonpath/json-path/2.9.0/37fe2217f577b0b68b18e62c4d17a8858ecf9b69/json-path-2.9.0.jar:/Users/shlee/.gradle/caches/modules-2/files-2.1/jakarta.xml.bind/jakarta.xml.bind-api/4.0.2/6cd5a999b834b63238005b7144136379dc36cad2/jakarta.xml.bind-api-4.0.2.jar:/Users/shlee/.gradle/caches/modules-2/files-2.1/net.minidev/json-smart/2.5.2/95d166b18f95907be0f46cdb9e1c0695eed03387/json-smart-2.5.2.jar:/Users/shlee/.gradle/caches/modules-2/files-2.1/org.assertj/assertj-core/3.26.3/d26263eb7524252d98e602fc6942996a3195e29/assertj-core-3.26.3.jar:/Users/shlee/.gradle/caches/modules-2/files-2.1/org.awaitility/awaitility/4.2.2/7336242073ebf83fe034e42b46a403c5501b63c9/awaitility-4.2.2.jar:/Users/shlee/.gradle/caches/modules-2/files-2.1/org.hamcrest/hamcrest/2.2/1820c0968dba3a11a1b30669bb1f01978a91dedc/hamcrest-2.2.jar:/Users/shlee/.gradle/caches/modules-2/files-2.1/org.mockito/mockito-core/5.14.2/f7bf936008d7664e2002c3faf0c02071c8d10e7c/mockito-core-5.14.2.jar:/Users/shlee/.gradle/caches/modules-2/files-2.1/org.skyscreamer/jsonassert/1.5.3/aaa43e0823d2a0e106e8754d6a9c4ab24e05e9bc/jsonassert-1.5.3.jar:/Users/shlee/.gradle/caches/modules-2/files-2.1/org.springframework/spring-test/6.2.5/4db580c1808ae17efab33dee218320a66adfade6/spring-test-6.2.5.jar:/Users/shlee/.gradle/caches/modules-2/files-2.1/org.springframework/spring-aop/6.2.5/9f436be65bf45ee8643d93e6823d6c81e8f9f91a/spring-aop-6.2.5.jar:/Users/shlee/.gradle/caches/modules-2/files-2.1/org.springframework/spring-beans/6.2.5/6f8eb671d9905da5a73bc9ab7e703e9fed6a0c3f/spring-beans-6.2.5.jar:/Users/shlee/.gradle/caches/modules-2/files-2.1/org.springframework/spring-expression/6.2.5/cb39181911dabe3b3d9992c4b4da38468726ea72/spring-expression-6.2.5.jar:/Users/shlee/.gradle/caches/modules-2/files-2.1/org.springframework/spring-core/6.2.5/d6786db7122037bf605e54e3b35f262a19b8d502/spring-core-6.2.5.jar:/Users/shlee/.gradle/caches/modules-2/files-2.1/org.xmlunit/xmlunit-core/2.10.0/1355088731b4ec2107ff7f319f0d7445d916bab/xmlunit-core-2.10.0.jar:/Users/shlee/.gradle/caches/modules-2/files-2.1/org.apache.zookeeper/zookeeper/3.8.4/6638e37b887b5a279044afbdc9928e19f678eb2e/zookeeper-3.8.4.jar:/Users/shlee/.gradle/caches/modules-2/files-2.1/org.springframework.boot/spring-boot-starter-logging/3.4.4/1a7bbfd57e0b2d0bde8ffd6aa7a0ebd94e4b0aed/spring-boot-starter-logging-3.4.4.jar:/Users/shlee/.gradle/caches/modules-2/files-2.1/jakarta.annotation/jakarta.annotation-api/2.1.1/48b9bda22b091b1f48b13af03fe36db3be6e1ae3/jakarta.annotation-api-2.1.1.jar:/Users/shlee/.gradle/caches/modules-2/files-2.1/org.yaml/snakeyaml/2.3/936b36210e27320f920536f695cf1af210c44586/snakeyaml-2.3.jar:/Users/shlee/.gradle/caches/modules-2/files-2.1/org.apache.tomcat.embed/tomcat-embed-websocket/10.1.39/abf50abbbae651f95dfcf9fae05f919bfa9b7046/tomcat-embed-websocket-10.1.39.jar:/Users/shlee/.gradle/caches/modules-2/files-2.1/org.apache.tomcat.embed/tomcat-embed-core/10.1.39/f6acead04214d5aaea82c2639392208df33b3abe/tomcat-embed-core-10.1.39.jar:/Users/shlee/.gradle/caches/modules-2/files-2.1/org.apache.tomcat.embed/tomcat-embed-el/10.1.39/88ff05c768c4654097ab0a9f2b49368a0877ad76/tomcat-embed-el-10.1.39.jar:/Users/shlee/.gradle/caches/modules-2/files-2.1/com.github.luben/zstd-jni/1.5.6-4/ba9e303e0b5e94cdd0017390d7d8c06f47fd61f7/zstd-jni-1.5.6-4.jar:/Users/shlee/.gradle/caches/modules-2/files-2.1/org.lz4/lz4-java/1.8.0/4b986a99445e49ea5fbf5d149c4b63f6ed6c6780/lz4-java-1.8.0.jar:/Users/shlee/.gradle/caches/modules-2/files-2.1/org.xerial.snappy/snappy-java/1.1.10.5/ac605269f3598506196e469f1fb0d7ed5c55059e/snappy-java-1.1.10.5.jar:/Users/shlee/.gradle/caches/modules-2/files-2.1/ch.qos.logback/logback-classic/1.5.18/fc371f3fc97a639de2d67947cffb7518ec5e3d40/logback-classic-1.5.18.jar:/Users/shlee/.gradle/caches/modules-2/files-2.1/com.yammer.metrics/metrics-core/2.2.0/f82c035cfa786d3cbec362c38c22a5f5b1bc8724/metrics-core-2.2.0.jar:/Users/shlee/.gradle/caches/modules-2/files-2.1/org.bitbucket.b_c/jose4j/0.9.4/7efe6ccf593e52a2b77f98de52238f15b4a67188/jose4j-0.9.4.jar:/Users/shlee/.gradle/caches/modules-2/files-2.1/com.typesafe.scala-logging/scala-logging_2.13/3.9.4/e5838ddcf8c1c2b612a7956b9f80423a508675db/scala-logging_2.13-3.9.4.jar:/Users/shlee/.gradle/caches/modules-2/files-2.1/io.dropwizard.metrics/metrics-core/4.1.12.1/cb2f351bf4463751201f43bb99865235d5ba07ca/metrics-core-4.1.12.1.jar:/Users/shlee/.gradle/caches/modules-2/files-2.1/org.apache.logging.log4j/log4j-to-slf4j/2.24.3/da1143e2a2531ee1c2d90baa98eb50a28a39d5a7/log4j-to-slf4j-2.24.3.jar:/Users/shlee/.gradle/caches/modules-2/files-2.1/org.slf4j/jul-to-slf4j/2.0.17/524cb6ccc2b68a57604750e1ab8b13b5a786a6aa/jul-to-slf4j-2.0.17.jar:/Users/shlee/.gradle/caches/modules-2/files-2.1/org.slf4j/slf4j-api/2.0.17/d9e58ac9c7779ba3bf8142aff6c830617a7fe60f/slf4j-api-2.0.17.jar:/Users/shlee/.gradle/caches/modules-2/files-2.1/io.micrometer/micrometer-commons/1.14.5/6201a40489ccedc9539c5f7a2c84e9e64702bf10/micrometer-commons-1.14.5.jar:/Users/shlee/.gradle/caches/modules-2/files-2.1/jakarta.activation/jakarta.activation-api/2.1.3/fa165bd70cda600368eee31555222776a46b881f/jakarta.activation-api-2.1.3.jar:/Users/shlee/.gradle/caches/modules-2/files-2.1/net.minidev/accessors-smart/2.5.2/ce16fd235cfee48e67eda33e684423bba09f7d07/accessors-smart-2.5.2.jar:/Users/shlee/.gradle/caches/modules-2/files-2.1/net.bytebuddy/byte-buddy/1.15.11/f61886478e0f9ee4c21d09574736f0ff45e0a46c/byte-buddy-1.15.11.jar:/Users/shlee/.gradle/caches/modules-2/files-2.1/net.bytebuddy/byte-buddy-agent/1.15.11/a38b16385e867f59a641330f0362ebe742788ed8/byte-buddy-agent-1.15.11.jar:/Users/shlee/.gradle/caches/modules-2/files-2.1/org.objenesis/objenesis/3.3/1049c09f1de4331e8193e579448d0916d75b7631/objenesis-3.3.jar:/Users/shlee/.gradle/caches/modules-2/files-2.1/com.vaadin.external.google/android-json/0.0.20131108.vaadin1/fa26d351fe62a6a17f5cda1287c1c6110dec413f/android-json-0.0.20131108.vaadin1.jar:/Users/shlee/.gradle/caches/modules-2/files-2.1/org.springframework/spring-jcl/6.2.5/1b8db3715dc78bd18b60869458d5ee8829ad8e99/spring-jcl-6.2.5.jar:/Users/shlee/.gradle/caches/modules-2/files-2.1/org.opentest4j/opentest4j/1.3.0/152ea56b3a72f655d4fd677fc0ef2596c3dd5e6e/opentest4j-1.3.0.jar:/Users/shlee/.gradle/caches/modules-2/files-2.1/org.apache.zookeeper/zookeeper-jute/3.8.4/de7b8a41bbe1ccdfc009de51fa6d160db3ca8025/zookeeper-jute-3.8.4.jar:/Users/shlee/.gradle/caches/modules-2/files-2.1/org.apache.yetus/audience-annotations/0.12.0/e0efa60318229590103e31c69ebdaae56d903644/audience-annotations-0.12.0.jar:/Users/shlee/.gradle/caches/modules-2/files-2.1/io.netty/netty-handler/4.1.119.Final/a0059b8d779ce566b524efd6d73ba4fcff3cd5d9/netty-handler-4.1.119.Final.jar:/Users/shlee/.gradle/caches/modules-2/files-2.1/io.netty/netty-transport-native-epoll/4.1.119.Final/c4b9339262b59f4808674d3b4ba4d5c4846921df/netty-transport-native-epoll-4.1.119.Final.jar:/Users/shlee/.gradle/caches/modules-2/files-2.1/ch.qos.logback/logback-core/1.5.18/6c0375624f6f36b4e089e2488ba21334a11ef13f/logback-core-1.5.18.jar:/Users/shlee/.gradle/caches/modules-2/files-2.1/commons-io/commons-io/2.14.0/a4c6e1f6c196339473cd2e1b037f0eb97c62755b/commons-io-2.14.0.jar:/Users/shlee/.gradle/caches/modules-2/files-2.1/org.apache.kafka/kafka-transaction-coordinator/3.8.1/3c2676a95a416f3d5a59a498c4553914be1d332d/kafka-transaction-coordinator-3.8.1.jar:/Users/shlee/.gradle/caches/modules-2/files-2.1/net.sf.jopt-simple/jopt-simple/5.0.4/4fdac2fbe92dfad86aa6e9301736f6b4342a3f5c/jopt-simple-5.0.4.jar:/Users/shlee/.gradle/caches/modules-2/files-2.1/org.pcollections/pcollections/4.0.1/59f3bf5fb28c5f5386804dcf129267416b75d7c/pcollections-4.0.1.jar:/Users/shlee/.gradle/caches/modules-2/files-2.1/net.sourceforge.argparse4j/argparse4j/0.7.0/6f0621d0c3888de39e0f06d01f37ba53a798e657/argparse4j-0.7.0.jar:/Users/shlee/.gradle/caches/modules-2/files-2.1/commons-validator/commons-validator/1.7/76069c915de3787f3ddd8726a56f47a95bfcbb0e/commons-validator-1.7.jar:/Users/shlee/.gradle/caches/modules-2/files-2.1/org.scala-lang.modules/scala-collection-compat_2.13/2.10.0/2464528d14329ff2b44bceff918d6c00300adcf4/scala-collection-compat_2.13-2.10.0.jar:/Users/shlee/.gradle/caches/modules-2/files-2.1/org.scala-lang.modules/scala-java8-compat_2.13/1.0.2/5b54f60ccf03e41e24c466462a72475ee918c304/scala-java8-compat_2.13-1.0.2.jar:/Users/shlee/.gradle/caches/modules-2/files-2.1/org.scala-lang/scala-reflect/2.13.14/8e275fefb2a01e178db2cdfebb2181062a790b82/scala-reflect-2.13.14.jar:/Users/shlee/.gradle/caches/modules-2/files-2.1/commons-cli/commons-cli/1.4/c51c00206bb913cd8612b24abd9fa98ae89719b1/commons-cli-1.4.jar:/Users/shlee/.gradle/caches/modules-2/files-2.1/org.scala-lang/scala-library/2.13.15/ed6f1d58968b16c5f9067d5cac032d952552de58/scala-library-2.13.15.jar:/Users/shlee/.gradle/caches/modules-2/files-2.1/org.ow2.asm/asm/9.7.1/f0ed132a49244b042cd0e15702ab9f2ce3cc8436/asm-9.7.1.jar:/Users/shlee/.gradle/caches/modules-2/files-2.1/io.netty/netty-transport-classes-epoll/4.1.119.Final/1d2d986ae0615d50f5cee338f0e9e73f46e8158/netty-transport-classes-epoll-4.1.119.Final.jar:/Users/shlee/.gradle/caches/modules-2/files-2.1/io.netty/netty-transport-native-unix-common/4.1.119.Final/55afdeb456bccf8eecb06431f3c1537269afe9af/netty-transport-native-unix-common-4.1.119.Final.jar:/Users/shlee/.gradle/caches/modules-2/files-2.1/io.netty/netty-codec/4.1.119.Final/337ca8e8c3ef23925e02d56347b414d7616d1d02/netty-codec-4.1.119.Final.jar:/Users/shlee/.gradle/caches/modules-2/files-2.1/io.netty/netty-transport/4.1.119.Final/d05df879054297962d056b77460fc4ff20e30073/netty-transport-4.1.119.Final.jar:/Users/shlee/.gradle/caches/modules-2/files-2.1/io.netty/netty-resolver/4.1.119.Final/e1f7c90aff71bdf0e2294a200b5f89244f88dcf7/netty-resolver-4.1.119.Final.jar:/Users/shlee/.gradle/caches/modules-2/files-2.1/io.netty/netty-buffer/4.1.119.Final/5864149b2e5a2af7b05fe9e9dc3e4b70d3060a7c/netty-buffer-4.1.119.Final.jar:/Users/shlee/.gradle/caches/modules-2/files-2.1/io.netty/netty-common/4.1.119.Final/2f7c360b03c0aceab7efc1f7c2b75274f0f35909/netty-common-4.1.119.Final.jar:/Users/shlee/.gradle/caches/modules-2/files-2.1/org.rocksdb/rocksdbjni/7.9.2/6409b667493149191b09fe1fce94bada6096a3e9/rocksdbjni-7.9.2.jar:/Users/shlee/.gradle/caches/modules-2/files-2.1/com.github.ben-manes.caffeine/caffeine/3.1.8/24795585df8afaf70a2cd534786904ea5889c047/caffeine-3.1.8.jar:/Users/shlee/.gradle/caches/modules-2/files-2.1/commons-beanutils/commons-beanutils/1.9.4/d52b9abcd97f38c81342bb7e7ae1eee9b73cba51/commons-beanutils-1.9.4.jar:/Users/shlee/.gradle/caches/modules-2/files-2.1/commons-digester/commons-digester/2.1/73a8001e7a54a255eef0f03521ec1805dc738ca0/commons-digester-2.1.jar:/Users/shlee/.gradle/caches/modules-2/files-2.1/commons-collections/commons-collections/3.2.2/8ad72fe39fa8c91eaaf12aadb21e0c3661fe26d5/commons-collections-3.2.2.jar:/Users/shlee/.gradle/caches/modules-2/files-2.1/org.apache.logging.log4j/log4j-api/2.24.3/b02c125db8b6d295adf72ae6e71af5d83bce2370/log4j-api-2.24.3.jar:/Users/shlee/.gradle/caches/modules-2/files-2.1/com.thoughtworks.paranamer/paranamer/2.8/619eba74c19ccf1da8ebec97a2d7f8ba05773dd6/paranamer-2.8.jar:/Users/shlee/.gradle/caches/modules-2/files-2.1/org.checkerframework/checker-qual/3.37.0/ba74746d38026581c12166e164bb3c15e90cc4ea/checker-qual-3.37.0.jar:/Users/shlee/.gradle/caches/modules-2/files-2.1/com.google.errorprone/error_prone_annotations/2.21.1/6d9b10773b5237df178a7b3c1b4208df7d0e7f94/error_prone_annotations-2.21.1.jar
2025-04-03T23:43:03.189+09:00  INFO 45581 --- [    Test worker] o.a.zookeeper.server.ZooKeeperServer     : Server environment:java.library.path=/Users/shlee/Library/Java/Extensions:/Library/Java/Extensions:/Network/Library/Java/Extensions:/System/Library/Java/Extensions:/usr/lib/java:.
2025-04-03T23:43:03.189+09:00  INFO 45581 --- [    Test worker] o.a.zookeeper.server.ZooKeeperServer     : Server environment:java.io.tmpdir=/var/folders/qb/nxyw0kj549bbf87mx3ss8vgc0000gn/T/
2025-04-03T23:43:03.189+09:00  INFO 45581 --- [    Test worker] o.a.zookeeper.server.ZooKeeperServer     : Server environment:java.compiler=<NA>
2025-04-03T23:43:03.189+09:00  INFO 45581 --- [    Test worker] o.a.zookeeper.server.ZooKeeperServer     : Server environment:os.name=Mac OS X
2025-04-03T23:43:03.189+09:00  INFO 45581 --- [    Test worker] o.a.zookeeper.server.ZooKeeperServer     : Server environment:os.arch=aarch64
2025-04-03T23:43:03.189+09:00  INFO 45581 --- [    Test worker] o.a.zookeeper.server.ZooKeeperServer     : Server environment:os.version=15.3.2
2025-04-03T23:43:03.189+09:00  INFO 45581 --- [    Test worker] o.a.zookeeper.server.ZooKeeperServer     : Server environment:user.name=shlee
2025-04-03T23:43:03.189+09:00  INFO 45581 --- [    Test worker] o.a.zookeeper.server.ZooKeeperServer     : Server environment:user.home=/Users/shlee
2025-04-03T23:43:03.189+09:00  INFO 45581 --- [    Test worker] o.a.zookeeper.server.ZooKeeperServer     : Server environment:user.dir=/Users/shlee/workspaces/study/iseunghan-Lab/spring-embedded-kafka-test
2025-04-03T23:43:03.189+09:00  INFO 45581 --- [    Test worker] o.a.zookeeper.server.ZooKeeperServer     : Server environment:os.memory.free=31MB
2025-04-03T23:43:03.189+09:00  INFO 45581 --- [    Test worker] o.a.zookeeper.server.ZooKeeperServer     : Server environment:os.memory.max=512MB
2025-04-03T23:43:03.189+09:00  INFO 45581 --- [    Test worker] o.a.zookeeper.server.ZooKeeperServer     : Server environment:os.memory.total=67MB
2025-04-03T23:43:03.189+09:00  INFO 45581 --- [    Test worker] o.a.zookeeper.server.ZooKeeperServer     : zookeeper.enableEagerACLCheck = false
2025-04-03T23:43:03.189+09:00  INFO 45581 --- [    Test worker] o.a.zookeeper.server.ZooKeeperServer     : zookeeper.digest.enabled = true
2025-04-03T23:43:03.189+09:00  INFO 45581 --- [    Test worker] o.a.zookeeper.server.ZooKeeperServer     : zookeeper.closeSessionTxn.enabled = true
2025-04-03T23:43:03.189+09:00  INFO 45581 --- [    Test worker] o.a.zookeeper.server.ZooKeeperServer     : zookeeper.flushDelay = 0 ms
2025-04-03T23:43:03.189+09:00  INFO 45581 --- [    Test worker] o.a.zookeeper.server.ZooKeeperServer     : zookeeper.maxWriteQueuePollTime = 0 ms
2025-04-03T23:43:03.189+09:00  INFO 45581 --- [    Test worker] o.a.zookeeper.server.ZooKeeperServer     : zookeeper.maxBatchSize=1000
2025-04-03T23:43:03.189+09:00  INFO 45581 --- [    Test worker] o.a.zookeeper.server.ZooKeeperServer     : zookeeper.intBufferStartingSizeBytes = 1024
2025-04-03T23:43:03.190+09:00  INFO 45581 --- [    Test worker] o.a.z.server.persistence.FileTxnSnapLog  : zookeeper.snapshot.trust.empty : false
2025-04-03T23:43:03.197+09:00  INFO 45581 --- [    Test worker] o.a.z.server.watch.WatchManagerFactory   : Using org.apache.zookeeper.server.watch.WatchManager as watch manager
2025-04-03T23:43:03.197+09:00  INFO 45581 --- [    Test worker] o.a.z.server.watch.WatchManagerFactory   : Using org.apache.zookeeper.server.watch.WatchManager as watch manager
2025-04-03T23:43:03.197+09:00  INFO 45581 --- [    Test worker] org.apache.zookeeper.server.ZKDatabase   : zookeeper.snapshotSizeFactor = 0.33
2025-04-03T23:43:03.197+09:00  INFO 45581 --- [    Test worker] org.apache.zookeeper.server.ZKDatabase   : zookeeper.commitLogCount=500
2025-04-03T23:43:03.199+09:00  INFO 45581 --- [    Test worker] o.apache.zookeeper.server.BlueThrottle   : Weighed connection throttling is disabled
2025-04-03T23:43:03.199+09:00  INFO 45581 --- [    Test worker] o.a.zookeeper.server.ZooKeeperServer     : minSessionTimeout set to 1600 ms
2025-04-03T23:43:03.199+09:00  INFO 45581 --- [    Test worker] o.a.zookeeper.server.ZooKeeperServer     : maxSessionTimeout set to 16000 ms
2025-04-03T23:43:03.199+09:00  INFO 45581 --- [    Test worker] o.apache.zookeeper.server.ResponseCache  : getData response cache size is initialized with value 400.
2025-04-03T23:43:03.200+09:00  INFO 45581 --- [    Test worker] o.apache.zookeeper.server.ResponseCache  : getChildren response cache size is initialized with value 400.
2025-04-03T23:43:03.200+09:00  INFO 45581 --- [    Test worker] o.a.z.s.u.RequestPathMetricsCollector    : zookeeper.pathStats.slotCapacity = 60
2025-04-03T23:43:03.200+09:00  INFO 45581 --- [    Test worker] o.a.z.s.u.RequestPathMetricsCollector    : zookeeper.pathStats.slotDuration = 15
2025-04-03T23:43:03.200+09:00  INFO 45581 --- [    Test worker] o.a.z.s.u.RequestPathMetricsCollector    : zookeeper.pathStats.maxDepth = 6
2025-04-03T23:43:03.200+09:00  INFO 45581 --- [    Test worker] o.a.z.s.u.RequestPathMetricsCollector    : zookeeper.pathStats.initialDelay = 5
2025-04-03T23:43:03.200+09:00  INFO 45581 --- [    Test worker] o.a.z.s.u.RequestPathMetricsCollector    : zookeeper.pathStats.delay = 5
2025-04-03T23:43:03.200+09:00  INFO 45581 --- [    Test worker] o.a.z.s.u.RequestPathMetricsCollector    : zookeeper.pathStats.enabled = false
2025-04-03T23:43:03.201+09:00  INFO 45581 --- [    Test worker] o.a.zookeeper.server.ZooKeeperServer     : The max bytes for all large requests are set to 104857600
2025-04-03T23:43:03.201+09:00  INFO 45581 --- [    Test worker] o.a.zookeeper.server.ZooKeeperServer     : The large request threshold is set to -1
2025-04-03T23:43:03.201+09:00  INFO 45581 --- [    Test worker] o.a.z.server.AuthenticationHelper        : zookeeper.enforce.auth.enabled = false
2025-04-03T23:43:03.201+09:00  INFO 45581 --- [    Test worker] o.a.z.server.AuthenticationHelper        : zookeeper.enforce.auth.schemes = []
2025-04-03T23:43:03.201+09:00  INFO 45581 --- [    Test worker] o.a.zookeeper.server.ZooKeeperServer     : Created server with tickTime 800 ms minSessionTimeout 1600 ms maxSessionTimeout 16000 ms clientPortListenBacklog -1 datadir /var/folders/qb/nxyw0kj549bbf87mx3ss8vgc0000gn/T/kafka-13420875248799346593/version-2 snapdir /var/folders/qb/nxyw0kj549bbf87mx3ss8vgc0000gn/T/kafka-520355760697797654/version-2
2025-04-03T23:43:03.203+09:00  WARN 45581 --- [    Test worker] o.a.zookeeper.server.ServerCnxnFactory   : maxCnxns is not configured, using default value 0.
2025-04-03T23:43:03.204+09:00  INFO 45581 --- [    Test worker] o.a.z.server.NIOServerCnxnFactory        : Configuring NIO connection handler with 10s sessionless connection timeout, 2 selector thread(s), 20 worker threads, and 64 kB direct buffers.
2025-04-03T23:43:03.205+09:00  INFO 45581 --- [    Test worker] o.a.z.server.NIOServerCnxnFactory        : binding to port /127.0.0.1:0
2025-04-03T23:43:03.211+09:00  INFO 45581 --- [    Test worker] o.a.z.server.persistence.SnapStream      : zookeeper.snapshot.compression.method = CHECKED
2025-04-03T23:43:03.211+09:00  INFO 45581 --- [    Test worker] o.a.z.server.persistence.FileTxnSnapLog  : Snapshotting: 0x0 to /var/folders/qb/nxyw0kj549bbf87mx3ss8vgc0000gn/T/kafka-520355760697797654/version-2/snapshot.0
2025-04-03T23:43:03.213+09:00  INFO 45581 --- [    Test worker] org.apache.zookeeper.server.ZKDatabase   : Snapshot loaded in 7 ms, highest zxid is 0x0, digest is 1371985504
2025-04-03T23:43:03.213+09:00  INFO 45581 --- [    Test worker] o.a.z.server.persistence.FileTxnSnapLog  : Snapshotting: 0x0 to /var/folders/qb/nxyw0kj549bbf87mx3ss8vgc0000gn/T/kafka-520355760697797654/version-2/snapshot.0
2025-04-03T23:43:03.214+09:00  INFO 45581 --- [    Test worker] o.a.zookeeper.server.ZooKeeperServer     : Snapshot taken in 1 ms
2025-04-03T23:43:03.217+09:00  INFO 45581 --- [0 cport:58604):] o.a.z.server.PrepRequestProcessor        : PrepRequestProcessor (sid:0) started, reconfigEnabled=false
2025-04-03T23:43:03.217+09:00  INFO 45581 --- [    Test worker] o.a.zookeeper.server.RequestThrottler    : zookeeper.request_throttler.shutdownTimeout = 10000 ms
2025-04-03T23:43:03.288+09:00  INFO 45581 --- [    Test worker] kafka.server.KafkaConfig                 : KafkaConfig values:
	advertised.listeners = null
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name =
	auto.create.topics.enable = true
	auto.include.jmx.reporter = true
	auto.leader.rebalance.enable = true
	background.threads = 2
	broker.heartbeat.interval.ms = 2000
	broker.id = 0
	broker.id.generation.enable = true
	broker.rack = null
	broker.session.timeout.ms = 9000
	client.quota.callback.class = null
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = producer
	compression.zstd.level = 3
	connection.failed.authentication.delay.ms = 100
	connections.max.idle.ms = 600000
	connections.max.reauth.ms = 0
	control.plane.listener.name = null
	controlled.shutdown.enable = false
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 100
	controller.listener.names = null
	controller.quorum.append.linger.ms = 25
	controller.quorum.bootstrap.servers = []
	controller.quorum.election.backoff.max.ms = 1000
	controller.quorum.election.timeout.ms = 1000
	controller.quorum.fetch.timeout.ms = 2000
	controller.quorum.request.timeout.ms = 2000
	controller.quorum.retry.backoff.ms = 20
	controller.quorum.voters = []
	controller.quota.window.num = 11
	controller.quota.window.size.seconds = 1
	controller.socket.timeout.ms = 1000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delegation.token.secret.key = null
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	early.start.listeners = null
	eligible.leader.replicas.enable = false
	fetch.max.bytes = 57671680
	fetch.purgatory.purge.interval.requests = 1000
	group.consumer.assignors = [org.apache.kafka.coordinator.group.assignor.UniformAssignor, org.apache.kafka.coordinator.group.assignor.RangeAssignor]
	group.consumer.heartbeat.interval.ms = 5000
	group.consumer.max.heartbeat.interval.ms = 15000
	group.consumer.max.session.timeout.ms = 60000
	group.consumer.max.size = 2147483647
	group.consumer.migration.policy = disabled
	group.consumer.min.heartbeat.interval.ms = 5000
	group.consumer.min.session.timeout.ms = 45000
	group.consumer.session.timeout.ms = 45000
	group.coordinator.append.linger.ms = 10
	group.coordinator.new.enable = false
	group.coordinator.rebalance.protocols = [classic]
	group.coordinator.threads = 1
	group.initial.rebalance.delay.ms = 0
	group.max.session.timeout.ms = 1800000
	group.max.size = 2147483647
	group.min.session.timeout.ms = 6000
	initial.broker.registration.timeout.ms = 60000
	inter.broker.listener.name = null
	inter.broker.protocol.version = 3.8-IV0
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = SASL_SSL:SASL_SSL,PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT
	listeners = PLAINTEXT://localhost:0
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 2097152
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.max.compaction.lag.ms = 9223372036854775807
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /var/folders/qb/nxyw0kj549bbf87mx3ss8vgc0000gn/T/spring.kafka.0f9f4ec2-491a-4eb8-ba3a-35f677f610e66802667832673744764
	log.dir.failure.timeout.ms = 30000
	log.dirs = null
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.initial.task.delay.ms = 30000
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	log.message.downconversion.enable = true
	log.message.format.version = 3.0-IV1
	log.message.timestamp.after.max.ms = 9223372036854775807
	log.message.timestamp.before.max.ms = 9223372036854775807
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 1000
	max.connection.creation.rate = 2147483647
	max.connections = 2147483647
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides =
	max.incremental.fetch.session.cache.slots = 1000
	max.request.partition.size.limit = 2000
	message.max.bytes = 1048588
	metadata.log.dir = null
	metadata.log.max.record.bytes.between.snapshots = 20971520
	metadata.log.max.snapshot.interval.ms = 3600000
	metadata.log.segment.bytes = 1073741824
	metadata.log.segment.min.bytes = 8388608
	metadata.log.segment.ms = 604800000
	metadata.max.idle.interval.ms = 500
	metadata.max.retention.bytes = 104857600
	metadata.max.retention.ms = 604800000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	node.id = 0
	num.io.threads = 8
	num.network.threads = 2
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 5
	offsets.topic.replication.factor = 1
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	principal.builder.class = class org.apache.kafka.common.security.authenticator.DefaultKafkaPrincipalBuilder
	process.roles = []
	producer.id.expiration.check.interval.ms = 600000
	producer.id.expiration.ms = 86400000
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.window.num = 11
	quota.window.size.seconds = 1
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 9223372036854775807
	replica.lag.time.max.ms = 30000
	replica.selector.class = null
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 1000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism.controller.protocol = GSSAPI
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	sasl.server.callback.handler.class = null
	sasl.server.max.receive.size = 524288
	security.inter.broker.protocol = PLAINTEXT
	security.providers = null
	server.max.startup.time.ms = 9223372036854775807
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	socket.listen.backlog.size = 50
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.allow.dn.changes = false
	ssl.allow.san.changes = false
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.principal.mapping.rules = DEFAULT
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	telemetry.max.bytes = 1048576
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 10000
	transaction.max.timeout.ms = 900000
	transaction.partition.verification.enable = true
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 2
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 1
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	unstable.api.versions.enable = false
	unstable.feature.versions.enable = true
	zookeeper.clientCnxnSocket = null
	zookeeper.connect = 127.0.0.1:58604
	zookeeper.connection.timeout.ms = 10000
	zookeeper.max.in.flight.requests = 10
	zookeeper.metadata.migration.enable = false
	zookeeper.metadata.migration.min.batch.size = 200
	zookeeper.session.timeout.ms = 18000
	zookeeper.set.acl = false
	zookeeper.ssl.cipher.suites = null
	zookeeper.ssl.client.enable = false
	zookeeper.ssl.crl.enable = false
	zookeeper.ssl.enabled.protocols = null
	zookeeper.ssl.endpoint.identification.algorithm = HTTPS
	zookeeper.ssl.keystore.location = null
	zookeeper.ssl.keystore.password = null
	zookeeper.ssl.keystore.type = null
	zookeeper.ssl.ocsp.enable = false
	zookeeper.ssl.protocol = TLSv1.2
	zookeeper.ssl.truststore.location = null
	zookeeper.ssl.truststore.password = null
	zookeeper.ssl.truststore.type = null

2025-04-03T23:43:03.302+09:00  INFO 45581 --- [    Test worker] org.apache.zookeeper.common.X509Util     : Setting -D jdk.tls.rejectClientInitiatedRenegotiation=true to disable client-initiated TLS renegotiation
2025-04-03T23:43:03.302+09:00  INFO 45581 --- [    Test worker] o.a.k.s.l.r.s.RemoteLogManagerConfig     : RemoteLogManagerConfig values:
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false

2025-04-03T23:43:03.324+09:00  WARN 45581 --- [ntainer#0-0-C-1] org.apache.kafka.clients.NetworkClient   : [Consumer clientId=consumer-my-group-1, groupId=my-group] Error connecting to node logs-service-kafka-1:9092 (id: 1 rack: null)

java.net.UnknownHostException: logs-service-kafka-1
	at java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801) ~[na:na]
	at java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1533) ~[na:na]
	at java.base/java.net.InetAddress.getAllByName(InetAddress.java:1385) ~[na:na]
	at java.base/java.net.InetAddress.getAllByName(InetAddress.java:1306) ~[na:na]
	at org.apache.kafka.clients.DefaultHostResolver.resolve(DefaultHostResolver.java:27) ~[kafka-clients-3.8.1.jar:na]
	at org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:124) ~[kafka-clients-3.8.1.jar:na]
	at org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.resolveAddresses(ClusterConnectionStates.java:536) ~[kafka-clients-3.8.1.jar:na]
	at org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:511) ~[kafka-clients-3.8.1.jar:na]
	at org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:466) ~[kafka-clients-3.8.1.jar:na]
	at org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:173) ~[kafka-clients-3.8.1.jar:na]
	at org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:1070) ~[kafka-clients-3.8.1.jar:na]
	at org.apache.kafka.clients.NetworkClient.access$800(NetworkClient.java:76) ~[kafka-clients-3.8.1.jar:na]
	at org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1259) ~[kafka-clients-3.8.1.jar:na]
	at org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1159) ~[kafka-clients-3.8.1.jar:na]
	at org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:592) ~[kafka-clients-3.8.1.jar:na]
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:281) ~[kafka-clients-3.8.1.jar:na]
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:252) ~[kafka-clients-3.8.1.jar:na]
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:243) ~[kafka-clients-3.8.1.jar:na]
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:165) ~[kafka-clients-3.8.1.jar:na]
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:302) ~[kafka-clients-3.8.1.jar:na]
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:263) ~[kafka-clients-3.8.1.jar:na]
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.coordinatorUnknownAndUnreadySync(ConsumerCoordinator.java:450) ~[kafka-clients-3.8.1.jar:na]
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:482) ~[kafka-clients-3.8.1.jar:na]
	at org.apache.kafka.clients.consumer.internals.LegacyKafkaConsumer.updateAssignmentMetadataIfNeeded(LegacyKafkaConsumer.java:652) ~[kafka-clients-3.8.1.jar:na]
	at org.apache.kafka.clients.consumer.internals.LegacyKafkaConsumer.poll(LegacyKafkaConsumer.java:611) ~[kafka-clients-3.8.1.jar:na]
	at org.apache.kafka.clients.consumer.internals.LegacyKafkaConsumer.poll(LegacyKafkaConsumer.java:591) ~[kafka-clients-3.8.1.jar:na]
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:874) ~[kafka-clients-3.8.1.jar:na]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.pollConsumer(KafkaMessageListenerContainer.java:1692) ~[spring-kafka-3.3.4.jar:3.3.4]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doPoll(KafkaMessageListenerContainer.java:1667) ~[spring-kafka-3.3.4.jar:3.3.4]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.pollAndInvoke(KafkaMessageListenerContainer.java:1445) ~[spring-kafka-3.3.4.jar:3.3.4]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.run(KafkaMessageListenerContainer.java:1335) ~[spring-kafka-3.3.4.jar:3.3.4]
	at java.base/java.util.concurrent.CompletableFuture$AsyncRun.run(CompletableFuture.java:1804) ~[na:na]
	at java.base/java.lang.Thread.run(Thread.java:840) ~[na:na]

2025-04-03T23:43:03.345+09:00  INFO 45581 --- [    Test worker] kafka.server.KafkaServer                 : starting
2025-04-03T23:43:03.345+09:00  INFO 45581 --- [    Test worker] kafka.server.KafkaServer                 : Connecting to zookeeper on 127.0.0.1:58604
2025-04-03T23:43:03.354+09:00  INFO 45581 --- [    Test worker] kafka.zookeeper.ZooKeeperClient          : [ZooKeeperClient Kafka server] Initializing a new session to 127.0.0.1:58604.
2025-04-03T23:43:03.356+09:00  INFO 45581 --- [    Test worker] org.apache.zookeeper.ZooKeeper           : Client environment:zookeeper.version=3.8.4-9316c2a7a97e1666d8f4593f34dd6fc36ecc436c, built on 2024-02-12 22:16 UTC
2025-04-03T23:43:03.356+09:00  INFO 45581 --- [    Test worker] org.apache.zookeeper.ZooKeeper           : Client environment:host.name=localhost
2025-04-03T23:43:03.356+09:00  INFO 45581 --- [    Test worker] org.apache.zookeeper.ZooKeeper           : Client environment:java.version=17.0.12
2025-04-03T23:43:03.356+09:00  INFO 45581 --- [    Test worker] org.apache.zookeeper.ZooKeeper           : Client environment:java.vendor=Eclipse Adoptium
2025-04-03T23:43:03.356+09:00  INFO 45581 --- [    Test worker] org.apache.zookeeper.ZooKeeper           : Client environment:java.home=/Library/Java/JavaVirtualMachines/temurin-17.jdk/Contents/Home
2025-04-03T23:43:03.356+09:00  INFO 45581 --- [    Test worker] org.apache.zookeeper.ZooKeeper           : Client environment:java.class.path=/Users/shlee/.gradle/caches/8.13/workerMain/gradle-worker.jar:/Users/shlee/workspaces/study/iseunghan-Lab/spring-embedded-kafka-test/build/classes/kotlin/test:/Users/shlee/workspaces/study/iseunghan-Lab/spring-embedded-kafka-test/build/classes/kotlin/main:/Users/shlee/workspaces/study/iseunghan-Lab/spring-embedded-kafka-test/build/resources/main:/Users/shlee/.gradle/caches/modules-2/files-2.1/org.springframework.boot/spring-boot-starter-web/3.4.4/441c0c71f38783c1d34b8bac5f06f1fa6b103274/spring-boot-starter-web-3.4.4.jar:/Users/shlee/.gradle/caches/modules-2/files-2.1/org.springframework.boot/spring-boot-starter-json/3.4.4/7f7ca72abcab50661b7b5d23f63b3bc2375bb2b3/spring-boot-starter-json-3.4.4.jar:/Users/shlee/.gradle/caches/modules-2/files-2.1/org.springframework.kafka/spring-kafka-test/3.3.4/d355af0a1960bca450aef05c73646edd869a839b/spring-kafka-test-3.3.4.jar:/Users/shlee/.gradle/caches/modules-2/files-2.1/org.apache.kafka/kafka_2.13/3.8.1/832245e638abf7ec3996b76a42944960abe39e23/kafka_2.13-3.8.1.jar:/Users/shlee/.gradle/caches/modules-2/files-2.1/org.apache.kafka/kafka_2.13/3.8.1/6278b0f00854c36ed12c8a016152332bc9d1a609/kafka_2.13-3.8.1-test.jar:/Users/shlee/.gradle/caches/modules-2/files-2.1/org.apache.kafka/kafka-server/3.8.1/9b339e97deeecaf1e7897b4c731d77392b4745de/kafka-server-3.8.1.jar:/Users/shlee/.gradle/caches/modules-2/files-2.1/org.apache.kafka/kafka-group-coordinator/3.8.1/d372d5bfee64939af353d4246cab795884cb477c/kafka-group-coordinator-3.8.1.jar:/Users/shlee/.gradle/caches/modules-2/files-2.1/org.apache.kafka/kafka-metadata/3.8.1/3d6662afa30ed98985422d35485962819253040b/kafka-metadata-3.8.1.jar:/Users/shlee/.gradle/caches/modules-2/files-2.1/org.apache.kafka/kafka-raft/3.8.1/8f44b9f0da4fa1670d8506d33c06ba7662b37045/kafka-raft-3.8.1.jar:/Users/shlee/.gradle/caches/modules-2/files-2.1/org.apache.kafka/kafka-storage/3.8.1/daf35a6f6a1b8da8d2baa754856b12d5ebec7be3/kafka-storage-3.8.1.jar:/Users/shlee/.gradle/caches/modules-2/files-2.1/org.apache.kafka/kafka-storage-api/3.8.1/a522b224b598f30b9637ad2eb3a5db64c14db4a4/kafka-storage-api-3.8.1.jar:/Users/shlee/.gradle/caches/modules-2/files-2.1/org.apache.kafka/kafka-server-common/3.8.1/483d5f635aa0a751946a6bd80b0b3acbdc6b7169/kafka-server-common-3.8.1.jar:/Users/shlee/.gradle/caches/modules-2/files-2.1/org.apache.kafka/kafka-server-common/3.8.1/4ad5b80346a131afa11708238441f16026f0ec5c/kafka-server-common-3.8.1-test.jar:/Users/shlee/.gradle/caches/modules-2/files-2.1/com.fasterxml.jackson.datatype/jackson-datatype-jdk8/2.18.3/621558295660935134b171ce2b0d9ad6842ec2ff/jackson-datatype-jdk8-2.18.3.jar:/Users/shlee/.gradle/caches/modules-2/files-2.1/com.fasterxml.jackson.datatype/jackson-datatype-jsr310/2.18.3/cc57924cccf42fc852081c36215272f84ffcd991/jackson-datatype-jsr310-2.18.3.jar:/Users/shlee/.gradle/caches/modules-2/files-2.1/com.fasterxml.jackson.module/jackson-module-parameter-names/2.18.3/fa63ae5eb3956b1a6d7a7ead2b513af89dea22e7/jackson-module-parameter-names-2.18.3.jar:/Users/shlee/.gradle/caches/modules-2/files-2.1/org.apache.kafka/kafka-streams-test-utils/3.8.1/edc3d933c78754476fc5564f4fc2cb3da060a25a/kafka-streams-test-utils-3.8.1.jar:/Users/shlee/.gradle/caches/modules-2/files-2.1/org.apache.kafka/kafka-streams/3.8.1/9e054a4e7b88bc80f069e22045826013b7b697ec/kafka-streams-3.8.1.jar:/Users/shlee/.gradle/caches/modules-2/files-2.1/com.fasterxml.jackson.module/jackson-module-scala_2.13/2.18.3/654e30ed3c2774c41fb285ded3076b9ff139e6fa/jackson-module-scala_2.13-2.18.3.jar:/Users/shlee/.gradle/caches/modules-2/files-2.1/com.fasterxml.jackson.dataformat/jackson-dataformat-csv/2.18.3/f560c1fca2340a9bdb0d6d5ebd903f9932962613/jackson-dataformat-csv-2.18.3.jar:/Users/shlee/.gradle/caches/modules-2/files-2.1/com.fasterxml.jackson.core/jackson-databind/2.18.3/537e3886263e3b3464385040453e92567fd509e2/jackson-databind-2.18.3.jar:/Users/shlee/.gradle/caches/modules-2/files-2.1/com.fasterxml.jackson.core/jackson-annotations/2.18.3/7fa21cf7da4598f8240e4ebd9779249622af1acd/jackson-annotations-2.18.3.jar:/Users/shlee/.gradle/caches/modules-2/files-2.1/com.fasterxml.jackson.core/jackson-core/2.18.3/78f80c259268200e588aa204dd97ecf09b76916e/jackson-core-2.18.3.jar:/Users/shlee/.gradle/caches/modules-2/files-2.1/com.fasterxml.jackson.module/jackson-module-kotlin/2.18.3/d79c117605fc157ec32ec9774690bb3e493938ce/jackson-module-kotlin-2.18.3.jar:/Users/shlee/.gradle/caches/modules-2/files-2.1/org.jetbrains.kotlin/kotlin-reflect/1.9.25/73023c38b7b20430232893cf9b556dc8486e07a4/kotlin-reflect-1.9.25.jar:/Users/shlee/.gradle/caches/modules-2/files-2.1/org.jetbrains.kotlin/kotlin-test-junit5/1.9.25/787558f7e75d14bac4b6e60421d434e66bd20f3f/kotlin-test-junit5-1.9.25.jar:/Users/shlee/.gradle/caches/modules-2/files-2.1/org.jetbrains.kotlin/kotlin-test/1.9.25/c76017b12cb5cfb74983979149cef7d6c3953e6e/kotlin-test-1.9.25.jar:/Users/shlee/.gradle/caches/modules-2/files-2.1/org.jetbrains.kotlin/kotlin-stdlib/1.9.25/f700a2f2b8f0d6d0fde48f56d894dc722fb029d7/kotlin-stdlib-1.9.25.jar:/Users/shlee/.gradle/caches/modules-2/files-2.1/org.springframework.kafka/spring-kafka/3.3.4/e9a6e9ef5e01379643e612bfde97bea06822341b/spring-kafka-3.3.4.jar:/Users/shlee/.gradle/caches/modules-2/files-2.1/org.springframework.boot/spring-boot-starter-test/3.4.4/4cb1fd0bdc34459bf389df17620e339897d1439c/spring-boot-starter-test-3.4.4.jar:/Users/shlee/.gradle/caches/modules-2/files-2.1/org.junit.jupiter/junit-jupiter/5.11.4/a699f024a4a4706b36bddbeb42d499aff9e09379/junit-jupiter-5.11.4.jar:/Users/shlee/.gradle/caches/modules-2/files-2.1/org.junit.jupiter/junit-jupiter-engine/5.11.4/dc10ec209623986a68ea07f67cdc7d2a65a60355/junit-jupiter-engine-5.11.4.jar:/Users/shlee/.gradle/caches/modules-2/files-2.1/org.mockito/mockito-junit-jupiter/5.14.2/3cfc377d4bb9fe729f3dd9098d9a9b27da58324a/mockito-junit-jupiter-5.14.2.jar:/Users/shlee/.gradle/caches/modules-2/files-2.1/org.junit.jupiter/junit-jupiter-params/5.11.4/e4c86fbe2a39c60c6b87260ef7f7e7c1a1906481/junit-jupiter-params-5.11.4.jar:/Users/shlee/.gradle/caches/modules-2/files-2.1/org.junit.jupiter/junit-jupiter-api/5.11.4/308315b28e667db4091b2ba1f7aa220d1ddadb97/junit-jupiter-api-5.11.4.jar:/Users/shlee/.gradle/caches/modules-2/files-2.1/org.junit.platform/junit-platform-engine/1.11.4/21f61b123ad6ac8f7e73971bff3a096c8d8e1cd0/junit-platform-engine-1.11.4.jar:/Users/shlee/.gradle/caches/modules-2/files-2.1/org.junit.platform/junit-platform-commons/1.11.4/8898eea3ed0da2641548d602c3e308804f166303/junit-platform-commons-1.11.4.jar:/Users/shlee/.gradle/caches/modules-2/files-2.1/org.junit.platform/junit-platform-launcher/1.11.4/3d83c201899d8c5e74e1a5d628eab900342a0e48/junit-platform-launcher-1.11.4.jar:/Users/shlee/.gradle/caches/modules-2/files-2.1/org.jetbrains/annotations/13.0/919f0dfe192fb4e063e7dacadee7f8bb9a2672a9/annotations-13.0.jar:/Users/shlee/.gradle/caches/modules-2/files-2.1/org.springframework.boot/spring-boot-starter/3.4.4/6ad00ebe69a28a5c1c97f80f81920d65e0e4250b/spring-boot-starter-3.4.4.jar:/Users/shlee/.gradle/caches/modules-2/files-2.1/org.springframework.boot/spring-boot-starter-tomcat/3.4.4/4b18ac49cf13ef2c9a98418ebd8fc212a5259da9/spring-boot-starter-tomcat-3.4.4.jar:/Users/shlee/.gradle/caches/modules-2/files-2.1/org.springframework/spring-webmvc/6.2.5/db8f1171041d7091f3de80cffdfb9d6c5fbf3015/spring-webmvc-6.2.5.jar:/Users/shlee/.gradle/caches/modules-2/files-2.1/org.springframework/spring-web/6.2.5/b42d2c0acbe05bad4c849883aa8816c25b6c1d94/spring-web-6.2.5.jar:/Users/shlee/.gradle/caches/modules-2/files-2.1/org.springframework.boot/spring-boot-test-autoconfigure/3.4.4/3d799c8cd92eed0d0624de08fec52ab01a638e51/spring-boot-test-autoconfigure-3.4.4.jar:/Users/shlee/.gradle/caches/modules-2/files-2.1/org.springframework.boot/spring-boot-test/3.4.4/cc93446b14f050793c57ecd74a066ecf9ebac96b/spring-boot-test-3.4.4.jar:/Users/shlee/.gradle/caches/modules-2/files-2.1/org.springframework.boot/spring-boot-autoconfigure/3.4.4/5ebc8c0682374768ee6eac9acf12f41e76762207/spring-boot-autoconfigure-3.4.4.jar:/Users/shlee/.gradle/caches/modules-2/files-2.1/org.springframework.boot/spring-boot/3.4.4/f9dbe14c2e5e35a2cd27156802ea6b7c42ab34fd/spring-boot-3.4.4.jar:/Users/shlee/.gradle/caches/modules-2/files-2.1/org.springframework/spring-context/6.2.5/237de0c3afca2099ab497cc7464726c02b8ab5c5/spring-context-6.2.5.jar:/Users/shlee/.gradle/caches/modules-2/files-2.1/org.springframework/spring-messaging/6.2.5/977fa11f45ce6391b4e1e76830f08b0647fe44a9/spring-messaging-6.2.5.jar:/Users/shlee/.gradle/caches/modules-2/files-2.1/org.springframework/spring-tx/6.2.5/99ecc1c976f58ad63df7205e18eac628f2f1fa9a/spring-tx-6.2.5.jar:/Users/shlee/.gradle/caches/modules-2/files-2.1/org.springframework.retry/spring-retry/2.0.11/bd4fae67445baf330b69b6b786748a308ab31f6/spring-retry-2.0.11.jar:/Users/shlee/.gradle/caches/modules-2/files-2.1/org.apache.kafka/kafka-group-coordinator-api/3.8.1/9f99f11956404ae64a2b210adaf6d1f3850ab26f/kafka-group-coordinator-api-3.8.1.jar:/Users/shlee/.gradle/caches/modules-2/files-2.1/org.apache.kafka/kafka-tools-api/3.8.1/43f69819f286340b5738e562122256b1b2470e99/kafka-tools-api-3.8.1.jar:/Users/shlee/.gradle/caches/modules-2/files-2.1/org.apache.kafka/kafka-clients/3.8.1/fd79e3aa252c6d818334e9c0bac8166b426e498c/kafka-clients-3.8.1.jar:/Users/shlee/.gradle/caches/modules-2/files-2.1/org.apache.kafka/kafka-clients/3.8.1/11d3ffefbc452fc4c5d45f4f6ec368bcab290a95/kafka-clients-3.8.1-test.jar:/Users/shlee/.gradle/caches/modules-2/files-2.1/io.micrometer/micrometer-observation/1.14.5/b23dff6bf07a29f67fdae8f3f3f8f1c78fa7b126/micrometer-observation-1.14.5.jar:/Users/shlee/.gradle/caches/modules-2/files-2.1/com.jayway.jsonpath/json-path/2.9.0/37fe2217f577b0b68b18e62c4d17a8858ecf9b69/json-path-2.9.0.jar:/Users/shlee/.gradle/caches/modules-2/files-2.1/jakarta.xml.bind/jakarta.xml.bind-api/4.0.2/6cd5a999b834b63238005b7144136379dc36cad2/jakarta.xml.bind-api-4.0.2.jar:/Users/shlee/.gradle/caches/modules-2/files-2.1/net.minidev/json-smart/2.5.2/95d166b18f95907be0f46cdb9e1c0695eed03387/json-smart-2.5.2.jar:/Users/shlee/.gradle/caches/modules-2/files-2.1/org.assertj/assertj-core/3.26.3/d26263eb7524252d98e602fc6942996a3195e29/assertj-core-3.26.3.jar:/Users/shlee/.gradle/caches/modules-2/files-2.1/org.awaitility/awaitility/4.2.2/7336242073ebf83fe034e42b46a403c5501b63c9/awaitility-4.2.2.jar:/Users/shlee/.gradle/caches/modules-2/files-2.1/org.hamcrest/hamcrest/2.2/1820c0968dba3a11a1b30669bb1f01978a91dedc/hamcrest-2.2.jar:/Users/shlee/.gradle/caches/modules-2/files-2.1/org.mockito/mockito-core/5.14.2/f7bf936008d7664e2002c3faf0c02071c8d10e7c/mockito-core-5.14.2.jar:/Users/shlee/.gradle/caches/modules-2/files-2.1/org.skyscreamer/jsonassert/1.5.3/aaa43e0823d2a0e106e8754d6a9c4ab24e05e9bc/jsonassert-1.5.3.jar:/Users/shlee/.gradle/caches/modules-2/files-2.1/org.springframework/spring-test/6.2.5/4db580c1808ae17efab33dee218320a66adfade6/spring-test-6.2.5.jar:/Users/shlee/.gradle/caches/modules-2/files-2.1/org.springframework/spring-aop/6.2.5/9f436be65bf45ee8643d93e6823d6c81e8f9f91a/spring-aop-6.2.5.jar:/Users/shlee/.gradle/caches/modules-2/files-2.1/org.springframework/spring-beans/6.2.5/6f8eb671d9905da5a73bc9ab7e703e9fed6a0c3f/spring-beans-6.2.5.jar:/Users/shlee/.gradle/caches/modules-2/files-2.1/org.springframework/spring-expression/6.2.5/cb39181911dabe3b3d9992c4b4da38468726ea72/spring-expression-6.2.5.jar:/Users/shlee/.gradle/caches/modules-2/files-2.1/org.springframework/spring-core/6.2.5/d6786db7122037bf605e54e3b35f262a19b8d502/spring-core-6.2.5.jar:/Users/shlee/.gradle/caches/modules-2/files-2.1/org.xmlunit/xmlunit-core/2.10.0/1355088731b4ec2107ff7f319f0d7445d916bab/xmlunit-core-2.10.0.jar:/Users/shlee/.gradle/caches/modules-2/files-2.1/org.apache.zookeeper/zookeeper/3.8.4/6638e37b887b5a279044afbdc9928e19f678eb2e/zookeeper-3.8.4.jar:/Users/shlee/.gradle/caches/modules-2/files-2.1/org.springframework.boot/spring-boot-starter-logging/3.4.4/1a7bbfd57e0b2d0bde8ffd6aa7a0ebd94e4b0aed/spring-boot-starter-logging-3.4.4.jar:/Users/shlee/.gradle/caches/modules-2/files-2.1/jakarta.annotation/jakarta.annotation-api/2.1.1/48b9bda22b091b1f48b13af03fe36db3be6e1ae3/jakarta.annotation-api-2.1.1.jar:/Users/shlee/.gradle/caches/modules-2/files-2.1/org.yaml/snakeyaml/2.3/936b36210e27320f920536f695cf1af210c44586/snakeyaml-2.3.jar:/Users/shlee/.gradle/caches/modules-2/files-2.1/org.apache.tomcat.embed/tomcat-embed-websocket/10.1.39/abf50abbbae651f95dfcf9fae05f919bfa9b7046/tomcat-embed-websocket-10.1.39.jar:/Users/shlee/.gradle/caches/modules-2/files-2.1/org.apache.tomcat.embed/tomcat-embed-core/10.1.39/f6acead04214d5aaea82c2639392208df33b3abe/tomcat-embed-core-10.1.39.jar:/Users/shlee/.gradle/caches/modules-2/files-2.1/org.apache.tomcat.embed/tomcat-embed-el/10.1.39/88ff05c768c4654097ab0a9f2b49368a0877ad76/tomcat-embed-el-10.1.39.jar:/Users/shlee/.gradle/caches/modules-2/files-2.1/com.github.luben/zstd-jni/1.5.6-4/ba9e303e0b5e94cdd0017390d7d8c06f47fd61f7/zstd-jni-1.5.6-4.jar:/Users/shlee/.gradle/caches/modules-2/files-2.1/org.lz4/lz4-java/1.8.0/4b986a99445e49ea5fbf5d149c4b63f6ed6c6780/lz4-java-1.8.0.jar:/Users/shlee/.gradle/caches/modules-2/files-2.1/org.xerial.snappy/snappy-java/1.1.10.5/ac605269f3598506196e469f1fb0d7ed5c55059e/snappy-java-1.1.10.5.jar:/Users/shlee/.gradle/caches/modules-2/files-2.1/ch.qos.logback/logback-classic/1.5.18/fc371f3fc97a639de2d67947cffb7518ec5e3d40/logback-classic-1.5.18.jar:/Users/shlee/.gradle/caches/modules-2/files-2.1/com.yammer.metrics/metrics-core/2.2.0/f82c035cfa786d3cbec362c38c22a5f5b1bc8724/metrics-core-2.2.0.jar:/Users/shlee/.gradle/caches/modules-2/files-2.1/org.bitbucket.b_c/jose4j/0.9.4/7efe6ccf593e52a2b77f98de52238f15b4a67188/jose4j-0.9.4.jar:/Users/shlee/.gradle/caches/modules-2/files-2.1/com.typesafe.scala-logging/scala-logging_2.13/3.9.4/e5838ddcf8c1c2b612a7956b9f80423a508675db/scala-logging_2.13-3.9.4.jar:/Users/shlee/.gradle/caches/modules-2/files-2.1/io.dropwizard.metrics/metrics-core/4.1.12.1/cb2f351bf4463751201f43bb99865235d5ba07ca/metrics-core-4.1.12.1.jar:/Users/shlee/.gradle/caches/modules-2/files-2.1/org.apache.logging.log4j/log4j-to-slf4j/2.24.3/da1143e2a2531ee1c2d90baa98eb50a28a39d5a7/log4j-to-slf4j-2.24.3.jar:/Users/shlee/.gradle/caches/modules-2/files-2.1/org.slf4j/jul-to-slf4j/2.0.17/524cb6ccc2b68a57604750e1ab8b13b5a786a6aa/jul-to-slf4j-2.0.17.jar:/Users/shlee/.gradle/caches/modules-2/files-2.1/org.slf4j/slf4j-api/2.0.17/d9e58ac9c7779ba3bf8142aff6c830617a7fe60f/slf4j-api-2.0.17.jar:/Users/shlee/.gradle/caches/modules-2/files-2.1/io.micrometer/micrometer-commons/1.14.5/6201a40489ccedc9539c5f7a2c84e9e64702bf10/micrometer-commons-1.14.5.jar:/Users/shlee/.gradle/caches/modules-2/files-2.1/jakarta.activation/jakarta.activation-api/2.1.3/fa165bd70cda600368eee31555222776a46b881f/jakarta.activation-api-2.1.3.jar:/Users/shlee/.gradle/caches/modules-2/files-2.1/net.minidev/accessors-smart/2.5.2/ce16fd235cfee48e67eda33e684423bba09f7d07/accessors-smart-2.5.2.jar:/Users/shlee/.gradle/caches/modules-2/files-2.1/net.bytebuddy/byte-buddy/1.15.11/f61886478e0f9ee4c21d09574736f0ff45e0a46c/byte-buddy-1.15.11.jar:/Users/shlee/.gradle/caches/modules-2/files-2.1/net.bytebuddy/byte-buddy-agent/1.15.11/a38b16385e867f59a641330f0362ebe742788ed8/byte-buddy-agent-1.15.11.jar:/Users/shlee/.gradle/caches/modules-2/files-2.1/org.objenesis/objenesis/3.3/1049c09f1de4331e8193e579448d0916d75b7631/objenesis-3.3.jar:/Users/shlee/.gradle/caches/modules-2/files-2.1/com.vaadin.external.google/android-json/0.0.20131108.vaadin1/fa26d351fe62a6a17f5cda1287c1c6110dec413f/android-json-0.0.20131108.vaadin1.jar:/Users/shlee/.gradle/caches/modules-2/files-2.1/org.springframework/spring-jcl/6.2.5/1b8db3715dc78bd18b60869458d5ee8829ad8e99/spring-jcl-6.2.5.jar:/Users/shlee/.gradle/caches/modules-2/files-2.1/org.opentest4j/opentest4j/1.3.0/152ea56b3a72f655d4fd677fc0ef2596c3dd5e6e/opentest4j-1.3.0.jar:/Users/shlee/.gradle/caches/modules-2/files-2.1/org.apache.zookeeper/zookeeper-jute/3.8.4/de7b8a41bbe1ccdfc009de51fa6d160db3ca8025/zookeeper-jute-3.8.4.jar:/Users/shlee/.gradle/caches/modules-2/files-2.1/org.apache.yetus/audience-annotations/0.12.0/e0efa60318229590103e31c69ebdaae56d903644/audience-annotations-0.12.0.jar:/Users/shlee/.gradle/caches/modules-2/files-2.1/io.netty/netty-handler/4.1.119.Final/a0059b8d779ce566b524efd6d73ba4fcff3cd5d9/netty-handler-4.1.119.Final.jar:/Users/shlee/.gradle/caches/modules-2/files-2.1/io.netty/netty-transport-native-epoll/4.1.119.Final/c4b9339262b59f4808674d3b4ba4d5c4846921df/netty-transport-native-epoll-4.1.119.Final.jar:/Users/shlee/.gradle/caches/modules-2/files-2.1/ch.qos.logback/logback-core/1.5.18/6c0375624f6f36b4e089e2488ba21334a11ef13f/logback-core-1.5.18.jar:/Users/shlee/.gradle/caches/modules-2/files-2.1/commons-io/commons-io/2.14.0/a4c6e1f6c196339473cd2e1b037f0eb97c62755b/commons-io-2.14.0.jar:/Users/shlee/.gradle/caches/modules-2/files-2.1/org.apache.kafka/kafka-transaction-coordinator/3.8.1/3c2676a95a416f3d5a59a498c4553914be1d332d/kafka-transaction-coordinator-3.8.1.jar:/Users/shlee/.gradle/caches/modules-2/files-2.1/net.sf.jopt-simple/jopt-simple/5.0.4/4fdac2fbe92dfad86aa6e9301736f6b4342a3f5c/jopt-simple-5.0.4.jar:/Users/shlee/.gradle/caches/modules-2/files-2.1/org.pcollections/pcollections/4.0.1/59f3bf5fb28c5f5386804dcf129267416b75d7c/pcollections-4.0.1.jar:/Users/shlee/.gradle/caches/modules-2/files-2.1/net.sourceforge.argparse4j/argparse4j/0.7.0/6f0621d0c3888de39e0f06d01f37ba53a798e657/argparse4j-0.7.0.jar:/Users/shlee/.gradle/caches/modules-2/files-2.1/commons-validator/commons-validator/1.7/76069c915de3787f3ddd8726a56f47a95bfcbb0e/commons-validator-1.7.jar:/Users/shlee/.gradle/caches/modules-2/files-2.1/org.scala-lang.modules/scala-collection-compat_2.13/2.10.0/2464528d14329ff2b44bceff918d6c00300adcf4/scala-collection-compat_2.13-2.10.0.jar:/Users/shlee/.gradle/caches/modules-2/files-2.1/org.scala-lang.modules/scala-java8-compat_2.13/1.0.2/5b54f60ccf03e41e24c466462a72475ee918c304/scala-java8-compat_2.13-1.0.2.jar:/Users/shlee/.gradle/caches/modules-2/files-2.1/org.scala-lang/scala-reflect/2.13.14/8e275fefb2a01e178db2cdfebb2181062a790b82/scala-reflect-2.13.14.jar:/Users/shlee/.gradle/caches/modules-2/files-2.1/commons-cli/commons-cli/1.4/c51c00206bb913cd8612b24abd9fa98ae89719b1/commons-cli-1.4.jar:/Users/shlee/.gradle/caches/modules-2/files-2.1/org.scala-lang/scala-library/2.13.15/ed6f1d58968b16c5f9067d5cac032d952552de58/scala-library-2.13.15.jar:/Users/shlee/.gradle/caches/modules-2/files-2.1/org.ow2.asm/asm/9.7.1/f0ed132a49244b042cd0e15702ab9f2ce3cc8436/asm-9.7.1.jar:/Users/shlee/.gradle/caches/modules-2/files-2.1/io.netty/netty-transport-classes-epoll/4.1.119.Final/1d2d986ae0615d50f5cee338f0e9e73f46e8158/netty-transport-classes-epoll-4.1.119.Final.jar:/Users/shlee/.gradle/caches/modules-2/files-2.1/io.netty/netty-transport-native-unix-common/4.1.119.Final/55afdeb456bccf8eecb06431f3c1537269afe9af/netty-transport-native-unix-common-4.1.119.Final.jar:/Users/shlee/.gradle/caches/modules-2/files-2.1/io.netty/netty-codec/4.1.119.Final/337ca8e8c3ef23925e02d56347b414d7616d1d02/netty-codec-4.1.119.Final.jar:/Users/shlee/.gradle/caches/modules-2/files-2.1/io.netty/netty-transport/4.1.119.Final/d05df879054297962d056b77460fc4ff20e30073/netty-transport-4.1.119.Final.jar:/Users/shlee/.gradle/caches/modules-2/files-2.1/io.netty/netty-resolver/4.1.119.Final/e1f7c90aff71bdf0e2294a200b5f89244f88dcf7/netty-resolver-4.1.119.Final.jar:/Users/shlee/.gradle/caches/modules-2/files-2.1/io.netty/netty-buffer/4.1.119.Final/5864149b2e5a2af7b05fe9e9dc3e4b70d3060a7c/netty-buffer-4.1.119.Final.jar:/Users/shlee/.gradle/caches/modules-2/files-2.1/io.netty/netty-common/4.1.119.Final/2f7c360b03c0aceab7efc1f7c2b75274f0f35909/netty-common-4.1.119.Final.jar:/Users/shlee/.gradle/caches/modules-2/files-2.1/org.rocksdb/rocksdbjni/7.9.2/6409b667493149191b09fe1fce94bada6096a3e9/rocksdbjni-7.9.2.jar:/Users/shlee/.gradle/caches/modules-2/files-2.1/com.github.ben-manes.caffeine/caffeine/3.1.8/24795585df8afaf70a2cd534786904ea5889c047/caffeine-3.1.8.jar:/Users/shlee/.gradle/caches/modules-2/files-2.1/commons-beanutils/commons-beanutils/1.9.4/d52b9abcd97f38c81342bb7e7ae1eee9b73cba51/commons-beanutils-1.9.4.jar:/Users/shlee/.gradle/caches/modules-2/files-2.1/commons-digester/commons-digester/2.1/73a8001e7a54a255eef0f03521ec1805dc738ca0/commons-digester-2.1.jar:/Users/shlee/.gradle/caches/modules-2/files-2.1/commons-collections/commons-collections/3.2.2/8ad72fe39fa8c91eaaf12aadb21e0c3661fe26d5/commons-collections-3.2.2.jar:/Users/shlee/.gradle/caches/modules-2/files-2.1/org.apache.logging.log4j/log4j-api/2.24.3/b02c125db8b6d295adf72ae6e71af5d83bce2370/log4j-api-2.24.3.jar:/Users/shlee/.gradle/caches/modules-2/files-2.1/com.thoughtworks.paranamer/paranamer/2.8/619eba74c19ccf1da8ebec97a2d7f8ba05773dd6/paranamer-2.8.jar:/Users/shlee/.gradle/caches/modules-2/files-2.1/org.checkerframework/checker-qual/3.37.0/ba74746d38026581c12166e164bb3c15e90cc4ea/checker-qual-3.37.0.jar:/Users/shlee/.gradle/caches/modules-2/files-2.1/com.google.errorprone/error_prone_annotations/2.21.1/6d9b10773b5237df178a7b3c1b4208df7d0e7f94/error_prone_annotations-2.21.1.jar
2025-04-03T23:43:03.356+09:00  INFO 45581 --- [    Test worker] org.apache.zookeeper.ZooKeeper           : Client environment:java.library.path=/Users/shlee/Library/Java/Extensions:/Library/Java/Extensions:/Network/Library/Java/Extensions:/System/Library/Java/Extensions:/usr/lib/java:.
2025-04-03T23:43:03.356+09:00  INFO 45581 --- [    Test worker] org.apache.zookeeper.ZooKeeper           : Client environment:java.io.tmpdir=/var/folders/qb/nxyw0kj549bbf87mx3ss8vgc0000gn/T/
2025-04-03T23:43:03.356+09:00  INFO 45581 --- [    Test worker] org.apache.zookeeper.ZooKeeper           : Client environment:java.compiler=<NA>
2025-04-03T23:43:03.356+09:00  INFO 45581 --- [    Test worker] org.apache.zookeeper.ZooKeeper           : Client environment:os.name=Mac OS X
2025-04-03T23:43:03.356+09:00  INFO 45581 --- [    Test worker] org.apache.zookeeper.ZooKeeper           : Client environment:os.arch=aarch64
2025-04-03T23:43:03.356+09:00  INFO 45581 --- [    Test worker] org.apache.zookeeper.ZooKeeper           : Client environment:os.version=15.3.2
2025-04-03T23:43:03.356+09:00  INFO 45581 --- [    Test worker] org.apache.zookeeper.ZooKeeper           : Client environment:user.name=shlee
2025-04-03T23:43:03.356+09:00  INFO 45581 --- [    Test worker] org.apache.zookeeper.ZooKeeper           : Client environment:user.home=/Users/shlee
2025-04-03T23:43:03.356+09:00  INFO 45581 --- [    Test worker] org.apache.zookeeper.ZooKeeper           : Client environment:user.dir=/Users/shlee/workspaces/study/iseunghan-Lab/spring-embedded-kafka-test
2025-04-03T23:43:03.356+09:00  INFO 45581 --- [    Test worker] org.apache.zookeeper.ZooKeeper           : Client environment:os.memory.free=24MB
2025-04-03T23:43:03.356+09:00  INFO 45581 --- [    Test worker] org.apache.zookeeper.ZooKeeper           : Client environment:os.memory.max=512MB
2025-04-03T23:43:03.356+09:00  INFO 45581 --- [    Test worker] org.apache.zookeeper.ZooKeeper           : Client environment:os.memory.total=67MB
2025-04-03T23:43:03.358+09:00  INFO 45581 --- [    Test worker] org.apache.zookeeper.ZooKeeper           : Initiating client connection, connectString=127.0.0.1:58604 sessionTimeout=18000 watcher=kafka.zookeeper.ZooKeeperClient$ZooKeeperClientWatcher$@723c5f86
2025-04-03T23:43:03.358+09:00  INFO 45581 --- [    Test worker] org.apache.zookeeper.ClientCnxnSocket    : jute.maxbuffer value is 4194304 Bytes
2025-04-03T23:43:03.363+09:00  INFO 45581 --- [    Test worker] org.apache.zookeeper.ClientCnxn          : zookeeper.request.timeout value is 0. feature enabled=false
2025-04-03T23:43:03.363+09:00  INFO 45581 --- [27.0.0.1:58604)] org.apache.zookeeper.ClientCnxn          : Opening socket connection to server /127.0.0.1:58604.
2025-04-03T23:43:03.364+09:00  INFO 45581 --- [    Test worker] kafka.zookeeper.ZooKeeperClient          : [ZooKeeperClient Kafka server] Waiting until connected.
2025-04-03T23:43:03.364+09:00  INFO 45581 --- [27.0.0.1:58604)] org.apache.zookeeper.ClientCnxn          : Socket connection established, initiating session, client: /127.0.0.1:58605, server: /127.0.0.1:58604
2025-04-03T23:43:03.369+09:00  INFO 45581 --- [   SyncThread:0] o.a.z.server.persistence.FileTxnLog      : Creating new log file: log.1
2025-04-03T23:43:03.371+09:00  INFO 45581 --- [   SyncThread:0] o.a.zookeeper.audit.ZKAuditProvider      : ZooKeeper audit is disabled.
2025-04-03T23:43:03.372+09:00  INFO 45581 --- [27.0.0.1:58604)] org.apache.zookeeper.ClientCnxn          : Session establishment complete on server /127.0.0.1:58604, session id = 0x100090a96600000, negotiated timeout = 16000
2025-04-03T23:43:03.374+09:00  INFO 45581 --- [    Test worker] kafka.zookeeper.ZooKeeperClient          : [ZooKeeperClient Kafka server] Connected.
2025-04-03T23:43:03.443+09:00  INFO 45581 --- [    Test worker] kafka.server.KafkaServer                 : Cluster ID = -VPPvggYRSiyrN8wldWTNg
2025-04-03T23:43:03.445+09:00  INFO 45581 --- [    Test worker] o.a.k.s.l.r.s.RemoteLogManagerConfig     : RemoteLogManagerConfig values:
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false

2025-04-03T23:43:03.460+09:00  INFO 45581 --- [    Test worker] o.a.k.s.l.r.s.RemoteLogManagerConfig     : RemoteLogManagerConfig values:
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false

2025-04-03T23:43:03.462+09:00  INFO 45581 --- [    Test worker] kafka.server.KafkaConfig                 : KafkaConfig values:
	advertised.listeners = null
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name =
	auto.create.topics.enable = true
	auto.include.jmx.reporter = true
	auto.leader.rebalance.enable = true
	background.threads = 2
	broker.heartbeat.interval.ms = 2000
	broker.id = 0
	broker.id.generation.enable = true
	broker.rack = null
	broker.session.timeout.ms = 9000
	client.quota.callback.class = null
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = producer
	compression.zstd.level = 3
	connection.failed.authentication.delay.ms = 100
	connections.max.idle.ms = 600000
	connections.max.reauth.ms = 0
	control.plane.listener.name = null
	controlled.shutdown.enable = false
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 100
	controller.listener.names = null
	controller.quorum.append.linger.ms = 25
	controller.quorum.bootstrap.servers = []
	controller.quorum.election.backoff.max.ms = 1000
	controller.quorum.election.timeout.ms = 1000
	controller.quorum.fetch.timeout.ms = 2000
	controller.quorum.request.timeout.ms = 2000
	controller.quorum.retry.backoff.ms = 20
	controller.quorum.voters = []
	controller.quota.window.num = 11
	controller.quota.window.size.seconds = 1
	controller.socket.timeout.ms = 1000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delegation.token.secret.key = null
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	early.start.listeners = null
	eligible.leader.replicas.enable = false
	fetch.max.bytes = 57671680
	fetch.purgatory.purge.interval.requests = 1000
	group.consumer.assignors = [org.apache.kafka.coordinator.group.assignor.UniformAssignor, org.apache.kafka.coordinator.group.assignor.RangeAssignor]
	group.consumer.heartbeat.interval.ms = 5000
	group.consumer.max.heartbeat.interval.ms = 15000
	group.consumer.max.session.timeout.ms = 60000
	group.consumer.max.size = 2147483647
	group.consumer.migration.policy = disabled
	group.consumer.min.heartbeat.interval.ms = 5000
	group.consumer.min.session.timeout.ms = 45000
	group.consumer.session.timeout.ms = 45000
	group.coordinator.append.linger.ms = 10
	group.coordinator.new.enable = false
	group.coordinator.rebalance.protocols = [classic]
	group.coordinator.threads = 1
	group.initial.rebalance.delay.ms = 0
	group.max.session.timeout.ms = 1800000
	group.max.size = 2147483647
	group.min.session.timeout.ms = 6000
	initial.broker.registration.timeout.ms = 60000
	inter.broker.listener.name = null
	inter.broker.protocol.version = 3.8-IV0
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = SASL_SSL:SASL_SSL,PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT
	listeners = PLAINTEXT://localhost:0
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 2097152
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.max.compaction.lag.ms = 9223372036854775807
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /var/folders/qb/nxyw0kj549bbf87mx3ss8vgc0000gn/T/spring.kafka.0f9f4ec2-491a-4eb8-ba3a-35f677f610e66802667832673744764
	log.dir.failure.timeout.ms = 30000
	log.dirs = null
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.initial.task.delay.ms = 30000
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	log.message.downconversion.enable = true
	log.message.format.version = 3.0-IV1
	log.message.timestamp.after.max.ms = 9223372036854775807
	log.message.timestamp.before.max.ms = 9223372036854775807
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 1000
	max.connection.creation.rate = 2147483647
	max.connections = 2147483647
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides =
	max.incremental.fetch.session.cache.slots = 1000
	max.request.partition.size.limit = 2000
	message.max.bytes = 1048588
	metadata.log.dir = null
	metadata.log.max.record.bytes.between.snapshots = 20971520
	metadata.log.max.snapshot.interval.ms = 3600000
	metadata.log.segment.bytes = 1073741824
	metadata.log.segment.min.bytes = 8388608
	metadata.log.segment.ms = 604800000
	metadata.max.idle.interval.ms = 500
	metadata.max.retention.bytes = 104857600
	metadata.max.retention.ms = 604800000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	node.id = 0
	num.io.threads = 8
	num.network.threads = 2
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 5
	offsets.topic.replication.factor = 1
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	principal.builder.class = class org.apache.kafka.common.security.authenticator.DefaultKafkaPrincipalBuilder
	process.roles = []
	producer.id.expiration.check.interval.ms = 600000
	producer.id.expiration.ms = 86400000
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.window.num = 11
	quota.window.size.seconds = 1
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 9223372036854775807
	replica.lag.time.max.ms = 30000
	replica.selector.class = null
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 1000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism.controller.protocol = GSSAPI
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	sasl.server.callback.handler.class = null
	sasl.server.max.receive.size = 524288
	security.inter.broker.protocol = PLAINTEXT
	security.providers = null
	server.max.startup.time.ms = 9223372036854775807
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	socket.listen.backlog.size = 50
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.allow.dn.changes = false
	ssl.allow.san.changes = false
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.principal.mapping.rules = DEFAULT
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	telemetry.max.bytes = 1048576
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 10000
	transaction.max.timeout.ms = 900000
	transaction.partition.verification.enable = true
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 2
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 1
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	unstable.api.versions.enable = false
	unstable.feature.versions.enable = true
	zookeeper.clientCnxnSocket = null
	zookeeper.connect = 127.0.0.1:58604
	zookeeper.connection.timeout.ms = 10000
	zookeeper.max.in.flight.requests = 10
	zookeeper.metadata.migration.enable = false
	zookeeper.metadata.migration.min.batch.size = 200
	zookeeper.session.timeout.ms = 18000
	zookeeper.set.acl = false
	zookeeper.ssl.cipher.suites = null
	zookeeper.ssl.client.enable = false
	zookeeper.ssl.crl.enable = false
	zookeeper.ssl.enabled.protocols = null
	zookeeper.ssl.endpoint.identification.algorithm = HTTPS
	zookeeper.ssl.keystore.location = null
	zookeeper.ssl.keystore.password = null
	zookeeper.ssl.keystore.type = null
	zookeeper.ssl.ocsp.enable = false
	zookeeper.ssl.protocol = TLSv1.2
	zookeeper.ssl.truststore.location = null
	zookeeper.ssl.truststore.password = null
	zookeeper.ssl.truststore.type = null

2025-04-03T23:43:03.463+09:00  INFO 45581 --- [    Test worker] o.a.k.s.l.r.s.RemoteLogManagerConfig     : RemoteLogManagerConfig values:
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false

2025-04-03T23:43:03.473+09:00  INFO 45581 --- [nelReaper-Fetch] lientQuotaManager$ThrottledChannelReaper : [ThrottledChannelReaper-Fetch]: Starting
2025-04-03T23:43:03.474+09:00  INFO 45581 --- [lReaper-Produce] lientQuotaManager$ThrottledChannelReaper : [ThrottledChannelReaper-Produce]: Starting
2025-04-03T23:43:03.474+09:00  INFO 45581 --- [lReaper-Request] lientQuotaManager$ThrottledChannelReaper : [ThrottledChannelReaper-Request]: Starting
2025-04-03T23:43:03.475+09:00  INFO 45581 --- [trollerMutation] lientQuotaManager$ThrottledChannelReaper : [ThrottledChannelReaper-ControllerMutation]: Starting
2025-04-03T23:43:03.477+09:00  INFO 45581 --- [    Test worker] kafka.server.KafkaServer                 : [KafkaServer id=0] Rewriting /var/folders/qb/nxyw0kj549bbf87mx3ss8vgc0000gn/T/spring.kafka.0f9f4ec2-491a-4eb8-ba3a-35f677f610e66802667832673744764/meta.properties
2025-04-03T23:43:03.496+09:00  INFO 45581 --- [    Test worker] kafka.log.LogManager                     : Loading logs from log dirs ArrayBuffer(/var/folders/qb/nxyw0kj549bbf87mx3ss8vgc0000gn/T/spring.kafka.0f9f4ec2-491a-4eb8-ba3a-35f677f610e66802667832673744764)
2025-04-03T23:43:03.498+09:00  INFO 45581 --- [    Test worker] kafka.log.LogManager                     : No logs found to be loaded in /var/folders/qb/nxyw0kj549bbf87mx3ss8vgc0000gn/T/spring.kafka.0f9f4ec2-491a-4eb8-ba3a-35f677f610e66802667832673744764
2025-04-03T23:43:03.502+09:00  INFO 45581 --- [    Test worker] kafka.log.LogManager                     : Loaded 0 logs in 6ms
2025-04-03T23:43:03.503+09:00  INFO 45581 --- [    Test worker] kafka.log.LogManager                     : Starting log cleanup with a period of 300000 ms.
2025-04-03T23:43:03.503+09:00  INFO 45581 --- [    Test worker] kafka.log.LogManager                     : Starting log flusher with a default period of 9223372036854775807 ms.
2025-04-03T23:43:03.507+09:00  INFO 45581 --- [    Test worker] kafka.log.LogCleaner                     : Starting the log cleaner
2025-04-03T23:43:03.511+09:00  INFO 45581 --- [leaner-thread-0] kafka.log.LogCleaner$CleanerThread       : [kafka-log-cleaner-thread-0]: Starting
2025-04-03T23:43:03.517+09:00  INFO 45581 --- [-process-thread] stener$ChangeNotificationProcessorThread : [feature-zk-node-event-process-thread]: Starting
2025-04-03T23:43:03.520+09:00  INFO 45581 --- [-process-thread] k.server.FinalizedFeatureChangeListener  : Feature ZK node at path: /feature does not exist
2025-04-03T23:43:03.523+09:00  INFO 45581 --- [channel-manager] k.server.NodeToControllerRequestThread   : [zk-broker-0-to-controller-forwarding-channel-manager]: Starting
2025-04-03T23:43:03.528+09:00  WARN 45581 --- [ntainer#0-0-C-1] org.apache.kafka.clients.NetworkClient   : [Consumer clientId=consumer-my-group-1, groupId=my-group] Error connecting to node logs-service-kafka-1:9092 (id: 1 rack: null)

java.net.UnknownHostException: logs-service-kafka-1
	at java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801) ~[na:na]
	at java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1533) ~[na:na]
	at java.base/java.net.InetAddress.getAllByName(InetAddress.java:1385) ~[na:na]
	at java.base/java.net.InetAddress.getAllByName(InetAddress.java:1306) ~[na:na]
	at org.apache.kafka.clients.DefaultHostResolver.resolve(DefaultHostResolver.java:27) ~[kafka-clients-3.8.1.jar:na]
	at org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:124) ~[kafka-clients-3.8.1.jar:na]
	at org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.resolveAddresses(ClusterConnectionStates.java:536) ~[kafka-clients-3.8.1.jar:na]
	at org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:511) ~[kafka-clients-3.8.1.jar:na]
	at org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:466) ~[kafka-clients-3.8.1.jar:na]
	at org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:173) ~[kafka-clients-3.8.1.jar:na]
	at org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:1070) ~[kafka-clients-3.8.1.jar:na]
	at org.apache.kafka.clients.NetworkClient.access$800(NetworkClient.java:76) ~[kafka-clients-3.8.1.jar:na]
	at org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1259) ~[kafka-clients-3.8.1.jar:na]
	at org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1159) ~[kafka-clients-3.8.1.jar:na]
	at org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:592) ~[kafka-clients-3.8.1.jar:na]
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:281) ~[kafka-clients-3.8.1.jar:na]
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:252) ~[kafka-clients-3.8.1.jar:na]
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:243) ~[kafka-clients-3.8.1.jar:na]
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:165) ~[kafka-clients-3.8.1.jar:na]
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:302) ~[kafka-clients-3.8.1.jar:na]
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:263) ~[kafka-clients-3.8.1.jar:na]
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.coordinatorUnknownAndUnreadySync(ConsumerCoordinator.java:450) ~[kafka-clients-3.8.1.jar:na]
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:482) ~[kafka-clients-3.8.1.jar:na]
	at org.apache.kafka.clients.consumer.internals.LegacyKafkaConsumer.updateAssignmentMetadataIfNeeded(LegacyKafkaConsumer.java:652) ~[kafka-clients-3.8.1.jar:na]
	at org.apache.kafka.clients.consumer.internals.LegacyKafkaConsumer.poll(LegacyKafkaConsumer.java:611) ~[kafka-clients-3.8.1.jar:na]
	at org.apache.kafka.clients.consumer.internals.LegacyKafkaConsumer.poll(LegacyKafkaConsumer.java:591) ~[kafka-clients-3.8.1.jar:na]
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:874) ~[kafka-clients-3.8.1.jar:na]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.pollConsumer(KafkaMessageListenerContainer.java:1692) ~[spring-kafka-3.3.4.jar:3.3.4]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doPoll(KafkaMessageListenerContainer.java:1667) ~[spring-kafka-3.3.4.jar:3.3.4]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.pollAndInvoke(KafkaMessageListenerContainer.java:1445) ~[spring-kafka-3.3.4.jar:3.3.4]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.run(KafkaMessageListenerContainer.java:1335) ~[spring-kafka-3.3.4.jar:3.3.4]
	at java.base/java.util.concurrent.CompletableFuture$AsyncRun.run(CompletableFuture.java:1804) ~[na:na]
	at java.base/java.lang.Thread.run(Thread.java:840) ~[na:na]

2025-04-03T23:43:03.551+09:00  INFO 45581 --- [    Test worker] kafka.network.ConnectionQuotas           : Updated connection-accept-rate max connection creation rate to 2147483647
2025-04-03T23:43:03.552+09:00  INFO 45581 --- [    Test worker] kafka.network.DataPlaneAcceptor          : Awaiting socket connections on localhost:58606.
2025-04-03T23:43:03.552+09:00  INFO 45581 --- [    Test worker] kafka.network.DataPlaneAcceptor          : Opened wildcard endpoint localhost:58606
2025-04-03T23:43:03.557+09:00  INFO 45581 --- [    Test worker] kafka.network.SocketServer               : [SocketServer listenerType=ZK_BROKER, nodeId=0] Created data-plane acceptor and processors for endpoint : ListenerName(PLAINTEXT)
2025-04-03T23:43:03.559+09:00  INFO 45581 --- [channel-manager] k.server.NodeToControllerRequestThread   : [zk-broker-0-to-controller-alter-partition-channel-manager]: Starting
2025-04-03T23:43:03.570+09:00  INFO 45581 --- [eaper-0-Produce] perationPurgatory$ExpiredOperationReaper : [ExpirationReaper-0-Produce]: Starting
2025-04-03T23:43:03.570+09:00  INFO 45581 --- [nReaper-0-Fetch] perationPurgatory$ExpiredOperationReaper : [ExpirationReaper-0-Fetch]: Starting
2025-04-03T23:43:03.570+09:00  INFO 45581 --- [0-DeleteRecords] perationPurgatory$ExpiredOperationReaper : [ExpirationReaper-0-DeleteRecords]: Starting
2025-04-03T23:43:03.571+09:00  INFO 45581 --- [r-0-ElectLeader] perationPurgatory$ExpiredOperationReaper : [ExpirationReaper-0-ElectLeader]: Starting
2025-04-03T23:43:03.571+09:00  INFO 45581 --- [r-0-RemoteFetch] perationPurgatory$ExpiredOperationReaper : [ExpirationReaper-0-RemoteFetch]: Starting
2025-04-03T23:43:03.576+09:00  INFO 45581 --- [rFailureHandler] k.s.ReplicaManager$LogDirFailureHandler  : [LogDirFailureHandler]: Starting
2025-04-03T23:43:03.576+09:00  INFO 45581 --- [nSenderThread-0] kafka.server.AddPartitionsToTxnManager   : [AddPartitionsToTxnSenderThread-0]: Starting
2025-04-03T23:43:03.585+09:00  INFO 45581 --- [    Test worker] kafka.zk.KafkaZkClient                   : Creating /brokers/ids/0 (is it secure? false)
2025-04-03T23:43:03.590+09:00  INFO 45581 --- [    Test worker] kafka.zk.KafkaZkClient                   : Stat of the created znode at /brokers/ids/0 is: 25,25,1743691383588,1743691383588,1,0,0,72067535115124736,204,0,25

2025-04-03T23:43:03.590+09:00  INFO 45581 --- [    Test worker] kafka.zk.KafkaZkClient                   : Registered broker 0 at path /brokers/ids/0 with addresses: PLAINTEXT://localhost:58606, czxid (broker epoch): 25
2025-04-03T23:43:03.607+09:00  INFO 45581 --- [er-event-thread] rollerEventManager$ControllerEventThread : [ControllerEventThread controllerId=0] Starting
2025-04-03T23:43:03.612+09:00  INFO 45581 --- [nReaper-0-topic] perationPurgatory$ExpiredOperationReaper : [ExpirationReaper-0-topic]: Starting
2025-04-03T23:43:03.614+09:00  INFO 45581 --- [er-event-thread] kafka.zk.KafkaZkClient                   : Successfully created /controller_epoch with initial epoch 0
2025-04-03T23:43:03.616+09:00  INFO 45581 --- [er-event-thread] kafka.controller.KafkaController         : [Controller id=0] 0 successfully elected as the controller. Epoch incremented to 1 and epoch zk version is now 1
2025-04-03T23:43:03.616+09:00  INFO 45581 --- [per-0-Heartbeat] perationPurgatory$ExpiredOperationReaper : [ExpirationReaper-0-Heartbeat]: Starting
2025-04-03T23:43:03.616+09:00  INFO 45581 --- [per-0-Rebalance] perationPurgatory$ExpiredOperationReaper : [ExpirationReaper-0-Rebalance]: Starting
2025-04-03T23:43:03.617+09:00  INFO 45581 --- [er-event-thread] kafka.controller.KafkaController         : [Controller id=0] Creating FeatureZNode at path: /feature with contents: FeatureZNode(2,Enabled,Map())
2025-04-03T23:43:03.618+09:00  INFO 45581 --- [ker-EventThread] k.server.FinalizedFeatureChangeListener  : Feature ZK node created at path: /feature
2025-04-03T23:43:03.623+09:00  INFO 45581 --- [    Test worker] k.coordinator.group.GroupCoordinator     : [GroupCoordinator 0]: Starting up.
2025-04-03T23:43:03.625+09:00  INFO 45581 --- [    Test worker] k.coordinator.group.GroupCoordinator     : [GroupCoordinator 0]: Startup complete.
2025-04-03T23:43:03.627+09:00  INFO 45581 --- [-process-thread] kafka.server.metadata.ZkMetadataCache    : [MetadataCache brokerId=0] Updated cache from existing None to latest Features(metadataVersion=3.8-IV0, finalizedFeatures={}, finalizedFeaturesEpoch=0).
2025-04-03T23:43:03.627+09:00  INFO 45581 --- [er-event-thread] kafka.controller.KafkaController         : [Controller id=0] Registering handlers
2025-04-03T23:43:03.628+09:00  INFO 45581 --- [er-event-thread] kafka.controller.KafkaController         : [Controller id=0] Deleting log dir event notifications
2025-04-03T23:43:03.629+09:00  INFO 45581 --- [er-event-thread] kafka.controller.KafkaController         : [Controller id=0] Deleting isr change notifications
2025-04-03T23:43:03.629+09:00  INFO 45581 --- [er-event-thread] kafka.controller.KafkaController         : [Controller id=0] Initializing controller context
2025-04-03T23:43:03.631+09:00  INFO 45581 --- [    Test worker] k.c.transaction.TransactionCoordinator   : [TransactionCoordinator id=0] Starting up.
2025-04-03T23:43:03.632+09:00  INFO 45581 --- [rSenderThread-0] k.c.t.TransactionMarkerChannelManager    : [TxnMarkerSenderThread-0]: Starting
2025-04-03T23:43:03.632+09:00  INFO 45581 --- [    Test worker] k.c.transaction.TransactionCoordinator   : [TransactionCoordinator id=0] Startup complete.
2025-04-03T23:43:03.634+09:00  INFO 45581 --- [er-event-thread] kafka.controller.KafkaController         : [Controller id=0] Initialized broker epochs cache: HashMap(0 -> 25)
2025-04-03T23:43:03.638+09:00  INFO 45581 --- [r-0-send-thread] kafka.controller.RequestSendThread       : [RequestSendThread controllerId=0] Starting
2025-04-03T23:43:03.639+09:00  INFO 45581 --- [er-event-thread] kafka.controller.KafkaController         : [Controller id=0] Currently active brokers in the cluster: Set(0)
2025-04-03T23:43:03.639+09:00  INFO 45581 --- [er-event-thread] kafka.controller.KafkaController         : [Controller id=0] Currently shutting brokers in the cluster: HashSet()
2025-04-03T23:43:03.639+09:00  INFO 45581 --- [er-event-thread] kafka.controller.KafkaController         : [Controller id=0] Current list of topics in the cluster: HashSet()
2025-04-03T23:43:03.639+09:00  INFO 45581 --- [er-event-thread] kafka.controller.KafkaController         : [Controller id=0] Fetching topic deletions in progress
2025-04-03T23:43:03.640+09:00  INFO 45581 --- [er-event-thread] kafka.controller.KafkaController         : [Controller id=0] List of topics to be deleted:
2025-04-03T23:43:03.640+09:00  INFO 45581 --- [er-event-thread] kafka.controller.KafkaController         : [Controller id=0] List of topics ineligible for deletion:
2025-04-03T23:43:03.640+09:00  INFO 45581 --- [er-event-thread] kafka.controller.KafkaController         : [Controller id=0] Initializing topic deletion manager
2025-04-03T23:43:03.640+09:00  INFO 45581 --- [er-event-thread] kafka.controller.TopicDeletionManager    : [Topic Deletion Manager 0] Initializing manager with initial deletions: Set(), initial ineligible deletions: HashSet()
2025-04-03T23:43:03.641+09:00  INFO 45581 --- [er-event-thread] kafka.controller.KafkaController         : [Controller id=0] Sending update metadata request
2025-04-03T23:43:03.642+09:00  INFO 45581 --- [er-event-thread] state.change.logger                      : [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet(0) for 0 partitions
2025-04-03T23:43:03.644+09:00  INFO 45581 --- [r-0-send-thread] kafka.controller.RequestSendThread       : [RequestSendThread controllerId=0] Controller 0 connected to localhost:58606 (id: 0 rack: null) for sending state change requests
2025-04-03T23:43:03.644+09:00  INFO 45581 --- [er-event-thread] kafka.controller.ZkReplicaStateMachine   : [ReplicaStateMachine controllerId=0] Initializing replica state
2025-04-03T23:43:03.644+09:00  INFO 45581 --- [er-event-thread] kafka.controller.ZkReplicaStateMachine   : [ReplicaStateMachine controllerId=0] Triggering online replica state changes
2025-04-03T23:43:03.645+09:00  INFO 45581 --- [er-event-thread] kafka.controller.ZkReplicaStateMachine   : [ReplicaStateMachine controllerId=0] Triggering offline replica state changes
2025-04-03T23:43:03.645+09:00  INFO 45581 --- [er-event-thread] k.controller.ZkPartitionStateMachine     : [PartitionStateMachine controllerId=0] Initializing partition state
2025-04-03T23:43:03.645+09:00  INFO 45581 --- [er-event-thread] k.controller.ZkPartitionStateMachine     : [PartitionStateMachine controllerId=0] Triggering online partition state changes
2025-04-03T23:43:03.646+09:00  INFO 45581 --- [er-event-thread] kafka.controller.KafkaController         : [Controller id=0] Ready to serve as the new controller with epoch 1
2025-04-03T23:43:03.648+09:00  INFO 45581 --- [er-event-thread] kafka.controller.KafkaController         : [Controller id=0] Partitions undergoing preferred replica election:
2025-04-03T23:43:03.648+09:00  INFO 45581 --- [er-event-thread] kafka.controller.KafkaController         : [Controller id=0] Partitions that completed preferred replica election:
2025-04-03T23:43:03.648+09:00  INFO 45581 --- [er-event-thread] kafka.controller.KafkaController         : [Controller id=0] Skipping preferred replica election for partitions due to topic deletion:
2025-04-03T23:43:03.648+09:00  INFO 45581 --- [er-event-thread] kafka.controller.KafkaController         : [Controller id=0] Resuming preferred replica election for partitions:
2025-04-03T23:43:03.648+09:00  INFO 45581 --- [er-event-thread] kafka.controller.KafkaController         : [Controller id=0] Starting replica leader election (PREFERRED) for partitions  triggered by ZkTriggered
2025-04-03T23:43:03.649+09:00  INFO 45581 --- [per-0-AlterAcls] perationPurgatory$ExpiredOperationReaper : [ExpirationReaper-0-AlterAcls]: Starting
2025-04-03T23:43:03.652+09:00  INFO 45581 --- [er-event-thread] kafka.controller.KafkaController         : [Controller id=0] Starting the controller scheduler
2025-04-03T23:43:03.661+09:00  INFO 45581 --- [-process-thread] icationListener$ChangeEventProcessThread : [/config/changes-event-process-thread]: Starting
2025-04-03T23:43:03.664+09:00  INFO 45581 --- [    Test worker] kafka.network.SocketServer               : [SocketServer listenerType=ZK_BROKER, nodeId=0] Enabling request processing.
2025-04-03T23:43:03.665+09:00  INFO 45581 --- [    Test worker] kafka.server.KafkaServer                 : [KafkaServer id=0] Start processing authorizer futures
2025-04-03T23:43:03.665+09:00  INFO 45581 --- [    Test worker] kafka.server.KafkaServer                 : [KafkaServer id=0] End processing authorizer futures
2025-04-03T23:43:03.665+09:00  INFO 45581 --- [    Test worker] kafka.server.KafkaServer                 : [KafkaServer id=0] Start processing enable request processing future
2025-04-03T23:43:03.665+09:00  INFO 45581 --- [    Test worker] kafka.server.KafkaServer                 : [KafkaServer id=0] End processing enable request processing future
2025-04-03T23:43:03.665+09:00  INFO 45581 --- [    Test worker] o.a.kafka.common.utils.AppInfoParser     : Kafka version: 3.8.1
2025-04-03T23:43:03.665+09:00  INFO 45581 --- [    Test worker] o.a.kafka.common.utils.AppInfoParser     : Kafka commitId: 70d6ff42debf7e17
2025-04-03T23:43:03.665+09:00  INFO 45581 --- [    Test worker] o.a.kafka.common.utils.AppInfoParser     : Kafka startTimeMs: 1743691383665
2025-04-03T23:43:03.665+09:00  INFO 45581 --- [    Test worker] kafka.server.KafkaServer                 : [KafkaServer id=0] started
2025-04-03T23:43:03.677+09:00  INFO 45581 --- [    Test worker] o.a.k.clients.admin.AdminClientConfig    : AdminClientConfig values:
	auto.include.jmx.reporter = true
	bootstrap.controllers = []
	bootstrap.servers = [127.0.0.1:58606]
	client.dns.lookup = use_all_dns_ips
	client.id =
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	enable.metrics.push = true
	metadata.max.age.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2025-04-03T23:43:03.689+09:00  INFO 45581 --- [    Test worker] o.a.kafka.common.utils.AppInfoParser     : Kafka version: 3.8.1
2025-04-03T23:43:03.689+09:00  INFO 45581 --- [    Test worker] o.a.kafka.common.utils.AppInfoParser     : Kafka commitId: 70d6ff42debf7e17
2025-04-03T23:43:03.689+09:00  INFO 45581 --- [    Test worker] o.a.kafka.common.utils.AppInfoParser     : Kafka startTimeMs: 1743691383689
2025-04-03T23:43:03.705+09:00  INFO 45581 --- [quest-handler-4] kafka.zk.AdminZkClient                   : Creating topic topic1 with configuration {} and initial partition assignment HashMap(0 -> ArrayBuffer(0))
2025-04-03T23:43:03.712+09:00  INFO 45581 --- [er-event-thread] kafka.controller.KafkaController         : [Controller id=0] New topics: [Set(topic1)], deleted topics: [HashSet()], new partition replica assignment [Set(TopicIdReplicaAssignment(topic1,Some(igT2QCHQQDKU_dUFPuhizw),Map(topic1-0 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=))))]
2025-04-03T23:43:03.712+09:00  INFO 45581 --- [er-event-thread] kafka.controller.KafkaController         : [Controller id=0] New partition creation callback for topic1-0
2025-04-03T23:43:03.713+09:00  INFO 45581 --- [er-event-thread] state.change.logger                      : [Controller id=0 epoch=1] Changed partition topic1-0 state from NonExistentPartition to NewPartition with assigned replicas 0
2025-04-03T23:43:03.713+09:00  INFO 45581 --- [er-event-thread] state.change.logger                      : [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions
2025-04-03T23:43:03.714+09:00  INFO 45581 --- [quest-handler-4] kafka.zk.AdminZkClient                   : Creating topic my-topic with configuration {} and initial partition assignment HashMap(0 -> ArrayBuffer(0))
2025-04-03T23:43:03.715+09:00  INFO 45581 --- [er-event-thread] state.change.logger                      : [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions
2025-04-03T23:43:03.721+09:00  INFO 45581 --- [er-event-thread] state.change.logger                      : [Controller id=0 epoch=1] Changed partition topic1-0 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isrWithBrokerEpoch=List(BrokerState(brokerId=0, brokerEpoch=-1)), leaderRecoveryState=RECOVERED, partitionEpoch=0)
2025-04-03T23:43:03.721+09:00  INFO 45581 --- [er-event-thread] state.change.logger                      : [Controller id=0 epoch=1] Sending LeaderAndIsr request to broker 0 with 1 become-leader and 0 become-follower partitions
2025-04-03T23:43:03.722+09:00  INFO 45581 --- [er-event-thread] state.change.logger                      : [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet(0) for 1 partitions
2025-04-03T23:43:03.722+09:00  INFO 45581 --- [er-event-thread] state.change.logger                      : [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions
2025-04-03T23:43:03.723+09:00  INFO 45581 --- [er-event-thread] kafka.controller.KafkaController         : [Controller id=0] New topics: [Set(my-topic)], deleted topics: [HashSet()], new partition replica assignment [Set(TopicIdReplicaAssignment(my-topic,Some(sSNy-FpkTyOe-mIWT4kGjQ),Map(my-topic-0 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=))))]
2025-04-03T23:43:03.723+09:00  INFO 45581 --- [er-event-thread] kafka.controller.KafkaController         : [Controller id=0] New partition creation callback for my-topic-0
2025-04-03T23:43:03.723+09:00  INFO 45581 --- [er-event-thread] state.change.logger                      : [Controller id=0 epoch=1] Changed partition my-topic-0 state from NonExistentPartition to NewPartition with assigned replicas 0
2025-04-03T23:43:03.723+09:00  INFO 45581 --- [er-event-thread] state.change.logger                      : [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions
2025-04-03T23:43:03.723+09:00  INFO 45581 --- [er-event-thread] state.change.logger                      : [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions
2025-04-03T23:43:03.723+09:00  INFO 45581 --- [quest-handler-5] state.change.logger                      : [Broker id=0] Handling LeaderAndIsr request correlationId 1 from controller 0 for 1 partitions
2025-04-03T23:43:03.725+09:00  INFO 45581 --- [er-event-thread] state.change.logger                      : [Controller id=0 epoch=1] Changed partition my-topic-0 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isrWithBrokerEpoch=List(BrokerState(brokerId=0, brokerEpoch=-1)), leaderRecoveryState=RECOVERED, partitionEpoch=0)
2025-04-03T23:43:03.725+09:00  INFO 45581 --- [er-event-thread] state.change.logger                      : [Controller id=0 epoch=1] Sending LeaderAndIsr request to broker 0 with 1 become-leader and 0 become-follower partitions
2025-04-03T23:43:03.725+09:00  INFO 45581 --- [er-event-thread] state.change.logger                      : [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet(0) for 1 partitions
2025-04-03T23:43:03.725+09:00  INFO 45581 --- [er-event-thread] state.change.logger                      : [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions
2025-04-03T23:43:03.726+09:00  INFO 45581 --- [channel-manager] k.server.NodeToControllerRequestThread   : [zk-broker-0-to-controller-forwarding-channel-manager]: Recorded new ZK controller, from now on will use node localhost:58606 (id: 0 rack: null)
2025-04-03T23:43:03.730+09:00  INFO 45581 --- [quest-handler-5] kafka.server.ReplicaFetcherManager       : [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(topic1-0)
2025-04-03T23:43:03.730+09:00  INFO 45581 --- [quest-handler-5] state.change.logger                      : [Broker id=0] Stopped fetchers as part of LeaderAndIsr request correlationId 1 from controller 0 epoch 1 as part of the become-leader transition for 1 partitions
2025-04-03T23:43:03.753+09:00  INFO 45581 --- [quest-handler-5] kafka.log.UnifiedLog$                    : [LogLoader partition=topic1-0, dir=/var/folders/qb/nxyw0kj549bbf87mx3ss8vgc0000gn/T/spring.kafka.0f9f4ec2-491a-4eb8-ba3a-35f677f610e66802667832673744764] Loading producer state till offset 0 with message format version 2
2025-04-03T23:43:03.757+09:00  INFO 45581 --- [quest-handler-5] kafka.log.LogManager                     : Created log for partition topic1-0 in /var/folders/qb/nxyw0kj549bbf87mx3ss8vgc0000gn/T/spring.kafka.0f9f4ec2-491a-4eb8-ba3a-35f677f610e66802667832673744764/topic1-0 with properties {}
2025-04-03T23:43:03.757+09:00  INFO 45581 --- [quest-handler-5] kafka.cluster.Partition                  : [Partition topic1-0 broker=0] No checkpointed highwatermark is found for partition topic1-0
2025-04-03T23:43:03.758+09:00  INFO 45581 --- [quest-handler-5] kafka.cluster.Partition                  : [Partition topic1-0 broker=0] Log loaded for partition topic1-0 with initial high watermark 0
2025-04-03T23:43:03.759+09:00  INFO 45581 --- [quest-handler-5] state.change.logger                      : [Broker id=0] Leader topic1-0 with topic id Some(igT2QCHQQDKU_dUFPuhizw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [0], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1.
2025-04-03T23:43:03.760+09:00  INFO 45581 --- [channel-manager] k.server.NodeToControllerRequestThread   : [zk-broker-0-to-controller-alter-partition-channel-manager]: Recorded new ZK controller, from now on will use node localhost:58606 (id: 0 rack: null)
2025-04-03T23:43:03.766+09:00  INFO 45581 --- [quest-handler-5] state.change.logger                      : [Broker id=0] Finished LeaderAndIsr request in 43ms correlationId 1 from controller 0 for 1 partitions
2025-04-03T23:43:03.768+09:00  INFO 45581 --- [quest-handler-6] state.change.logger                      : [Broker id=0] Add 1 partitions and deleted 0 partitions from metadata cache in response to UpdateMetadata request sent by controller 0 epoch 1 with correlation id 2
2025-04-03T23:43:03.769+09:00  INFO 45581 --- [quest-handler-7] state.change.logger                      : [Broker id=0] Handling LeaderAndIsr request correlationId 3 from controller 0 for 1 partitions
2025-04-03T23:43:03.769+09:00  INFO 45581 --- [quest-handler-7] kafka.server.ReplicaFetcherManager       : [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(my-topic-0)
2025-04-03T23:43:03.769+09:00  INFO 45581 --- [quest-handler-7] state.change.logger                      : [Broker id=0] Stopped fetchers as part of LeaderAndIsr request correlationId 3 from controller 0 epoch 1 as part of the become-leader transition for 1 partitions
2025-04-03T23:43:03.772+09:00  INFO 45581 --- [quest-handler-7] kafka.log.UnifiedLog$                    : [LogLoader partition=my-topic-0, dir=/var/folders/qb/nxyw0kj549bbf87mx3ss8vgc0000gn/T/spring.kafka.0f9f4ec2-491a-4eb8-ba3a-35f677f610e66802667832673744764] Loading producer state till offset 0 with message format version 2
2025-04-03T23:43:03.773+09:00  INFO 45581 --- [quest-handler-7] kafka.log.LogManager                     : Created log for partition my-topic-0 in /var/folders/qb/nxyw0kj549bbf87mx3ss8vgc0000gn/T/spring.kafka.0f9f4ec2-491a-4eb8-ba3a-35f677f610e66802667832673744764/my-topic-0 with properties {}
2025-04-03T23:43:03.773+09:00  INFO 45581 --- [quest-handler-7] kafka.cluster.Partition                  : [Partition my-topic-0 broker=0] No checkpointed highwatermark is found for partition my-topic-0
2025-04-03T23:43:03.773+09:00  INFO 45581 --- [quest-handler-7] kafka.cluster.Partition                  : [Partition my-topic-0 broker=0] Log loaded for partition my-topic-0 with initial high watermark 0
2025-04-03T23:43:03.773+09:00  INFO 45581 --- [quest-handler-7] state.change.logger                      : [Broker id=0] Leader my-topic-0 with topic id Some(sSNy-FpkTyOe-mIWT4kGjQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [0], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1.
2025-04-03T23:43:03.776+09:00  INFO 45581 --- [quest-handler-7] state.change.logger                      : [Broker id=0] Finished LeaderAndIsr request in 7ms correlationId 3 from controller 0 for 1 partitions
2025-04-03T23:43:03.777+09:00  INFO 45581 --- [quest-handler-0] state.change.logger                      : [Broker id=0] Add 1 partitions and deleted 0 partitions from metadata cache in response to UpdateMetadata request sent by controller 0 epoch 1 with correlation id 4
2025-04-03T23:43:03.779+09:00  INFO 45581 --- [| adminclient-1] o.a.kafka.common.utils.AppInfoParser     : App info kafka.admin.client for adminclient-1 unregistered
2025-04-03T23:43:03.781+09:00  INFO 45581 --- [| adminclient-1] o.apache.kafka.common.metrics.Metrics    : Metrics scheduler closed
2025-04-03T23:43:03.781+09:00  INFO 45581 --- [| adminclient-1] o.apache.kafka.common.metrics.Metrics    : Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-04-03T23:43:03.781+09:00  INFO 45581 --- [| adminclient-1] o.apache.kafka.common.metrics.Metrics    : Metrics reporters closed
2025-04-03T23:43:03.783+09:00  INFO 45581 --- [    Test worker] m.i.s.test2.KafkaTest2                   : Starting KafkaTest2 using Java 17.0.12 with PID 45581 (started by shlee in /Users/shlee/workspaces/study/iseunghan-Lab/spring-embedded-kafka-test)
2025-04-03T23:43:03.783+09:00  INFO 45581 --- [    Test worker] m.i.s.test2.KafkaTest2                   : No active profile set, falling back to 1 default profile: "default"
2025-04-03T23:43:03.902+09:00  INFO 45581 --- [    Test worker] o.s.b.w.embedded.tomcat.TomcatWebServer  : Tomcat initialized with port 0 (http)
2025-04-03T23:43:03.908+09:00  INFO 45581 --- [    Test worker] o.apache.catalina.core.StandardService   : Starting service [Tomcat]
2025-04-03T23:43:03.908+09:00  INFO 45581 --- [    Test worker] o.apache.catalina.core.StandardEngine    : Starting Servlet engine: [Apache Tomcat/10.1.39]
2025-04-03T23:43:03.933+09:00  INFO 45581 --- [    Test worker] o.a.c.c.C.[Tomcat].[localhost].[/]       : Initializing Spring embedded WebApplicationContext
2025-04-03T23:43:03.933+09:00  INFO 45581 --- [    Test worker] w.s.c.ServletWebServerApplicationContext : Root WebApplicationContext: initialization completed in 149 ms
2025-04-03T23:43:04.026+09:00  WARN 45581 --- [ntainer#0-0-C-1] org.apache.kafka.clients.NetworkClient   : [Consumer clientId=consumer-my-group-1, groupId=my-group] Error connecting to node logs-service-kafka-1:9092 (id: 1 rack: null)

java.net.UnknownHostException: logs-service-kafka-1
	at java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801) ~[na:na]
	at java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1533) ~[na:na]
	at java.base/java.net.InetAddress.getAllByName(InetAddress.java:1385) ~[na:na]
	at java.base/java.net.InetAddress.getAllByName(InetAddress.java:1306) ~[na:na]
	at org.apache.kafka.clients.DefaultHostResolver.resolve(DefaultHostResolver.java:27) ~[kafka-clients-3.8.1.jar:na]
	at org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:124) ~[kafka-clients-3.8.1.jar:na]
	at org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.resolveAddresses(ClusterConnectionStates.java:536) ~[kafka-clients-3.8.1.jar:na]
	at org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:511) ~[kafka-clients-3.8.1.jar:na]
	at org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:466) ~[kafka-clients-3.8.1.jar:na]
	at org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:173) ~[kafka-clients-3.8.1.jar:na]
	at org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:1070) ~[kafka-clients-3.8.1.jar:na]
	at org.apache.kafka.clients.NetworkClient.access$800(NetworkClient.java:76) ~[kafka-clients-3.8.1.jar:na]
	at org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1259) ~[kafka-clients-3.8.1.jar:na]
	at org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1159) ~[kafka-clients-3.8.1.jar:na]
	at org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:592) ~[kafka-clients-3.8.1.jar:na]
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:281) ~[kafka-clients-3.8.1.jar:na]
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:252) ~[kafka-clients-3.8.1.jar:na]
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:243) ~[kafka-clients-3.8.1.jar:na]
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:165) ~[kafka-clients-3.8.1.jar:na]
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:302) ~[kafka-clients-3.8.1.jar:na]
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:263) ~[kafka-clients-3.8.1.jar:na]
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.coordinatorUnknownAndUnreadySync(ConsumerCoordinator.java:450) ~[kafka-clients-3.8.1.jar:na]
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:482) ~[kafka-clients-3.8.1.jar:na]
	at org.apache.kafka.clients.consumer.internals.LegacyKafkaConsumer.updateAssignmentMetadataIfNeeded(LegacyKafkaConsumer.java:652) ~[kafka-clients-3.8.1.jar:na]
	at org.apache.kafka.clients.consumer.internals.LegacyKafkaConsumer.poll(LegacyKafkaConsumer.java:611) ~[kafka-clients-3.8.1.jar:na]
	at org.apache.kafka.clients.consumer.internals.LegacyKafkaConsumer.poll(LegacyKafkaConsumer.java:591) ~[kafka-clients-3.8.1.jar:na]
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:874) ~[kafka-clients-3.8.1.jar:na]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.pollConsumer(KafkaMessageListenerContainer.java:1692) ~[spring-kafka-3.3.4.jar:3.3.4]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doPoll(KafkaMessageListenerContainer.java:1667) ~[spring-kafka-3.3.4.jar:3.3.4]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.pollAndInvoke(KafkaMessageListenerContainer.java:1445) ~[spring-kafka-3.3.4.jar:3.3.4]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.run(KafkaMessageListenerContainer.java:1335) ~[spring-kafka-3.3.4.jar:3.3.4]
	at java.base/java.util.concurrent.CompletableFuture$AsyncRun.run(CompletableFuture.java:1804) ~[na:na]
	at java.base/java.lang.Thread.run(Thread.java:840) ~[na:na]

2025-04-03T23:43:04.101+09:00  INFO 45581 --- [    Test worker] o.s.b.w.embedded.tomcat.TomcatWebServer  : Tomcat started on port 58610 (http) with context path '/'
2025-04-03T23:43:04.102+09:00  INFO 45581 --- [    Test worker] o.a.k.clients.consumer.ConsumerConfig    : ConsumerConfig values:
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:58606]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-my-group-2
	client.rack =
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = my-group
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2025-04-03T23:43:04.103+09:00  INFO 45581 --- [    Test worker] o.a.k.c.t.i.KafkaMetricsCollector        : initializing Kafka metrics collector
2025-04-03T23:43:04.105+09:00  INFO 45581 --- [    Test worker] o.a.kafka.common.utils.AppInfoParser     : Kafka version: 3.8.1
2025-04-03T23:43:04.105+09:00  INFO 45581 --- [    Test worker] o.a.kafka.common.utils.AppInfoParser     : Kafka commitId: 70d6ff42debf7e17
2025-04-03T23:43:04.105+09:00  INFO 45581 --- [    Test worker] o.a.kafka.common.utils.AppInfoParser     : Kafka startTimeMs: 1743691384105
2025-04-03T23:43:04.105+09:00  INFO 45581 --- [    Test worker] o.a.k.c.c.internals.LegacyKafkaConsumer  : [Consumer clientId=consumer-my-group-2, groupId=my-group] Subscribed to topic(s): my-topic
2025-04-03T23:43:04.107+09:00  INFO 45581 --- [    Test worker] m.i.s.test2.KafkaTest2                   : Started KafkaTest2 in 0.97 seconds (process running for 2.706)
2025-04-03T23:43:04.110+09:00  INFO 45581 --- [ntainer#0-0-C-1] org.apache.kafka.clients.Metadata        : [Consumer clientId=consumer-my-group-2, groupId=my-group] Cluster ID: -VPPvggYRSiyrN8wldWTNg
2025-04-03T23:43:04.112+09:00  INFO 45581 --- [quest-handler-3] kafka.zk.AdminZkClient                   : Creating topic __consumer_offsets with configuration {compression.type=producer, cleanup.policy=compact, segment.bytes=104857600} and initial partition assignment HashMap(0 -> ArrayBuffer(0), 1 -> ArrayBuffer(0), 2 -> ArrayBuffer(0), 3 -> ArrayBuffer(0), 4 -> ArrayBuffer(0))
2025-04-03T23:43:04.116+09:00  INFO 45581 --- [er-event-thread] kafka.controller.KafkaController         : [Controller id=0] New topics: [Set(__consumer_offsets)], deleted topics: [HashSet()], new partition replica assignment [Set(TopicIdReplicaAssignment(__consumer_offsets,Some(ohc0nNHLRBikISM55NBYow),HashMap(__consumer_offsets-4 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-3 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-2 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-0 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-1 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=))))]
2025-04-03T23:43:04.116+09:00  INFO 45581 --- [er-event-thread] kafka.controller.KafkaController         : [Controller id=0] New partition creation callback for __consumer_offsets-4,__consumer_offsets-3,__consumer_offsets-2,__consumer_offsets-0,__consumer_offsets-1
2025-04-03T23:43:04.116+09:00  INFO 45581 --- [er-event-thread] state.change.logger                      : [Controller id=0 epoch=1] Changed partition __consumer_offsets-4 state from NonExistentPartition to NewPartition with assigned replicas 0
2025-04-03T23:43:04.116+09:00  INFO 45581 --- [er-event-thread] state.change.logger                      : [Controller id=0 epoch=1] Changed partition __consumer_offsets-3 state from NonExistentPartition to NewPartition with assigned replicas 0
2025-04-03T23:43:04.116+09:00  INFO 45581 --- [er-event-thread] state.change.logger                      : [Controller id=0 epoch=1] Changed partition __consumer_offsets-2 state from NonExistentPartition to NewPartition with assigned replicas 0
2025-04-03T23:43:04.116+09:00  INFO 45581 --- [er-event-thread] state.change.logger                      : [Controller id=0 epoch=1] Changed partition __consumer_offsets-0 state from NonExistentPartition to NewPartition with assigned replicas 0
2025-04-03T23:43:04.116+09:00  INFO 45581 --- [er-event-thread] state.change.logger                      : [Controller id=0 epoch=1] Changed partition __consumer_offsets-1 state from NonExistentPartition to NewPartition with assigned replicas 0
2025-04-03T23:43:04.116+09:00  INFO 45581 --- [er-event-thread] state.change.logger                      : [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions
2025-04-03T23:43:04.117+09:00  INFO 45581 --- [er-event-thread] state.change.logger                      : [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions
2025-04-03T23:43:04.120+09:00  INFO 45581 --- [er-event-thread] state.change.logger                      : [Controller id=0 epoch=1] Changed partition __consumer_offsets-4 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isrWithBrokerEpoch=List(BrokerState(brokerId=0, brokerEpoch=-1)), leaderRecoveryState=RECOVERED, partitionEpoch=0)
2025-04-03T23:43:04.120+09:00  INFO 45581 --- [er-event-thread] state.change.logger                      : [Controller id=0 epoch=1] Changed partition __consumer_offsets-3 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isrWithBrokerEpoch=List(BrokerState(brokerId=0, brokerEpoch=-1)), leaderRecoveryState=RECOVERED, partitionEpoch=0)
2025-04-03T23:43:04.120+09:00  INFO 45581 --- [er-event-thread] state.change.logger                      : [Controller id=0 epoch=1] Changed partition __consumer_offsets-2 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isrWithBrokerEpoch=List(BrokerState(brokerId=0, brokerEpoch=-1)), leaderRecoveryState=RECOVERED, partitionEpoch=0)
2025-04-03T23:43:04.120+09:00  INFO 45581 --- [er-event-thread] state.change.logger                      : [Controller id=0 epoch=1] Changed partition __consumer_offsets-0 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isrWithBrokerEpoch=List(BrokerState(brokerId=0, brokerEpoch=-1)), leaderRecoveryState=RECOVERED, partitionEpoch=0)
2025-04-03T23:43:04.120+09:00  INFO 45581 --- [er-event-thread] state.change.logger                      : [Controller id=0 epoch=1] Changed partition __consumer_offsets-1 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isrWithBrokerEpoch=List(BrokerState(brokerId=0, brokerEpoch=-1)), leaderRecoveryState=RECOVERED, partitionEpoch=0)
2025-04-03T23:43:04.120+09:00  INFO 45581 --- [er-event-thread] state.change.logger                      : [Controller id=0 epoch=1] Sending LeaderAndIsr request to broker 0 with 5 become-leader and 0 become-follower partitions
2025-04-03T23:43:04.120+09:00  INFO 45581 --- [er-event-thread] state.change.logger                      : [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet(0) for 5 partitions
2025-04-03T23:43:04.121+09:00  INFO 45581 --- [er-event-thread] state.change.logger                      : [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions
2025-04-03T23:43:04.121+09:00  INFO 45581 --- [quest-handler-4] state.change.logger                      : [Broker id=0] Handling LeaderAndIsr request correlationId 5 from controller 0 for 5 partitions
2025-04-03T23:43:04.122+09:00  INFO 45581 --- [quest-handler-4] kafka.server.ReplicaFetcherManager       : [ReplicaFetcherManager on broker 0] Removed fetcher for partitions HashSet(__consumer_offsets-4, __consumer_offsets-3, __consumer_offsets-2, __consumer_offsets-0, __consumer_offsets-1)
2025-04-03T23:43:04.122+09:00  INFO 45581 --- [quest-handler-4] state.change.logger                      : [Broker id=0] Stopped fetchers as part of LeaderAndIsr request correlationId 5 from controller 0 epoch 1 as part of the become-leader transition for 5 partitions
testBean
2025-04-03T23:43:04.127+09:00  INFO 45581 --- [quest-handler-4] kafka.log.UnifiedLog$                    : [LogLoader partition=__consumer_offsets-3, dir=/var/folders/qb/nxyw0kj549bbf87mx3ss8vgc0000gn/T/spring.kafka.0f9f4ec2-491a-4eb8-ba3a-35f677f610e66802667832673744764] Loading producer state till offset 0 with message format version 2
2025-04-03T23:43:04.127+09:00  INFO 45581 --- [quest-handler-4] kafka.log.LogManager                     : Created log for partition __consumer_offsets-3 in /var/folders/qb/nxyw0kj549bbf87mx3ss8vgc0000gn/T/spring.kafka.0f9f4ec2-491a-4eb8-ba3a-35f677f610e66802667832673744764/__consumer_offsets-3 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600}
2025-04-03T23:43:04.127+09:00  INFO 45581 --- [quest-handler-4] kafka.cluster.Partition                  : [Partition __consumer_offsets-3 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-3
2025-04-03T23:43:04.127+09:00  INFO 45581 --- [quest-handler-4] kafka.cluster.Partition                  : [Partition __consumer_offsets-3 broker=0] Log loaded for partition __consumer_offsets-3 with initial high watermark 0
2025-04-03T23:43:04.127+09:00  INFO 45581 --- [quest-handler-4] state.change.logger                      : [Broker id=0] Leader __consumer_offsets-3 with topic id Some(ohc0nNHLRBikISM55NBYow) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [0], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1.
2025-04-03T23:43:04.129+09:00  INFO 45581 --- [    Test worker] o.a.k.clients.producer.ProducerConfig    : ProducerConfig values:
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [127.0.0.1:58606]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-1
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-03T23:43:04.129+09:00  INFO 45581 --- [    Test worker] o.a.k.c.t.i.KafkaMetricsCollector        : initializing Kafka metrics collector
2025-04-03T23:43:04.133+09:00  INFO 45581 --- [    Test worker] o.a.k.clients.producer.KafkaProducer     : [Producer clientId=producer-1] Instantiated an idempotent producer.
2025-04-03T23:43:04.136+09:00  INFO 45581 --- [quest-handler-4] kafka.log.UnifiedLog$                    : [LogLoader partition=__consumer_offsets-2, dir=/var/folders/qb/nxyw0kj549bbf87mx3ss8vgc0000gn/T/spring.kafka.0f9f4ec2-491a-4eb8-ba3a-35f677f610e66802667832673744764] Loading producer state till offset 0 with message format version 2
2025-04-03T23:43:04.138+09:00  INFO 45581 --- [quest-handler-4] kafka.log.LogManager                     : Created log for partition __consumer_offsets-2 in /var/folders/qb/nxyw0kj549bbf87mx3ss8vgc0000gn/T/spring.kafka.0f9f4ec2-491a-4eb8-ba3a-35f677f610e66802667832673744764/__consumer_offsets-2 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600}
2025-04-03T23:43:04.138+09:00  INFO 45581 --- [quest-handler-4] kafka.cluster.Partition                  : [Partition __consumer_offsets-2 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-2
2025-04-03T23:43:04.138+09:00  INFO 45581 --- [quest-handler-4] kafka.cluster.Partition                  : [Partition __consumer_offsets-2 broker=0] Log loaded for partition __consumer_offsets-2 with initial high watermark 0
2025-04-03T23:43:04.138+09:00  INFO 45581 --- [quest-handler-4] state.change.logger                      : [Broker id=0] Leader __consumer_offsets-2 with topic id Some(ohc0nNHLRBikISM55NBYow) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [0], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1.
2025-04-03T23:43:04.140+09:00  INFO 45581 --- [    Test worker] o.a.kafka.common.utils.AppInfoParser     : Kafka version: 3.8.1
2025-04-03T23:43:04.140+09:00  INFO 45581 --- [    Test worker] o.a.kafka.common.utils.AppInfoParser     : Kafka commitId: 70d6ff42debf7e17
2025-04-03T23:43:04.140+09:00  INFO 45581 --- [    Test worker] o.a.kafka.common.utils.AppInfoParser     : Kafka startTimeMs: 1743691384140
2025-04-03T23:43:04.142+09:00  INFO 45581 --- [ad | producer-1] org.apache.kafka.clients.Metadata        : [Producer clientId=producer-1] Cluster ID: -VPPvggYRSiyrN8wldWTNg
2025-04-03T23:43:04.146+09:00  INFO 45581 --- [quest-handler-4] kafka.log.UnifiedLog$                    : [LogLoader partition=__consumer_offsets-4, dir=/var/folders/qb/nxyw0kj549bbf87mx3ss8vgc0000gn/T/spring.kafka.0f9f4ec2-491a-4eb8-ba3a-35f677f610e66802667832673744764] Loading producer state till offset 0 with message format version 2
2025-04-03T23:43:04.147+09:00  INFO 45581 --- [quest-handler-4] kafka.log.LogManager                     : Created log for partition __consumer_offsets-4 in /var/folders/qb/nxyw0kj549bbf87mx3ss8vgc0000gn/T/spring.kafka.0f9f4ec2-491a-4eb8-ba3a-35f677f610e66802667832673744764/__consumer_offsets-4 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600}
2025-04-03T23:43:04.147+09:00  INFO 45581 --- [quest-handler-4] kafka.cluster.Partition                  : [Partition __consumer_offsets-4 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-4
2025-04-03T23:43:04.147+09:00  INFO 45581 --- [quest-handler-4] kafka.cluster.Partition                  : [Partition __consumer_offsets-4 broker=0] Log loaded for partition __consumer_offsets-4 with initial high watermark 0
2025-04-03T23:43:04.147+09:00  INFO 45581 --- [quest-handler-4] state.change.logger                      : [Broker id=0] Leader __consumer_offsets-4 with topic id Some(ohc0nNHLRBikISM55NBYow) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [0], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1.
isSucceed2
2025-04-03T23:43:04.147+09:00  INFO 45581 --- [er-event-thread] kafka.controller.KafkaController         : [Controller id=0] Acquired new producerId block ProducerIdsBlock(assignedBrokerId=0, firstProducerId=0, size=1000) by writing to Zk with path version 1
2025-04-03T23:43:04.150+09:00  INFO 45581 --- [    Test worker] t.c.s.AnnotationConfigContextLoaderUtils : Could not detect default configuration classes for test class [me.iseunghan.springembeddedkafkatest.test2.KafkaTest3]: KafkaTest3 does not declare any static, non-private, non-final, nested classes annotated with @Configuration.
2025-04-03T23:43:04.151+09:00  INFO 45581 --- [    Test worker] .b.t.c.SpringBootTestContextBootstrapper : Found @SpringBootConfiguration me.iseunghan.springembeddedkafkatest.SpringEmbeddedKafkaTestApplication for test class me.iseunghan.springembeddedkafkatest.test2.KafkaTest3
2025-04-03T23:43:04.155+09:00  INFO 45581 --- [quest-handler-4] kafka.log.UnifiedLog$                    : [LogLoader partition=__consumer_offsets-1, dir=/var/folders/qb/nxyw0kj549bbf87mx3ss8vgc0000gn/T/spring.kafka.0f9f4ec2-491a-4eb8-ba3a-35f677f610e66802667832673744764] Loading producer state till offset 0 with message format version 2
2025-04-03T23:43:04.155+09:00  INFO 45581 --- [quest-handler-4] kafka.log.LogManager                     : Created log for partition __consumer_offsets-1 in /var/folders/qb/nxyw0kj549bbf87mx3ss8vgc0000gn/T/spring.kafka.0f9f4ec2-491a-4eb8-ba3a-35f677f610e66802667832673744764/__consumer_offsets-1 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600}
2025-04-03T23:43:04.155+09:00  INFO 45581 --- [quest-handler-4] kafka.cluster.Partition                  : [Partition __consumer_offsets-1 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-1
2025-04-03T23:43:04.155+09:00  INFO 45581 --- [quest-handler-4] kafka.cluster.Partition                  : [Partition __consumer_offsets-1 broker=0] Log loaded for partition __consumer_offsets-1 with initial high watermark 0
2025-04-03T23:43:04.155+09:00  INFO 45581 --- [quest-handler-4] state.change.logger                      : [Broker id=0] Leader __consumer_offsets-1 with topic id Some(ohc0nNHLRBikISM55NBYow) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [0], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1.

  .   ____          _            __ _ _
 /\\ / ___'_ __ _ _(_)_ __  __ _ \ \ \ \
( ( )\___ | '_ | '_| | '_ \/ _` | \ \ \ \
 \\/  ___)| |_)| | | | | || (_| |  ) ) ) )
  '  |____| .__|_| |_|_| |_\__, | / / / /
 =========|_|==============|___/=/_/_/_/

 :: Spring Boot ::                (v3.4.4)

2025-04-03T23:43:04.163+09:00  INFO 45581 --- [quest-handler-4] kafka.log.UnifiedLog$                    : [LogLoader partition=__consumer_offsets-0, dir=/var/folders/qb/nxyw0kj549bbf87mx3ss8vgc0000gn/T/spring.kafka.0f9f4ec2-491a-4eb8-ba3a-35f677f610e66802667832673744764] Loading producer state till offset 0 with message format version 2
2025-04-03T23:43:04.163+09:00  INFO 45581 --- [quest-handler-4] kafka.log.LogManager                     : Created log for partition __consumer_offsets-0 in /var/folders/qb/nxyw0kj549bbf87mx3ss8vgc0000gn/T/spring.kafka.0f9f4ec2-491a-4eb8-ba3a-35f677f610e66802667832673744764/__consumer_offsets-0 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600}
2025-04-03T23:43:04.163+09:00  INFO 45581 --- [quest-handler-4] kafka.cluster.Partition                  : [Partition __consumer_offsets-0 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-0
2025-04-03T23:43:04.163+09:00  INFO 45581 --- [quest-handler-4] kafka.cluster.Partition                  : [Partition __consumer_offsets-0 broker=0] Log loaded for partition __consumer_offsets-0 with initial high watermark 0
2025-04-03T23:43:04.163+09:00  INFO 45581 --- [quest-handler-4] state.change.logger                      : [Broker id=0] Leader __consumer_offsets-0 with topic id Some(ohc0nNHLRBikISM55NBYow) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [0], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1.
2025-04-03T23:43:04.163+09:00  INFO 45581 --- [    Test worker] o.a.z.server.persistence.FileTxnSnapLog  : zookeeper.snapshot.trust.empty : false
2025-04-03T23:43:04.164+09:00  INFO 45581 --- [    Test worker] o.a.z.server.watch.WatchManagerFactory   : Using org.apache.zookeeper.server.watch.WatchManager as watch manager
2025-04-03T23:43:04.164+09:00  INFO 45581 --- [    Test worker] o.a.z.server.watch.WatchManagerFactory   : Using org.apache.zookeeper.server.watch.WatchManager as watch manager
2025-04-03T23:43:04.164+09:00  INFO 45581 --- [    Test worker] org.apache.zookeeper.server.ZKDatabase   : zookeeper.snapshotSizeFactor = 0.33
2025-04-03T23:43:04.164+09:00  INFO 45581 --- [    Test worker] org.apache.zookeeper.server.ZKDatabase   : zookeeper.commitLogCount=500
2025-04-03T23:43:04.164+09:00  INFO 45581 --- [    Test worker] o.a.zookeeper.server.ZooKeeperServer     : minSessionTimeout set to 1600 ms
2025-04-03T23:43:04.164+09:00  INFO 45581 --- [    Test worker] o.a.zookeeper.server.ZooKeeperServer     : maxSessionTimeout set to 16000 ms
2025-04-03T23:43:04.164+09:00  INFO 45581 --- [    Test worker] o.apache.zookeeper.server.ResponseCache  : getData response cache size is initialized with value 400.
2025-04-03T23:43:04.164+09:00  INFO 45581 --- [    Test worker] o.apache.zookeeper.server.ResponseCache  : getChildren response cache size is initialized with value 400.
2025-04-03T23:43:04.164+09:00  INFO 45581 --- [    Test worker] o.a.z.s.u.RequestPathMetricsCollector    : zookeeper.pathStats.slotCapacity = 60
2025-04-03T23:43:04.164+09:00  INFO 45581 --- [    Test worker] o.a.z.s.u.RequestPathMetricsCollector    : zookeeper.pathStats.slotDuration = 15
2025-04-03T23:43:04.164+09:00  INFO 45581 --- [    Test worker] o.a.z.s.u.RequestPathMetricsCollector    : zookeeper.pathStats.maxDepth = 6
2025-04-03T23:43:04.164+09:00  INFO 45581 --- [    Test worker] o.a.z.s.u.RequestPathMetricsCollector    : zookeeper.pathStats.initialDelay = 5
2025-04-03T23:43:04.164+09:00  INFO 45581 --- [    Test worker] o.a.z.s.u.RequestPathMetricsCollector    : zookeeper.pathStats.delay = 5
2025-04-03T23:43:04.165+09:00  INFO 45581 --- [    Test worker] o.a.z.s.u.RequestPathMetricsCollector    : zookeeper.pathStats.enabled = false
2025-04-03T23:43:04.165+09:00  INFO 45581 --- [    Test worker] o.a.zookeeper.server.ZooKeeperServer     : The max bytes for all large requests are set to 104857600
2025-04-03T23:43:04.165+09:00  INFO 45581 --- [    Test worker] o.a.zookeeper.server.ZooKeeperServer     : The large request threshold is set to -1
2025-04-03T23:43:04.165+09:00  INFO 45581 --- [    Test worker] o.a.z.server.AuthenticationHelper        : zookeeper.enforce.auth.enabled = false
2025-04-03T23:43:04.165+09:00  INFO 45581 --- [    Test worker] o.a.z.server.AuthenticationHelper        : zookeeper.enforce.auth.schemes = []
2025-04-03T23:43:04.165+09:00  INFO 45581 --- [    Test worker] o.a.zookeeper.server.ZooKeeperServer     : Created server with tickTime 800 ms minSessionTimeout 1600 ms maxSessionTimeout 16000 ms clientPortListenBacklog -1 datadir /var/folders/qb/nxyw0kj549bbf87mx3ss8vgc0000gn/T/kafka-12392772511521020833/version-2 snapdir /var/folders/qb/nxyw0kj549bbf87mx3ss8vgc0000gn/T/kafka-11008909457358635018/version-2
2025-04-03T23:43:04.165+09:00  WARN 45581 --- [    Test worker] o.a.zookeeper.server.ServerCnxnFactory   : maxCnxns is not configured, using default value 0.
2025-04-03T23:43:04.165+09:00  INFO 45581 --- [    Test worker] o.a.z.server.NIOServerCnxnFactory        : Configuring NIO connection handler with 10s sessionless connection timeout, 2 selector thread(s), 20 worker threads, and 64 kB direct buffers.
2025-04-03T23:43:04.165+09:00  INFO 45581 --- [    Test worker] o.a.z.server.NIOServerCnxnFactory        : binding to port /127.0.0.1:0
2025-04-03T23:43:04.166+09:00  INFO 45581 --- [    Test worker] o.a.z.server.persistence.FileTxnSnapLog  : Snapshotting: 0x0 to /var/folders/qb/nxyw0kj549bbf87mx3ss8vgc0000gn/T/kafka-11008909457358635018/version-2/snapshot.0
2025-04-03T23:43:04.168+09:00  INFO 45581 --- [    Test worker] org.apache.zookeeper.server.ZKDatabase   : Snapshot loaded in 3 ms, highest zxid is 0x0, digest is 1371985504
2025-04-03T23:43:04.168+09:00  INFO 45581 --- [    Test worker] o.a.z.server.persistence.FileTxnSnapLog  : Snapshotting: 0x0 to /var/folders/qb/nxyw0kj549bbf87mx3ss8vgc0000gn/T/kafka-11008909457358635018/version-2/snapshot.0
2025-04-03T23:43:04.171+09:00  INFO 45581 --- [    Test worker] o.a.zookeeper.server.ZooKeeperServer     : Snapshot taken in 3 ms
2025-04-03T23:43:04.171+09:00  INFO 45581 --- [quest-handler-4] k.coordinator.group.GroupCoordinator     : [GroupCoordinator 0]: Elected as the group coordinator for partition 3 in epoch 0
2025-04-03T23:43:04.171+09:00  INFO 45581 --- [0 cport:58615):] o.a.z.server.PrepRequestProcessor        : PrepRequestProcessor (sid:0) started, reconfigEnabled=false
2025-04-03T23:43:04.171+09:00  INFO 45581 --- [quest-handler-4] k.c.group.GroupMetadataManager           : [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-3 for epoch 0
2025-04-03T23:43:04.171+09:00  INFO 45581 --- [quest-handler-4] k.coordinator.group.GroupCoordinator     : [GroupCoordinator 0]: Elected as the group coordinator for partition 2 in epoch 0
2025-04-03T23:43:04.171+09:00  INFO 45581 --- [quest-handler-4] k.c.group.GroupMetadataManager           : [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-2 for epoch 0
2025-04-03T23:43:04.171+09:00  INFO 45581 --- [quest-handler-4] k.coordinator.group.GroupCoordinator     : [GroupCoordinator 0]: Elected as the group coordinator for partition 4 in epoch 0
2025-04-03T23:43:04.171+09:00  INFO 45581 --- [quest-handler-4] k.c.group.GroupMetadataManager           : [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-4 for epoch 0
2025-04-03T23:43:04.171+09:00  INFO 45581 --- [quest-handler-4] k.coordinator.group.GroupCoordinator     : [GroupCoordinator 0]: Elected as the group coordinator for partition 1 in epoch 0
2025-04-03T23:43:04.171+09:00  INFO 45581 --- [quest-handler-4] k.c.group.GroupMetadataManager           : [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-1 for epoch 0
2025-04-03T23:43:04.171+09:00  INFO 45581 --- [quest-handler-4] k.coordinator.group.GroupCoordinator     : [GroupCoordinator 0]: Elected as the group coordinator for partition 0 in epoch 0
2025-04-03T23:43:04.171+09:00  INFO 45581 --- [quest-handler-4] k.c.group.GroupMetadataManager           : [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-0 for epoch 0
2025-04-03T23:43:04.172+09:00  INFO 45581 --- [quest-handler-4] state.change.logger                      : [Broker id=0] Finished LeaderAndIsr request in 51ms correlationId 5 from controller 0 for 5 partitions
2025-04-03T23:43:04.172+09:00  INFO 45581 --- [    Test worker] kafka.server.KafkaConfig                 : KafkaConfig values:
	advertised.listeners = null
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name =
	auto.create.topics.enable = true
	auto.include.jmx.reporter = true
	auto.leader.rebalance.enable = true
	background.threads = 2
	broker.heartbeat.interval.ms = 2000
	broker.id = 0
	broker.id.generation.enable = true
	broker.rack = null
	broker.session.timeout.ms = 9000
	client.quota.callback.class = null
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = producer
	compression.zstd.level = 3
	connection.failed.authentication.delay.ms = 100
	connections.max.idle.ms = 600000
	connections.max.reauth.ms = 0
	control.plane.listener.name = null
	controlled.shutdown.enable = false
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 100
	controller.listener.names = null
	controller.quorum.append.linger.ms = 25
	controller.quorum.bootstrap.servers = []
	controller.quorum.election.backoff.max.ms = 1000
	controller.quorum.election.timeout.ms = 1000
	controller.quorum.fetch.timeout.ms = 2000
	controller.quorum.request.timeout.ms = 2000
	controller.quorum.retry.backoff.ms = 20
	controller.quorum.voters = []
	controller.quota.window.num = 11
	controller.quota.window.size.seconds = 1
	controller.socket.timeout.ms = 1000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delegation.token.secret.key = null
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	early.start.listeners = null
	eligible.leader.replicas.enable = false
	fetch.max.bytes = 57671680
	fetch.purgatory.purge.interval.requests = 1000
	group.consumer.assignors = [org.apache.kafka.coordinator.group.assignor.UniformAssignor, org.apache.kafka.coordinator.group.assignor.RangeAssignor]
	group.consumer.heartbeat.interval.ms = 5000
	group.consumer.max.heartbeat.interval.ms = 15000
	group.consumer.max.session.timeout.ms = 60000
	group.consumer.max.size = 2147483647
	group.consumer.migration.policy = disabled
	group.consumer.min.heartbeat.interval.ms = 5000
	group.consumer.min.session.timeout.ms = 45000
	group.consumer.session.timeout.ms = 45000
	group.coordinator.append.linger.ms = 10
	group.coordinator.new.enable = false
	group.coordinator.rebalance.protocols = [classic]
	group.coordinator.threads = 1
	group.initial.rebalance.delay.ms = 0
	group.max.session.timeout.ms = 1800000
	group.max.size = 2147483647
	group.min.session.timeout.ms = 6000
	initial.broker.registration.timeout.ms = 60000
	inter.broker.listener.name = null
	inter.broker.protocol.version = 3.8-IV0
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = SASL_SSL:SASL_SSL,PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT
	listeners = PLAINTEXT://localhost:0
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 2097152
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.max.compaction.lag.ms = 9223372036854775807
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /var/folders/qb/nxyw0kj549bbf87mx3ss8vgc0000gn/T/spring.kafka.d6ee9d52-45d4-418e-bc16-f6488da9fcbf7176368033373570203
	log.dir.failure.timeout.ms = 30000
	log.dirs = null
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.initial.task.delay.ms = 30000
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	log.message.downconversion.enable = true
	log.message.format.version = 3.0-IV1
	log.message.timestamp.after.max.ms = 9223372036854775807
	log.message.timestamp.before.max.ms = 9223372036854775807
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 1000
	max.connection.creation.rate = 2147483647
	max.connections = 2147483647
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides =
	max.incremental.fetch.session.cache.slots = 1000
	max.request.partition.size.limit = 2000
	message.max.bytes = 1048588
	metadata.log.dir = null
	metadata.log.max.record.bytes.between.snapshots = 20971520
	metadata.log.max.snapshot.interval.ms = 3600000
	metadata.log.segment.bytes = 1073741824
	metadata.log.segment.min.bytes = 8388608
	metadata.log.segment.ms = 604800000
	metadata.max.idle.interval.ms = 500
	metadata.max.retention.bytes = 104857600
	metadata.max.retention.ms = 604800000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	node.id = 0
	num.io.threads = 8
	num.network.threads = 2
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 5
	offsets.topic.replication.factor = 1
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	principal.builder.class = class org.apache.kafka.common.security.authenticator.DefaultKafkaPrincipalBuilder
	process.roles = []
	producer.id.expiration.check.interval.ms = 600000
	producer.id.expiration.ms = 86400000
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.window.num = 11
	quota.window.size.seconds = 1
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 9223372036854775807
	replica.lag.time.max.ms = 30000
	replica.selector.class = null
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 1000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism.controller.protocol = GSSAPI
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	sasl.server.callback.handler.class = null
	sasl.server.max.receive.size = 524288
	security.inter.broker.protocol = PLAINTEXT
	security.providers = null
	server.max.startup.time.ms = 9223372036854775807
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	socket.listen.backlog.size = 50
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.allow.dn.changes = false
	ssl.allow.san.changes = false
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.principal.mapping.rules = DEFAULT
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	telemetry.max.bytes = 1048576
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 10000
	transaction.max.timeout.ms = 900000
	transaction.partition.verification.enable = true
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 2
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 1
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	unstable.api.versions.enable = false
	unstable.feature.versions.enable = true
	zookeeper.clientCnxnSocket = null
	zookeeper.connect = 127.0.0.1:58615
	zookeeper.connection.timeout.ms = 10000
	zookeeper.max.in.flight.requests = 10
	zookeeper.metadata.migration.enable = false
	zookeeper.metadata.migration.min.batch.size = 200
	zookeeper.session.timeout.ms = 18000
	zookeeper.set.acl = false
	zookeeper.ssl.cipher.suites = null
	zookeeper.ssl.client.enable = false
	zookeeper.ssl.crl.enable = false
	zookeeper.ssl.enabled.protocols = null
	zookeeper.ssl.endpoint.identification.algorithm = HTTPS
	zookeeper.ssl.keystore.location = null
	zookeeper.ssl.keystore.password = null
	zookeeper.ssl.keystore.type = null
	zookeeper.ssl.ocsp.enable = false
	zookeeper.ssl.protocol = TLSv1.2
	zookeeper.ssl.truststore.location = null
	zookeeper.ssl.truststore.password = null
	zookeeper.ssl.truststore.type = null

2025-04-03T23:43:04.173+09:00  INFO 45581 --- [    Test worker] o.a.k.s.l.r.s.RemoteLogManagerConfig     : RemoteLogManagerConfig values:
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false

2025-04-03T23:43:04.173+09:00  INFO 45581 --- [quest-handler-3] state.change.logger                      : [Broker id=0] Add 5 partitions and deleted 0 partitions from metadata cache in response to UpdateMetadata request sent by controller 0 epoch 1 with correlation id 6
2025-04-03T23:43:04.174+09:00  INFO 45581 --- [adata-manager-0] k.c.group.GroupMetadataManager           : [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-3 in 2 milliseconds for epoch 0, of which 1 milliseconds was spent in the scheduler.
2025-04-03T23:43:04.174+09:00  INFO 45581 --- [    Test worker] kafka.server.KafkaServer                 : starting
2025-04-03T23:43:04.174+09:00  INFO 45581 --- [    Test worker] kafka.server.KafkaServer                 : Connecting to zookeeper on 127.0.0.1:58615
2025-04-03T23:43:04.174+09:00  INFO 45581 --- [adata-manager-0] k.c.group.GroupMetadataManager           : [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-2 in 3 milliseconds for epoch 0, of which 3 milliseconds was spent in the scheduler.
2025-04-03T23:43:04.174+09:00  INFO 45581 --- [adata-manager-0] k.c.group.GroupMetadataManager           : [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-4 in 3 milliseconds for epoch 0, of which 3 milliseconds was spent in the scheduler.
2025-04-03T23:43:04.174+09:00  INFO 45581 --- [adata-manager-0] k.c.group.GroupMetadataManager           : [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-1 in 3 milliseconds for epoch 0, of which 3 milliseconds was spent in the scheduler.
2025-04-03T23:43:04.174+09:00  INFO 45581 --- [    Test worker] kafka.zookeeper.ZooKeeperClient          : [ZooKeeperClient Kafka server] Initializing a new session to 127.0.0.1:58615.
2025-04-03T23:43:04.174+09:00  INFO 45581 --- [adata-manager-0] k.c.group.GroupMetadataManager           : [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-0 in 3 milliseconds for epoch 0, of which 3 milliseconds was spent in the scheduler.
2025-04-03T23:43:04.174+09:00  INFO 45581 --- [    Test worker] org.apache.zookeeper.ZooKeeper           : Initiating client connection, connectString=127.0.0.1:58615 sessionTimeout=18000 watcher=kafka.zookeeper.ZooKeeperClient$ZooKeeperClientWatcher$@439054a7
2025-04-03T23:43:04.174+09:00  INFO 45581 --- [    Test worker] org.apache.zookeeper.ClientCnxnSocket    : jute.maxbuffer value is 4194304 Bytes
2025-04-03T23:43:04.174+09:00  INFO 45581 --- [    Test worker] org.apache.zookeeper.ClientCnxn          : zookeeper.request.timeout value is 0. feature enabled=false
2025-04-03T23:43:04.175+09:00  INFO 45581 --- [27.0.0.1:58615)] org.apache.zookeeper.ClientCnxn          : Opening socket connection to server /127.0.0.1:58615.
2025-04-03T23:43:04.175+09:00  INFO 45581 --- [    Test worker] kafka.zookeeper.ZooKeeperClient          : [ZooKeeperClient Kafka server] Waiting until connected.
2025-04-03T23:43:04.175+09:00  INFO 45581 --- [27.0.0.1:58615)] org.apache.zookeeper.ClientCnxn          : Socket connection established, initiating session, client: /127.0.0.1:58616, server: /127.0.0.1:58615
2025-04-03T23:43:04.176+09:00  INFO 45581 --- [   SyncThread:0] o.a.z.server.persistence.FileTxnLog      : Creating new log file: log.1
2025-04-03T23:43:04.177+09:00  INFO 45581 --- [27.0.0.1:58615)] org.apache.zookeeper.ClientCnxn          : Session establishment complete on server /127.0.0.1:58615, session id = 0x100090a9a1d0000, negotiated timeout = 16000
2025-04-03T23:43:04.177+09:00  INFO 45581 --- [    Test worker] kafka.zookeeper.ZooKeeperClient          : [ZooKeeperClient Kafka server] Connected.
2025-04-03T23:43:04.189+09:00  INFO 45581 --- [    Test worker] kafka.server.KafkaServer                 : Cluster ID = dQrwwGaZSdGniJ1RPZEy7w
2025-04-03T23:43:04.190+09:00  INFO 45581 --- [    Test worker] o.a.k.s.l.r.s.RemoteLogManagerConfig     : RemoteLogManagerConfig values:
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false

2025-04-03T23:43:04.192+09:00  INFO 45581 --- [    Test worker] o.a.k.s.l.r.s.RemoteLogManagerConfig     : RemoteLogManagerConfig values:
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false

2025-04-03T23:43:04.193+09:00  INFO 45581 --- [    Test worker] kafka.server.KafkaConfig                 : KafkaConfig values:
	advertised.listeners = null
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name =
	auto.create.topics.enable = true
	auto.include.jmx.reporter = true
	auto.leader.rebalance.enable = true
	background.threads = 2
	broker.heartbeat.interval.ms = 2000
	broker.id = 0
	broker.id.generation.enable = true
	broker.rack = null
	broker.session.timeout.ms = 9000
	client.quota.callback.class = null
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = producer
	compression.zstd.level = 3
	connection.failed.authentication.delay.ms = 100
	connections.max.idle.ms = 600000
	connections.max.reauth.ms = 0
	control.plane.listener.name = null
	controlled.shutdown.enable = false
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 100
	controller.listener.names = null
	controller.quorum.append.linger.ms = 25
	controller.quorum.bootstrap.servers = []
	controller.quorum.election.backoff.max.ms = 1000
	controller.quorum.election.timeout.ms = 1000
	controller.quorum.fetch.timeout.ms = 2000
	controller.quorum.request.timeout.ms = 2000
	controller.quorum.retry.backoff.ms = 20
	controller.quorum.voters = []
	controller.quota.window.num = 11
	controller.quota.window.size.seconds = 1
	controller.socket.timeout.ms = 1000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delegation.token.secret.key = null
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	early.start.listeners = null
	eligible.leader.replicas.enable = false
	fetch.max.bytes = 57671680
	fetch.purgatory.purge.interval.requests = 1000
	group.consumer.assignors = [org.apache.kafka.coordinator.group.assignor.UniformAssignor, org.apache.kafka.coordinator.group.assignor.RangeAssignor]
	group.consumer.heartbeat.interval.ms = 5000
	group.consumer.max.heartbeat.interval.ms = 15000
	group.consumer.max.session.timeout.ms = 60000
	group.consumer.max.size = 2147483647
	group.consumer.migration.policy = disabled
	group.consumer.min.heartbeat.interval.ms = 5000
	group.consumer.min.session.timeout.ms = 45000
	group.consumer.session.timeout.ms = 45000
	group.coordinator.append.linger.ms = 10
	group.coordinator.new.enable = false
	group.coordinator.rebalance.protocols = [classic]
	group.coordinator.threads = 1
	group.initial.rebalance.delay.ms = 0
	group.max.session.timeout.ms = 1800000
	group.max.size = 2147483647
	group.min.session.timeout.ms = 6000
	initial.broker.registration.timeout.ms = 60000
	inter.broker.listener.name = null
	inter.broker.protocol.version = 3.8-IV0
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = SASL_SSL:SASL_SSL,PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT
	listeners = PLAINTEXT://localhost:0
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 2097152
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.max.compaction.lag.ms = 9223372036854775807
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /var/folders/qb/nxyw0kj549bbf87mx3ss8vgc0000gn/T/spring.kafka.d6ee9d52-45d4-418e-bc16-f6488da9fcbf7176368033373570203
	log.dir.failure.timeout.ms = 30000
	log.dirs = null
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.initial.task.delay.ms = 30000
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	log.message.downconversion.enable = true
	log.message.format.version = 3.0-IV1
	log.message.timestamp.after.max.ms = 9223372036854775807
	log.message.timestamp.before.max.ms = 9223372036854775807
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 1000
	max.connection.creation.rate = 2147483647
	max.connections = 2147483647
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides =
	max.incremental.fetch.session.cache.slots = 1000
	max.request.partition.size.limit = 2000
	message.max.bytes = 1048588
	metadata.log.dir = null
	metadata.log.max.record.bytes.between.snapshots = 20971520
	metadata.log.max.snapshot.interval.ms = 3600000
	metadata.log.segment.bytes = 1073741824
	metadata.log.segment.min.bytes = 8388608
	metadata.log.segment.ms = 604800000
	metadata.max.idle.interval.ms = 500
	metadata.max.retention.bytes = 104857600
	metadata.max.retention.ms = 604800000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	node.id = 0
	num.io.threads = 8
	num.network.threads = 2
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 5
	offsets.topic.replication.factor = 1
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	principal.builder.class = class org.apache.kafka.common.security.authenticator.DefaultKafkaPrincipalBuilder
	process.roles = []
	producer.id.expiration.check.interval.ms = 600000
	producer.id.expiration.ms = 86400000
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.window.num = 11
	quota.window.size.seconds = 1
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 9223372036854775807
	replica.lag.time.max.ms = 30000
	replica.selector.class = null
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 1000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism.controller.protocol = GSSAPI
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	sasl.server.callback.handler.class = null
	sasl.server.max.receive.size = 524288
	security.inter.broker.protocol = PLAINTEXT
	security.providers = null
	server.max.startup.time.ms = 9223372036854775807
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	socket.listen.backlog.size = 50
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.allow.dn.changes = false
	ssl.allow.san.changes = false
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.principal.mapping.rules = DEFAULT
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	telemetry.max.bytes = 1048576
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 10000
	transaction.max.timeout.ms = 900000
	transaction.partition.verification.enable = true
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 2
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 1
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	unstable.api.versions.enable = false
	unstable.feature.versions.enable = true
	zookeeper.clientCnxnSocket = null
	zookeeper.connect = 127.0.0.1:58615
	zookeeper.connection.timeout.ms = 10000
	zookeeper.max.in.flight.requests = 10
	zookeeper.metadata.migration.enable = false
	zookeeper.metadata.migration.min.batch.size = 200
	zookeeper.session.timeout.ms = 18000
	zookeeper.set.acl = false
	zookeeper.ssl.cipher.suites = null
	zookeeper.ssl.client.enable = false
	zookeeper.ssl.crl.enable = false
	zookeeper.ssl.enabled.protocols = null
	zookeeper.ssl.endpoint.identification.algorithm = HTTPS
	zookeeper.ssl.keystore.location = null
	zookeeper.ssl.keystore.password = null
	zookeeper.ssl.keystore.type = null
	zookeeper.ssl.ocsp.enable = false
	zookeeper.ssl.protocol = TLSv1.2
	zookeeper.ssl.truststore.location = null
	zookeeper.ssl.truststore.password = null
	zookeeper.ssl.truststore.type = null

2025-04-03T23:43:04.195+09:00  INFO 45581 --- [    Test worker] o.a.k.s.l.r.s.RemoteLogManagerConfig     : RemoteLogManagerConfig values:
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false

2025-04-03T23:43:04.202+09:00  INFO 45581 --- [nelReaper-Fetch] lientQuotaManager$ThrottledChannelReaper : [ThrottledChannelReaper-Fetch]: Starting
2025-04-03T23:43:04.202+09:00  INFO 45581 --- [lReaper-Produce] lientQuotaManager$ThrottledChannelReaper : [ThrottledChannelReaper-Produce]: Starting
2025-04-03T23:43:04.202+09:00  INFO 45581 --- [lReaper-Request] lientQuotaManager$ThrottledChannelReaper : [ThrottledChannelReaper-Request]: Starting
2025-04-03T23:43:04.202+09:00  INFO 45581 --- [trollerMutation] lientQuotaManager$ThrottledChannelReaper : [ThrottledChannelReaper-ControllerMutation]: Starting
2025-04-03T23:43:04.202+09:00  INFO 45581 --- [    Test worker] kafka.server.KafkaServer                 : [KafkaServer id=0] Rewriting /var/folders/qb/nxyw0kj549bbf87mx3ss8vgc0000gn/T/spring.kafka.d6ee9d52-45d4-418e-bc16-f6488da9fcbf7176368033373570203/meta.properties
2025-04-03T23:43:04.208+09:00  INFO 45581 --- [    Test worker] kafka.log.LogManager                     : Loading logs from log dirs ArrayBuffer(/var/folders/qb/nxyw0kj549bbf87mx3ss8vgc0000gn/T/spring.kafka.d6ee9d52-45d4-418e-bc16-f6488da9fcbf7176368033373570203)
2025-04-03T23:43:04.209+09:00  INFO 45581 --- [    Test worker] kafka.log.LogManager                     : No logs found to be loaded in /var/folders/qb/nxyw0kj549bbf87mx3ss8vgc0000gn/T/spring.kafka.d6ee9d52-45d4-418e-bc16-f6488da9fcbf7176368033373570203
2025-04-03T23:43:04.209+09:00  INFO 45581 --- [    Test worker] kafka.log.LogManager                     : Loaded 0 logs in 1ms
2025-04-03T23:43:04.209+09:00  INFO 45581 --- [    Test worker] kafka.log.LogManager                     : Starting log cleanup with a period of 300000 ms.
2025-04-03T23:43:04.209+09:00  INFO 45581 --- [    Test worker] kafka.log.LogManager                     : Starting log flusher with a default period of 9223372036854775807 ms.
2025-04-03T23:43:04.210+09:00  INFO 45581 --- [    Test worker] kafka.log.LogCleaner                     : Starting the log cleaner
2025-04-03T23:43:04.211+09:00  INFO 45581 --- [leaner-thread-0] kafka.log.LogCleaner$CleanerThread       : [kafka-log-cleaner-thread-0]: Starting
2025-04-03T23:43:04.211+09:00  INFO 45581 --- [-process-thread] stener$ChangeNotificationProcessorThread : [feature-zk-node-event-process-thread]: Starting
2025-04-03T23:43:04.212+09:00  INFO 45581 --- [-process-thread] k.server.FinalizedFeatureChangeListener  : Feature ZK node at path: /feature does not exist
2025-04-03T23:43:04.213+09:00  INFO 45581 --- [channel-manager] k.server.NodeToControllerRequestThread   : [zk-broker-0-to-controller-forwarding-channel-manager]: Starting
2025-04-03T23:43:04.220+09:00  INFO 45581 --- [    Test worker] kafka.network.ConnectionQuotas           : Updated connection-accept-rate max connection creation rate to 2147483647
2025-04-03T23:43:04.220+09:00  INFO 45581 --- [    Test worker] kafka.network.DataPlaneAcceptor          : Awaiting socket connections on localhost:58617.
2025-04-03T23:43:04.220+09:00  INFO 45581 --- [    Test worker] kafka.network.DataPlaneAcceptor          : Opened wildcard endpoint localhost:58617
2025-04-03T23:43:04.220+09:00  INFO 45581 --- [    Test worker] kafka.network.SocketServer               : [SocketServer listenerType=ZK_BROKER, nodeId=0] Created data-plane acceptor and processors for endpoint : ListenerName(PLAINTEXT)
2025-04-03T23:43:04.221+09:00  INFO 45581 --- [channel-manager] k.server.NodeToControllerRequestThread   : [zk-broker-0-to-controller-alter-partition-channel-manager]: Starting
2025-04-03T23:43:04.221+09:00  INFO 45581 --- [eaper-0-Produce] perationPurgatory$ExpiredOperationReaper : [ExpirationReaper-0-Produce]: Starting
2025-04-03T23:43:04.221+09:00  INFO 45581 --- [nReaper-0-Fetch] perationPurgatory$ExpiredOperationReaper : [ExpirationReaper-0-Fetch]: Starting
2025-04-03T23:43:04.221+09:00  INFO 45581 --- [0-DeleteRecords] perationPurgatory$ExpiredOperationReaper : [ExpirationReaper-0-DeleteRecords]: Starting
2025-04-03T23:43:04.222+09:00  INFO 45581 --- [r-0-ElectLeader] perationPurgatory$ExpiredOperationReaper : [ExpirationReaper-0-ElectLeader]: Starting
2025-04-03T23:43:04.222+09:00  INFO 45581 --- [r-0-RemoteFetch] perationPurgatory$ExpiredOperationReaper : [ExpirationReaper-0-RemoteFetch]: Starting
2025-04-03T23:43:04.223+09:00  INFO 45581 --- [rFailureHandler] k.s.ReplicaManager$LogDirFailureHandler  : [LogDirFailureHandler]: Starting
2025-04-03T23:43:04.223+09:00  INFO 45581 --- [nSenderThread-0] kafka.server.AddPartitionsToTxnManager   : [AddPartitionsToTxnSenderThread-0]: Starting
2025-04-03T23:43:04.224+09:00  INFO 45581 --- [    Test worker] kafka.zk.KafkaZkClient                   : Creating /brokers/ids/0 (is it secure? false)
2025-04-03T23:43:04.224+09:00  INFO 45581 --- [    Test worker] kafka.zk.KafkaZkClient                   : Stat of the created znode at /brokers/ids/0 is: 25,25,1743691384224,1743691384224,1,0,0,72067535177842688,204,0,25

2025-04-03T23:43:04.224+09:00  INFO 45581 --- [    Test worker] kafka.zk.KafkaZkClient                   : Registered broker 0 at path /brokers/ids/0 with addresses: PLAINTEXT://localhost:58617, czxid (broker epoch): 25
2025-04-03T23:43:04.225+09:00  INFO 45581 --- [er-event-thread] rollerEventManager$ControllerEventThread : [ControllerEventThread controllerId=0] Starting
2025-04-03T23:43:04.225+09:00  INFO 45581 --- [nReaper-0-topic] perationPurgatory$ExpiredOperationReaper : [ExpirationReaper-0-topic]: Starting
2025-04-03T23:43:04.225+09:00  INFO 45581 --- [per-0-Heartbeat] perationPurgatory$ExpiredOperationReaper : [ExpirationReaper-0-Heartbeat]: Starting
2025-04-03T23:43:04.225+09:00  INFO 45581 --- [per-0-Rebalance] perationPurgatory$ExpiredOperationReaper : [ExpirationReaper-0-Rebalance]: Starting
2025-04-03T23:43:04.225+09:00  INFO 45581 --- [    Test worker] k.coordinator.group.GroupCoordinator     : [GroupCoordinator 0]: Starting up.
2025-04-03T23:43:04.226+09:00  INFO 45581 --- [    Test worker] k.coordinator.group.GroupCoordinator     : [GroupCoordinator 0]: Startup complete.
2025-04-03T23:43:04.226+09:00  INFO 45581 --- [    Test worker] k.c.transaction.TransactionCoordinator   : [TransactionCoordinator id=0] Starting up.
2025-04-03T23:43:04.228+09:00  INFO 45581 --- [er-event-thread] kafka.zk.KafkaZkClient                   : Successfully created /controller_epoch with initial epoch 0
2025-04-03T23:43:04.228+09:00  INFO 45581 --- [    Test worker] k.c.transaction.TransactionCoordinator   : [TransactionCoordinator id=0] Startup complete.
2025-04-03T23:43:04.228+09:00  INFO 45581 --- [rSenderThread-0] k.c.t.TransactionMarkerChannelManager    : [TxnMarkerSenderThread-0]: Starting
2025-04-03T23:43:04.229+09:00  INFO 45581 --- [per-0-AlterAcls] perationPurgatory$ExpiredOperationReaper : [ExpirationReaper-0-AlterAcls]: Starting
2025-04-03T23:43:04.229+09:00  INFO 45581 --- [er-event-thread] kafka.controller.KafkaController         : [Controller id=0] 0 successfully elected as the controller. Epoch incremented to 1 and epoch zk version is now 1
2025-04-03T23:43:04.229+09:00  INFO 45581 --- [er-event-thread] kafka.controller.KafkaController         : [Controller id=0] Creating FeatureZNode at path: /feature with contents: FeatureZNode(2,Enabled,Map())
2025-04-03T23:43:04.230+09:00  INFO 45581 --- [ker-EventThread] k.server.FinalizedFeatureChangeListener  : Feature ZK node created at path: /feature
2025-04-03T23:43:04.230+09:00  INFO 45581 --- [-process-thread] icationListener$ChangeEventProcessThread : [/config/changes-event-process-thread]: Starting
2025-04-03T23:43:04.231+09:00  INFO 45581 --- [er-event-thread] kafka.controller.KafkaController         : [Controller id=0] Registering handlers
2025-04-03T23:43:04.231+09:00  INFO 45581 --- [-process-thread] kafka.server.metadata.ZkMetadataCache    : [MetadataCache brokerId=0] Updated cache from existing None to latest Features(metadataVersion=3.8-IV0, finalizedFeatures={}, finalizedFeaturesEpoch=0).
2025-04-03T23:43:04.232+09:00  INFO 45581 --- [er-event-thread] kafka.controller.KafkaController         : [Controller id=0] Deleting log dir event notifications
2025-04-03T23:43:04.232+09:00  INFO 45581 --- [er-event-thread] kafka.controller.KafkaController         : [Controller id=0] Deleting isr change notifications
2025-04-03T23:43:04.232+09:00  INFO 45581 --- [    Test worker] kafka.network.SocketServer               : [SocketServer listenerType=ZK_BROKER, nodeId=0] Enabling request processing.
2025-04-03T23:43:04.232+09:00  INFO 45581 --- [er-event-thread] kafka.controller.KafkaController         : [Controller id=0] Initializing controller context
2025-04-03T23:43:04.233+09:00  INFO 45581 --- [    Test worker] kafka.server.KafkaServer                 : [KafkaServer id=0] Start processing authorizer futures
2025-04-03T23:43:04.233+09:00  INFO 45581 --- [    Test worker] kafka.server.KafkaServer                 : [KafkaServer id=0] End processing authorizer futures
2025-04-03T23:43:04.233+09:00  INFO 45581 --- [    Test worker] kafka.server.KafkaServer                 : [KafkaServer id=0] Start processing enable request processing future
2025-04-03T23:43:04.233+09:00  INFO 45581 --- [    Test worker] kafka.server.KafkaServer                 : [KafkaServer id=0] End processing enable request processing future
2025-04-03T23:43:04.233+09:00  INFO 45581 --- [    Test worker] o.a.kafka.common.utils.AppInfoParser     : The mbean of App info: [kafka.server], id: [0] already exists, so skipping a new mbean creation.
2025-04-03T23:43:04.233+09:00  INFO 45581 --- [    Test worker] kafka.server.KafkaServer                 : [KafkaServer id=0] started
2025-04-03T23:43:04.233+09:00  INFO 45581 --- [    Test worker] o.a.k.clients.admin.AdminClientConfig    : AdminClientConfig values:
	auto.include.jmx.reporter = true
	bootstrap.controllers = []
	bootstrap.servers = [127.0.0.1:58617]
	client.dns.lookup = use_all_dns_ips
	client.id =
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	enable.metrics.push = true
	metadata.max.age.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2025-04-03T23:43:04.234+09:00  INFO 45581 --- [    Test worker] o.a.kafka.common.utils.AppInfoParser     : Kafka version: 3.8.1
2025-04-03T23:43:04.234+09:00  INFO 45581 --- [    Test worker] o.a.kafka.common.utils.AppInfoParser     : Kafka commitId: 70d6ff42debf7e17
2025-04-03T23:43:04.234+09:00  INFO 45581 --- [    Test worker] o.a.kafka.common.utils.AppInfoParser     : Kafka startTimeMs: 1743691384234
2025-04-03T23:43:04.234+09:00  INFO 45581 --- [er-event-thread] kafka.controller.KafkaController         : [Controller id=0] Initialized broker epochs cache: HashMap(0 -> 25)
2025-04-03T23:43:04.237+09:00  INFO 45581 --- [er-event-thread] kafka.controller.KafkaController         : [Controller id=0] Currently active brokers in the cluster: Set(0)
2025-04-03T23:43:04.237+09:00  INFO 45581 --- [er-event-thread] kafka.controller.KafkaController         : [Controller id=0] Currently shutting brokers in the cluster: HashSet()
2025-04-03T23:43:04.237+09:00  INFO 45581 --- [er-event-thread] kafka.controller.KafkaController         : [Controller id=0] Current list of topics in the cluster: HashSet()
2025-04-03T23:43:04.237+09:00  INFO 45581 --- [er-event-thread] kafka.controller.KafkaController         : [Controller id=0] Fetching topic deletions in progress
2025-04-03T23:43:04.237+09:00  INFO 45581 --- [r-0-send-thread] kafka.controller.RequestSendThread       : [RequestSendThread controllerId=0] Starting
2025-04-03T23:43:04.237+09:00  INFO 45581 --- [er-event-thread] kafka.controller.KafkaController         : [Controller id=0] List of topics to be deleted:
2025-04-03T23:43:04.237+09:00  INFO 45581 --- [er-event-thread] kafka.controller.KafkaController         : [Controller id=0] List of topics ineligible for deletion:
2025-04-03T23:43:04.237+09:00  INFO 45581 --- [er-event-thread] kafka.controller.KafkaController         : [Controller id=0] Initializing topic deletion manager
2025-04-03T23:43:04.237+09:00  INFO 45581 --- [er-event-thread] kafka.controller.TopicDeletionManager    : [Topic Deletion Manager 0] Initializing manager with initial deletions: Set(), initial ineligible deletions: HashSet()
2025-04-03T23:43:04.237+09:00  INFO 45581 --- [er-event-thread] kafka.controller.KafkaController         : [Controller id=0] Sending update metadata request
2025-04-03T23:43:04.237+09:00  INFO 45581 --- [er-event-thread] state.change.logger                      : [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet(0) for 0 partitions
2025-04-03T23:43:04.237+09:00  INFO 45581 --- [er-event-thread] kafka.controller.ZkReplicaStateMachine   : [ReplicaStateMachine controllerId=0] Initializing replica state
2025-04-03T23:43:04.238+09:00  INFO 45581 --- [er-event-thread] kafka.controller.ZkReplicaStateMachine   : [ReplicaStateMachine controllerId=0] Triggering online replica state changes
2025-04-03T23:43:04.238+09:00  INFO 45581 --- [er-event-thread] kafka.controller.ZkReplicaStateMachine   : [ReplicaStateMachine controllerId=0] Triggering offline replica state changes
2025-04-03T23:43:04.238+09:00  INFO 45581 --- [er-event-thread] k.controller.ZkPartitionStateMachine     : [PartitionStateMachine controllerId=0] Initializing partition state
2025-04-03T23:43:04.238+09:00  INFO 45581 --- [er-event-thread] k.controller.ZkPartitionStateMachine     : [PartitionStateMachine controllerId=0] Triggering online partition state changes
2025-04-03T23:43:04.238+09:00  INFO 45581 --- [er-event-thread] kafka.controller.KafkaController         : [Controller id=0] Ready to serve as the new controller with epoch 1
2025-04-03T23:43:04.238+09:00  INFO 45581 --- [r-0-send-thread] kafka.controller.RequestSendThread       : [RequestSendThread controllerId=0] Controller 0 connected to localhost:58617 (id: 0 rack: null) for sending state change requests
2025-04-03T23:43:04.239+09:00  INFO 45581 --- [er-event-thread] kafka.controller.KafkaController         : [Controller id=0] Partitions undergoing preferred replica election:
2025-04-03T23:43:04.239+09:00  INFO 45581 --- [er-event-thread] kafka.controller.KafkaController         : [Controller id=0] Partitions that completed preferred replica election:
2025-04-03T23:43:04.239+09:00  INFO 45581 --- [er-event-thread] kafka.controller.KafkaController         : [Controller id=0] Skipping preferred replica election for partitions due to topic deletion:
2025-04-03T23:43:04.239+09:00  INFO 45581 --- [er-event-thread] kafka.controller.KafkaController         : [Controller id=0] Resuming preferred replica election for partitions:
2025-04-03T23:43:04.239+09:00  INFO 45581 --- [er-event-thread] kafka.controller.KafkaController         : [Controller id=0] Starting replica leader election (PREFERRED) for partitions  triggered by ZkTriggered
2025-04-03T23:43:04.239+09:00  INFO 45581 --- [er-event-thread] kafka.controller.KafkaController         : [Controller id=0] Starting the controller scheduler
2025-04-03T23:43:04.242+09:00  INFO 45581 --- [ntainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-2, groupId=my-group] Discovered group coordinator localhost:58606 (id: 2147483647 rack: null)
2025-04-03T23:43:04.243+09:00  INFO 45581 --- [ntainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-2, groupId=my-group] (Re-)joining group
2025-04-03T23:43:04.250+09:00  INFO 45581 --- [quest-handler-1] k.coordinator.group.GroupCoordinator     : [GroupCoordinator 0]: Dynamic member with unknown member id joins group my-group in Empty state. Created a new member id consumer-my-group-2-9e457335-9ab7-4874-a802-30d1f6e2a57b and request the member to rejoin with this id.
2025-04-03T23:43:04.250+09:00  INFO 45581 --- [ad | producer-1] o.a.k.c.p.internals.TransactionManager   : [Producer clientId=producer-1] ProducerId set to 0 with epoch 0
2025-04-03T23:43:04.251+09:00  INFO 45581 --- [ntainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-2, groupId=my-group] Request joining group due to: need to re-join with the given member-id: consumer-my-group-2-9e457335-9ab7-4874-a802-30d1f6e2a57b
2025-04-03T23:43:04.251+09:00  INFO 45581 --- [ntainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-2, groupId=my-group] (Re-)joining group
2025-04-03T23:43:04.253+09:00  INFO 45581 --- [quest-handler-4] k.coordinator.group.GroupCoordinator     : [GroupCoordinator 0]: Preparing to rebalance group my-group in state PreparingRebalance with old generation 0 (__consumer_offsets-2) (reason: Adding new member consumer-my-group-2-9e457335-9ab7-4874-a802-30d1f6e2a57b with group instance id None; client reason: need to re-join with the given member-id: consumer-my-group-2-9e457335-9ab7-4874-a802-30d1f6e2a57b)
2025-04-03T23:43:04.255+09:00  INFO 45581 --- [cutor-Rebalance] k.coordinator.group.GroupCoordinator     : [GroupCoordinator 0]: Stabilized group my-group generation 1 (__consumer_offsets-2) with 1 members
2025-04-03T23:43:04.256+09:00  INFO 45581 --- [ntainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-2, groupId=my-group] Successfully joined group with generation Generation{generationId=1, memberId='consumer-my-group-2-9e457335-9ab7-4874-a802-30d1f6e2a57b', protocol='range'}
2025-04-03T23:43:04.258+09:00  INFO 45581 --- [ntainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-2, groupId=my-group] Finished assignment for group at generation 1: {consumer-my-group-2-9e457335-9ab7-4874-a802-30d1f6e2a57b=Assignment(partitions=[my-topic-0])}
2025-04-03T23:43:04.261+09:00  INFO 45581 --- [quest-handler-5] k.coordinator.group.GroupCoordinator     : [GroupCoordinator 0]: Assignment received from leader consumer-my-group-2-9e457335-9ab7-4874-a802-30d1f6e2a57b for group my-group for generation 1. The group has 1 members, 0 of which are static.
2025-04-03T23:43:04.266+09:00  INFO 45581 --- [ntainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-2, groupId=my-group] Successfully synced group in generation Generation{generationId=1, memberId='consumer-my-group-2-9e457335-9ab7-4874-a802-30d1f6e2a57b', protocol='range'}
2025-04-03T23:43:04.266+09:00  INFO 45581 --- [ntainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-2, groupId=my-group] Notifying assignor about the new Assignment(partitions=[my-topic-0])
2025-04-03T23:43:04.267+09:00  INFO 45581 --- [ntainer#0-0-C-1] k.c.c.i.ConsumerRebalanceListenerInvoker : [Consumer clientId=consumer-my-group-2, groupId=my-group] Adding newly assigned partitions: my-topic-0
2025-04-03T23:43:04.273+09:00  INFO 45581 --- [ntainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-2, groupId=my-group] Found no committed offset for partition my-topic-0
2025-04-03T23:43:04.278+09:00  INFO 45581 --- [ntainer#0-0-C-1] o.a.k.c.c.internals.SubscriptionState    : [Consumer clientId=consumer-my-group-2, groupId=my-group] Resetting offset for partition my-topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:58606 (id: 0 rack: null)], epoch=0}}.
2025-04-03T23:43:04.279+09:00  INFO 45581 --- [ntainer#0-0-C-1] o.s.k.l.KafkaMessageListenerContainer    : my-group: partitions assigned: [my-topic-0]
>> message2
2025-04-03T23:43:04.314+09:00  INFO 45581 --- [channel-manager] k.server.NodeToControllerRequestThread   : [zk-broker-0-to-controller-forwarding-channel-manager]: Recorded new ZK controller, from now on will use node localhost:58617 (id: 0 rack: null)
2025-04-03T23:43:04.322+09:00  INFO 45581 --- [channel-manager] k.server.NodeToControllerRequestThread   : [zk-broker-0-to-controller-alter-partition-channel-manager]: Recorded new ZK controller, from now on will use node localhost:58617 (id: 0 rack: null)
2025-04-03T23:43:04.340+09:00  INFO 45581 --- [quest-handler-5] kafka.zk.AdminZkClient                   : Creating topic topic2 with configuration {} and initial partition assignment HashMap(0 -> ArrayBuffer(0))
2025-04-03T23:43:04.344+09:00  INFO 45581 --- [er-event-thread] kafka.controller.KafkaController         : [Controller id=0] New topics: [Set(topic2)], deleted topics: [HashSet()], new partition replica assignment [Set(TopicIdReplicaAssignment(topic2,Some(97WxkAbcTdu0DB8nUD2A9w),Map(topic2-0 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=))))]
2025-04-03T23:43:04.344+09:00  INFO 45581 --- [er-event-thread] kafka.controller.KafkaController         : [Controller id=0] New partition creation callback for topic2-0
2025-04-03T23:43:04.344+09:00  INFO 45581 --- [er-event-thread] state.change.logger                      : [Controller id=0 epoch=1] Changed partition topic2-0 state from NonExistentPartition to NewPartition with assigned replicas 0
2025-04-03T23:43:04.344+09:00  INFO 45581 --- [er-event-thread] state.change.logger                      : [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions
2025-04-03T23:43:04.344+09:00  INFO 45581 --- [er-event-thread] state.change.logger                      : [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions
2025-04-03T23:43:04.346+09:00  INFO 45581 --- [quest-handler-5] kafka.zk.AdminZkClient                   : Creating topic test-my-topic with configuration {} and initial partition assignment HashMap(0 -> ArrayBuffer(0))
2025-04-03T23:43:04.347+09:00  INFO 45581 --- [er-event-thread] state.change.logger                      : [Controller id=0 epoch=1] Changed partition topic2-0 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isrWithBrokerEpoch=List(BrokerState(brokerId=0, brokerEpoch=-1)), leaderRecoveryState=RECOVERED, partitionEpoch=0)
2025-04-03T23:43:04.348+09:00  INFO 45581 --- [er-event-thread] state.change.logger                      : [Controller id=0 epoch=1] Sending LeaderAndIsr request to broker 0 with 1 become-leader and 0 become-follower partitions
2025-04-03T23:43:04.348+09:00  INFO 45581 --- [er-event-thread] state.change.logger                      : [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet(0) for 1 partitions
2025-04-03T23:43:04.348+09:00  INFO 45581 --- [er-event-thread] state.change.logger                      : [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions
2025-04-03T23:43:04.349+09:00  INFO 45581 --- [quest-handler-6] state.change.logger                      : [Broker id=0] Handling LeaderAndIsr request correlationId 1 from controller 0 for 1 partitions
2025-04-03T23:43:04.349+09:00  INFO 45581 --- [quest-handler-6] kafka.server.ReplicaFetcherManager       : [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(topic2-0)
2025-04-03T23:43:04.349+09:00  INFO 45581 --- [quest-handler-6] state.change.logger                      : [Broker id=0] Stopped fetchers as part of LeaderAndIsr request correlationId 1 from controller 0 epoch 1 as part of the become-leader transition for 1 partitions
2025-04-03T23:43:04.351+09:00  INFO 45581 --- [er-event-thread] kafka.controller.KafkaController         : [Controller id=0] New topics: [Set(test-my-topic)], deleted topics: [HashSet()], new partition replica assignment [Set(TopicIdReplicaAssignment(test-my-topic,Some(VJnei8V5RTKMAibuqI7GUg),Map(test-my-topic-0 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=))))]
2025-04-03T23:43:04.351+09:00  INFO 45581 --- [er-event-thread] kafka.controller.KafkaController         : [Controller id=0] New partition creation callback for test-my-topic-0
2025-04-03T23:43:04.351+09:00  INFO 45581 --- [er-event-thread] state.change.logger                      : [Controller id=0 epoch=1] Changed partition test-my-topic-0 state from NonExistentPartition to NewPartition with assigned replicas 0
2025-04-03T23:43:04.351+09:00  INFO 45581 --- [er-event-thread] state.change.logger                      : [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions
2025-04-03T23:43:04.351+09:00  INFO 45581 --- [er-event-thread] state.change.logger                      : [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions
2025-04-03T23:43:04.353+09:00  INFO 45581 --- [er-event-thread] state.change.logger                      : [Controller id=0 epoch=1] Changed partition test-my-topic-0 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isrWithBrokerEpoch=List(BrokerState(brokerId=0, brokerEpoch=-1)), leaderRecoveryState=RECOVERED, partitionEpoch=0)
2025-04-03T23:43:04.353+09:00  INFO 45581 --- [er-event-thread] state.change.logger                      : [Controller id=0 epoch=1] Sending LeaderAndIsr request to broker 0 with 1 become-leader and 0 become-follower partitions
2025-04-03T23:43:04.353+09:00  INFO 45581 --- [er-event-thread] state.change.logger                      : [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet(0) for 1 partitions
2025-04-03T23:43:04.353+09:00  INFO 45581 --- [er-event-thread] state.change.logger                      : [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions
2025-04-03T23:43:04.354+09:00  INFO 45581 --- [quest-handler-6] kafka.log.UnifiedLog$                    : [LogLoader partition=topic2-0, dir=/var/folders/qb/nxyw0kj549bbf87mx3ss8vgc0000gn/T/spring.kafka.d6ee9d52-45d4-418e-bc16-f6488da9fcbf7176368033373570203] Loading producer state till offset 0 with message format version 2
2025-04-03T23:43:04.354+09:00  INFO 45581 --- [quest-handler-6] kafka.log.LogManager                     : Created log for partition topic2-0 in /var/folders/qb/nxyw0kj549bbf87mx3ss8vgc0000gn/T/spring.kafka.d6ee9d52-45d4-418e-bc16-f6488da9fcbf7176368033373570203/topic2-0 with properties {}
2025-04-03T23:43:04.355+09:00  INFO 45581 --- [quest-handler-6] kafka.cluster.Partition                  : [Partition topic2-0 broker=0] No checkpointed highwatermark is found for partition topic2-0
2025-04-03T23:43:04.355+09:00  INFO 45581 --- [quest-handler-6] kafka.cluster.Partition                  : [Partition topic2-0 broker=0] Log loaded for partition topic2-0 with initial high watermark 0
2025-04-03T23:43:04.355+09:00  INFO 45581 --- [quest-handler-6] state.change.logger                      : [Broker id=0] Leader topic2-0 with topic id Some(97WxkAbcTdu0DB8nUD2A9w) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [0], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1.
2025-04-03T23:43:04.362+09:00  INFO 45581 --- [quest-handler-6] state.change.logger                      : [Broker id=0] Finished LeaderAndIsr request in 13ms correlationId 1 from controller 0 for 1 partitions
2025-04-03T23:43:04.363+09:00  INFO 45581 --- [quest-handler-7] state.change.logger                      : [Broker id=0] Add 1 partitions and deleted 0 partitions from metadata cache in response to UpdateMetadata request sent by controller 0 epoch 1 with correlation id 2
2025-04-03T23:43:04.363+09:00  INFO 45581 --- [quest-handler-0] state.change.logger                      : [Broker id=0] Handling LeaderAndIsr request correlationId 3 from controller 0 for 1 partitions
2025-04-03T23:43:04.363+09:00  INFO 45581 --- [quest-handler-0] kafka.server.ReplicaFetcherManager       : [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(test-my-topic-0)
2025-04-03T23:43:04.363+09:00  INFO 45581 --- [quest-handler-0] state.change.logger                      : [Broker id=0] Stopped fetchers as part of LeaderAndIsr request correlationId 3 from controller 0 epoch 1 as part of the become-leader transition for 1 partitions
2025-04-03T23:43:04.367+09:00  INFO 45581 --- [quest-handler-0] kafka.log.UnifiedLog$                    : [LogLoader partition=test-my-topic-0, dir=/var/folders/qb/nxyw0kj549bbf87mx3ss8vgc0000gn/T/spring.kafka.d6ee9d52-45d4-418e-bc16-f6488da9fcbf7176368033373570203] Loading producer state till offset 0 with message format version 2
2025-04-03T23:43:04.367+09:00  INFO 45581 --- [quest-handler-0] kafka.log.LogManager                     : Created log for partition test-my-topic-0 in /var/folders/qb/nxyw0kj549bbf87mx3ss8vgc0000gn/T/spring.kafka.d6ee9d52-45d4-418e-bc16-f6488da9fcbf7176368033373570203/test-my-topic-0 with properties {}
2025-04-03T23:43:04.367+09:00  INFO 45581 --- [quest-handler-0] kafka.cluster.Partition                  : [Partition test-my-topic-0 broker=0] No checkpointed highwatermark is found for partition test-my-topic-0
2025-04-03T23:43:04.367+09:00  INFO 45581 --- [quest-handler-0] kafka.cluster.Partition                  : [Partition test-my-topic-0 broker=0] Log loaded for partition test-my-topic-0 with initial high watermark 0
2025-04-03T23:43:04.367+09:00  INFO 45581 --- [quest-handler-0] state.change.logger                      : [Broker id=0] Leader test-my-topic-0 with topic id Some(VJnei8V5RTKMAibuqI7GUg) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [0], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1.
2025-04-03T23:43:04.374+09:00  INFO 45581 --- [quest-handler-0] state.change.logger                      : [Broker id=0] Finished LeaderAndIsr request in 11ms correlationId 3 from controller 0 for 1 partitions
2025-04-03T23:43:04.374+09:00  INFO 45581 --- [quest-handler-1] state.change.logger                      : [Broker id=0] Add 1 partitions and deleted 0 partitions from metadata cache in response to UpdateMetadata request sent by controller 0 epoch 1 with correlation id 4
2025-04-03T23:43:04.375+09:00  INFO 45581 --- [| adminclient-2] o.a.kafka.common.utils.AppInfoParser     : App info kafka.admin.client for adminclient-2 unregistered
2025-04-03T23:43:04.375+09:00  INFO 45581 --- [| adminclient-2] o.apache.kafka.common.metrics.Metrics    : Metrics scheduler closed
2025-04-03T23:43:04.375+09:00  INFO 45581 --- [| adminclient-2] o.apache.kafka.common.metrics.Metrics    : Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-04-03T23:43:04.375+09:00  INFO 45581 --- [| adminclient-2] o.apache.kafka.common.metrics.Metrics    : Metrics reporters closed
2025-04-03T23:43:04.376+09:00  INFO 45581 --- [    Test worker] m.i.s.test2.KafkaTest3                   : Starting KafkaTest3 using Java 17.0.12 with PID 45581 (started by shlee in /Users/shlee/workspaces/study/iseunghan-Lab/spring-embedded-kafka-test)
2025-04-03T23:43:04.376+09:00  INFO 45581 --- [    Test worker] m.i.s.test2.KafkaTest3                   : The following 1 profile is active: "test"
2025-04-03T23:43:04.442+09:00  INFO 45581 --- [    Test worker] o.s.b.w.embedded.tomcat.TomcatWebServer  : Tomcat initialized with port 0 (http)
2025-04-03T23:43:04.443+09:00  INFO 45581 --- [    Test worker] o.apache.catalina.core.StandardService   : Starting service [Tomcat]
2025-04-03T23:43:04.443+09:00  INFO 45581 --- [    Test worker] o.apache.catalina.core.StandardEngine    : Starting Servlet engine: [Apache Tomcat/10.1.39]
2025-04-03T23:43:04.454+09:00  INFO 45581 --- [    Test worker] o.a.c.c.C.[Tomcat-1].[localhost].[/]     : Initializing Spring embedded WebApplicationContext
2025-04-03T23:43:04.454+09:00  INFO 45581 --- [    Test worker] w.s.c.ServletWebServerApplicationContext : Root WebApplicationContext: initialization completed in 77 ms
2025-04-03T23:43:04.496+09:00  INFO 45581 --- [    Test worker] o.s.b.w.embedded.tomcat.TomcatWebServer  : Tomcat started on port 58623 (http) with context path '/'
2025-04-03T23:43:04.497+09:00  INFO 45581 --- [    Test worker] o.a.k.clients.consumer.ConsumerConfig    : ConsumerConfig values:
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:58617]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-test-my-group-3
	client.rack =
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = test-my-group
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2025-04-03T23:43:04.497+09:00  INFO 45581 --- [    Test worker] o.a.k.c.t.i.KafkaMetricsCollector        : initializing Kafka metrics collector
2025-04-03T23:43:04.499+09:00  INFO 45581 --- [    Test worker] o.a.kafka.common.utils.AppInfoParser     : Kafka version: 3.8.1
2025-04-03T23:43:04.499+09:00  INFO 45581 --- [    Test worker] o.a.kafka.common.utils.AppInfoParser     : Kafka commitId: 70d6ff42debf7e17
2025-04-03T23:43:04.499+09:00  INFO 45581 --- [    Test worker] o.a.kafka.common.utils.AppInfoParser     : Kafka startTimeMs: 1743691384499
2025-04-03T23:43:04.499+09:00  INFO 45581 --- [    Test worker] o.a.k.c.c.internals.LegacyKafkaConsumer  : [Consumer clientId=consumer-test-my-group-3, groupId=test-my-group] Subscribed to topic(s): test-my-topic
2025-04-03T23:43:04.501+09:00  INFO 45581 --- [    Test worker] m.i.s.test2.KafkaTest3                   : Started KafkaTest3 in 0.349 seconds (process running for 3.1)
2025-04-03T23:43:04.502+09:00  INFO 45581 --- [ntainer#0-0-C-1] org.apache.kafka.clients.Metadata        : [Consumer clientId=consumer-test-my-group-3, groupId=test-my-group] Cluster ID: dQrwwGaZSdGniJ1RPZEy7w
2025-04-03T23:43:04.503+09:00  INFO 45581 --- [quest-handler-4] kafka.zk.AdminZkClient                   : Creating topic __consumer_offsets with configuration {compression.type=producer, cleanup.policy=compact, segment.bytes=104857600} and initial partition assignment HashMap(0 -> ArrayBuffer(0), 1 -> ArrayBuffer(0), 2 -> ArrayBuffer(0), 3 -> ArrayBuffer(0), 4 -> ArrayBuffer(0))
testBean2
2025-04-03T23:43:04.504+09:00  INFO 45581 --- [    Test worker] o.a.k.clients.producer.ProducerConfig    : ProducerConfig values:
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [127.0.0.1:58617]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-2
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-03T23:43:04.505+09:00  INFO 45581 --- [    Test worker] o.a.k.c.t.i.KafkaMetricsCollector        : initializing Kafka metrics collector
2025-04-03T23:43:04.505+09:00  INFO 45581 --- [    Test worker] o.a.k.clients.producer.KafkaProducer     : [Producer clientId=producer-2] Instantiated an idempotent producer.
2025-04-03T23:43:04.507+09:00  INFO 45581 --- [    Test worker] o.a.kafka.common.utils.AppInfoParser     : Kafka version: 3.8.1
2025-04-03T23:43:04.507+09:00  INFO 45581 --- [    Test worker] o.a.kafka.common.utils.AppInfoParser     : Kafka commitId: 70d6ff42debf7e17
2025-04-03T23:43:04.507+09:00  INFO 45581 --- [    Test worker] o.a.kafka.common.utils.AppInfoParser     : Kafka startTimeMs: 1743691384507
2025-04-03T23:43:04.509+09:00  INFO 45581 --- [ad | producer-2] org.apache.kafka.clients.Metadata        : [Producer clientId=producer-2] Cluster ID: dQrwwGaZSdGniJ1RPZEy7w
isSucceed3
2025-04-03T23:43:04.509+09:00  INFO 45581 --- [er-event-thread] kafka.controller.KafkaController         : [Controller id=0] New topics: [Set(__consumer_offsets)], deleted topics: [HashSet()], new partition replica assignment [Set(TopicIdReplicaAssignment(__consumer_offsets,Some(Mr0xSfs8RWKjjiOhTqgPYw),HashMap(__consumer_offsets-4 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-3 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-2 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-0 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-1 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=))))]
2025-04-03T23:43:04.509+09:00  INFO 45581 --- [er-event-thread] kafka.controller.KafkaController         : [Controller id=0] New partition creation callback for __consumer_offsets-4,__consumer_offsets-3,__consumer_offsets-2,__consumer_offsets-0,__consumer_offsets-1
2025-04-03T23:43:04.510+09:00  INFO 45581 --- [er-event-thread] state.change.logger                      : [Controller id=0 epoch=1] Changed partition __consumer_offsets-4 state from NonExistentPartition to NewPartition with assigned replicas 0
2025-04-03T23:43:04.510+09:00  INFO 45581 --- [er-event-thread] state.change.logger                      : [Controller id=0 epoch=1] Changed partition __consumer_offsets-3 state from NonExistentPartition to NewPartition with assigned replicas 0
2025-04-03T23:43:04.510+09:00  INFO 45581 --- [er-event-thread] state.change.logger                      : [Controller id=0 epoch=1] Changed partition __consumer_offsets-2 state from NonExistentPartition to NewPartition with assigned replicas 0
2025-04-03T23:43:04.510+09:00  INFO 45581 --- [er-event-thread] state.change.logger                      : [Controller id=0 epoch=1] Changed partition __consumer_offsets-0 state from NonExistentPartition to NewPartition with assigned replicas 0
2025-04-03T23:43:04.510+09:00  INFO 45581 --- [er-event-thread] state.change.logger                      : [Controller id=0 epoch=1] Changed partition __consumer_offsets-1 state from NonExistentPartition to NewPartition with assigned replicas 0
2025-04-03T23:43:04.510+09:00  INFO 45581 --- [er-event-thread] state.change.logger                      : [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions
2025-04-03T23:43:04.510+09:00  INFO 45581 --- [er-event-thread] state.change.logger                      : [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions
2025-04-03T23:43:04.519+09:00  INFO 45581 --- [er-event-thread] state.change.logger                      : [Controller id=0 epoch=1] Changed partition __consumer_offsets-3 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isrWithBrokerEpoch=List(BrokerState(brokerId=0, brokerEpoch=-1)), leaderRecoveryState=RECOVERED, partitionEpoch=0)
2025-04-03T23:43:04.519+09:00  INFO 45581 --- [er-event-thread] state.change.logger                      : [Controller id=0 epoch=1] Changed partition __consumer_offsets-2 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isrWithBrokerEpoch=List(BrokerState(brokerId=0, brokerEpoch=-1)), leaderRecoveryState=RECOVERED, partitionEpoch=0)
2025-04-03T23:43:04.519+09:00  INFO 45581 --- [er-event-thread] state.change.logger                      : [Controller id=0 epoch=1] Changed partition __consumer_offsets-0 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isrWithBrokerEpoch=List(BrokerState(brokerId=0, brokerEpoch=-1)), leaderRecoveryState=RECOVERED, partitionEpoch=0)
2025-04-03T23:43:04.519+09:00  INFO 45581 --- [er-event-thread] state.change.logger                      : [Controller id=0 epoch=1] Changed partition __consumer_offsets-1 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isrWithBrokerEpoch=List(BrokerState(brokerId=0, brokerEpoch=-1)), leaderRecoveryState=RECOVERED, partitionEpoch=0)
2025-04-03T23:43:04.519+09:00  INFO 45581 --- [er-event-thread] state.change.logger                      : [Controller id=0 epoch=1] Sending LeaderAndIsr request to broker 0 with 5 become-leader and 0 become-follower partitions
2025-04-03T23:43:04.519+09:00  INFO 45581 --- [er-event-thread] state.change.logger                      : [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet(0) for 5 partitions
2025-04-03T23:43:04.519+09:00  INFO 45581 --- [er-event-thread] state.change.logger                      : [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions
2025-04-03T23:43:04.521+09:00  INFO 45581 --- [quest-handler-3] state.change.logger                      : [Broker id=0] Handling LeaderAndIsr request correlationId 5 from controller 0 for 5 partitions
2025-04-03T23:43:04.521+09:00  INFO 45581 --- [quest-handler-3] kafka.server.ReplicaFetcherManager       : [ReplicaFetcherManager on broker 0] Removed fetcher for partitions HashSet(__consumer_offsets-4, __consumer_offsets-3, __consumer_offsets-2, __consumer_offsets-0, __consumer_offsets-1)
2025-04-03T23:43:04.521+09:00  INFO 45581 --- [quest-handler-3] state.change.logger                      : [Broker id=0] Stopped fetchers as part of LeaderAndIsr request correlationId 5 from controller 0 epoch 1 as part of the become-leader transition for 5 partitions
2025-04-03T23:43:04.521+09:00  INFO 45581 --- [ntainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Resetting generation and member id due to: consumer pro-actively leaving the group
2025-04-03T23:43:04.521+09:00  INFO 45581 --- [ntainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Request joining group due to: consumer pro-actively leaving the group
2025-04-03T23:43:04.521+09:00  INFO 45581 --- [ntainer#0-0-C-1] o.a.k.c.c.internals.LegacyKafkaConsumer  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Unsubscribed all topics or patterns and assigned partitions
2025-04-03T23:43:04.522+09:00  INFO 45581 --- [er-event-thread] kafka.controller.KafkaController         : [Controller id=0] Acquired new producerId block ProducerIdsBlock(assignedBrokerId=0, firstProducerId=0, size=1000) by writing to Zk with path version 1
2025-04-03T23:43:04.522+09:00  INFO 45581 --- [ntainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Resetting generation and member id due to: consumer pro-actively leaving the group
2025-04-03T23:43:04.522+09:00  INFO 45581 --- [ntainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Request joining group due to: consumer pro-actively leaving the group
2025-04-03T23:43:04.523+09:00  INFO 45581 --- [ntainer#0-0-C-1] o.apache.kafka.common.metrics.Metrics    : Metrics scheduler closed
2025-04-03T23:43:04.523+09:00  INFO 45581 --- [ntainer#0-0-C-1] o.apache.kafka.common.metrics.Metrics    : Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-04-03T23:43:04.523+09:00  INFO 45581 --- [ntainer#0-0-C-1] o.apache.kafka.common.metrics.Metrics    : Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter
2025-04-03T23:43:04.523+09:00  INFO 45581 --- [ntainer#0-0-C-1] o.apache.kafka.common.metrics.Metrics    : Metrics reporters closed
2025-04-03T23:43:04.526+09:00  INFO 45581 --- [ntainer#0-0-C-1] o.a.kafka.common.utils.AppInfoParser     : App info kafka.consumer for consumer-my-group-1 unregistered
2025-04-03T23:43:04.526+09:00  INFO 45581 --- [ntainer#0-0-C-1] o.s.k.l.KafkaMessageListenerContainer    : my-group: Consumer stopped
2025-04-03T23:43:04.528+09:00  INFO 45581 --- [ntainer#0-0-C-1] k.c.c.i.ConsumerRebalanceListenerInvoker : [Consumer clientId=consumer-my-group-2, groupId=my-group] Revoke previously assigned partitions my-topic-0
2025-04-03T23:43:04.528+09:00  INFO 45581 --- [ntainer#0-0-C-1] o.s.k.l.KafkaMessageListenerContainer    : my-group: partitions revoked: [my-topic-0]
2025-04-03T23:43:04.528+09:00  INFO 45581 --- [ntainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-2, groupId=my-group] Member consumer-my-group-2-9e457335-9ab7-4874-a802-30d1f6e2a57b sending LeaveGroup request to coordinator localhost:58606 (id: 2147483647 rack: null) due to the consumer unsubscribed from all topics
2025-04-03T23:43:04.528+09:00  INFO 45581 --- [ntainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-2, groupId=my-group] Resetting generation and member id due to: consumer pro-actively leaving the group
2025-04-03T23:43:04.528+09:00  INFO 45581 --- [ntainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-2, groupId=my-group] Request joining group due to: consumer pro-actively leaving the group
2025-04-03T23:43:04.528+09:00  INFO 45581 --- [ntainer#0-0-C-1] o.a.k.c.c.internals.LegacyKafkaConsumer  : [Consumer clientId=consumer-my-group-2, groupId=my-group] Unsubscribed all topics or patterns and assigned partitions
2025-04-03T23:43:04.528+09:00  INFO 45581 --- [ntainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-2, groupId=my-group] Resetting generation and member id due to: consumer pro-actively leaving the group
2025-04-03T23:43:04.528+09:00  INFO 45581 --- [ntainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-2, groupId=my-group] Request joining group due to: consumer pro-actively leaving the group
2025-04-03T23:43:04.530+09:00  INFO 45581 --- [quest-handler-4] k.coordinator.group.GroupCoordinator     : [GroupCoordinator 0]: Preparing to rebalance group my-group in state PreparingRebalance with old generation 1 (__consumer_offsets-2) (reason: Removing member consumer-my-group-2-9e457335-9ab7-4874-a802-30d1f6e2a57b on LeaveGroup; client reason: the consumer unsubscribed from all topics)
2025-04-03T23:43:04.530+09:00  INFO 45581 --- [quest-handler-4] k.coordinator.group.GroupCoordinator     : [GroupCoordinator 0]: Group my-group with generation 2 is now empty (__consumer_offsets-2)
2025-04-03T23:43:04.530+09:00  INFO 45581 --- [quest-handler-3] kafka.log.UnifiedLog$                    : [LogLoader partition=__consumer_offsets-3, dir=/var/folders/qb/nxyw0kj549bbf87mx3ss8vgc0000gn/T/spring.kafka.d6ee9d52-45d4-418e-bc16-f6488da9fcbf7176368033373570203] Loading producer state till offset 0 with message format version 2
2025-04-03T23:43:04.530+09:00  INFO 45581 --- [quest-handler-3] kafka.log.LogManager                     : Created log for partition __consumer_offsets-3 in /var/folders/qb/nxyw0kj549bbf87mx3ss8vgc0000gn/T/spring.kafka.d6ee9d52-45d4-418e-bc16-f6488da9fcbf7176368033373570203/__consumer_offsets-3 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600}
2025-04-03T23:43:04.531+09:00  INFO 45581 --- [quest-handler-3] kafka.cluster.Partition                  : [Partition __consumer_offsets-3 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-3
2025-04-03T23:43:04.531+09:00  INFO 45581 --- [quest-handler-3] kafka.cluster.Partition                  : [Partition __consumer_offsets-3 broker=0] Log loaded for partition __consumer_offsets-3 with initial high watermark 0
2025-04-03T23:43:04.531+09:00  INFO 45581 --- [quest-handler-3] state.change.logger                      : [Broker id=0] Leader __consumer_offsets-3 with topic id Some(Mr0xSfs8RWKjjiOhTqgPYw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [0], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1.
2025-04-03T23:43:04.531+09:00  INFO 45581 --- [quest-handler-4] k.coordinator.group.GroupCoordinator     : [GroupCoordinator 0]: Member MemberMetadata(memberId=consumer-my-group-2-9e457335-9ab7-4874-a802-30d1f6e2a57b, groupInstanceId=None, clientId=consumer-my-group-2, clientHost=/127.0.0.1, sessionTimeoutMs=45000, rebalanceTimeoutMs=300000, supportedProtocols=List(range, cooperative-sticky)) has left group my-group through explicit `LeaveGroup`; client reason: the consumer unsubscribed from all topics
2025-04-03T23:43:04.540+09:00  INFO 45581 --- [quest-handler-3] kafka.log.UnifiedLog$                    : [LogLoader partition=__consumer_offsets-2, dir=/var/folders/qb/nxyw0kj549bbf87mx3ss8vgc0000gn/T/spring.kafka.d6ee9d52-45d4-418e-bc16-f6488da9fcbf7176368033373570203] Loading producer state till offset 0 with message format version 2
2025-04-03T23:43:04.541+09:00  INFO 45581 --- [quest-handler-3] kafka.log.LogManager                     : Created log for partition __consumer_offsets-2 in /var/folders/qb/nxyw0kj549bbf87mx3ss8vgc0000gn/T/spring.kafka.d6ee9d52-45d4-418e-bc16-f6488da9fcbf7176368033373570203/__consumer_offsets-2 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600}
2025-04-03T23:43:04.541+09:00  INFO 45581 --- [quest-handler-3] kafka.cluster.Partition                  : [Partition __consumer_offsets-2 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-2
2025-04-03T23:43:04.541+09:00  INFO 45581 --- [quest-handler-3] kafka.cluster.Partition                  : [Partition __consumer_offsets-2 broker=0] Log loaded for partition __consumer_offsets-2 with initial high watermark 0
2025-04-03T23:43:04.541+09:00  INFO 45581 --- [quest-handler-3] state.change.logger                      : [Broker id=0] Leader __consumer_offsets-2 with topic id Some(Mr0xSfs8RWKjjiOhTqgPYw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [0], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1.
2025-04-03T23:43:04.549+09:00  INFO 45581 --- [quest-handler-3] kafka.log.UnifiedLog$                    : [LogLoader partition=__consumer_offsets-4, dir=/var/folders/qb/nxyw0kj549bbf87mx3ss8vgc0000gn/T/spring.kafka.d6ee9d52-45d4-418e-bc16-f6488da9fcbf7176368033373570203] Loading producer state till offset 0 with message format version 2
2025-04-03T23:43:04.549+09:00  INFO 45581 --- [quest-handler-3] kafka.log.LogManager                     : Created log for partition __consumer_offsets-4 in /var/folders/qb/nxyw0kj549bbf87mx3ss8vgc0000gn/T/spring.kafka.d6ee9d52-45d4-418e-bc16-f6488da9fcbf7176368033373570203/__consumer_offsets-4 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600}
2025-04-03T23:43:04.549+09:00  INFO 45581 --- [quest-handler-3] kafka.cluster.Partition                  : [Partition __consumer_offsets-4 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-4
2025-04-03T23:43:04.549+09:00  INFO 45581 --- [quest-handler-3] kafka.cluster.Partition                  : [Partition __consumer_offsets-4 broker=0] Log loaded for partition __consumer_offsets-4 with initial high watermark 0
2025-04-03T23:43:04.549+09:00  INFO 45581 --- [quest-handler-3] state.change.logger                      : [Broker id=0] Leader __consumer_offsets-4 with topic id Some(Mr0xSfs8RWKjjiOhTqgPYw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [0], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1.
2025-04-03T23:43:04.559+09:00  INFO 45581 --- [quest-handler-3] kafka.log.UnifiedLog$                    : [LogLoader partition=__consumer_offsets-1, dir=/var/folders/qb/nxyw0kj549bbf87mx3ss8vgc0000gn/T/spring.kafka.d6ee9d52-45d4-418e-bc16-f6488da9fcbf7176368033373570203] Loading producer state till offset 0 with message format version 2
2025-04-03T23:43:04.559+09:00  INFO 45581 --- [quest-handler-3] kafka.log.LogManager                     : Created log for partition __consumer_offsets-1 in /var/folders/qb/nxyw0kj549bbf87mx3ss8vgc0000gn/T/spring.kafka.d6ee9d52-45d4-418e-bc16-f6488da9fcbf7176368033373570203/__consumer_offsets-1 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600}
2025-04-03T23:43:04.559+09:00  INFO 45581 --- [quest-handler-3] kafka.cluster.Partition                  : [Partition __consumer_offsets-1 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-1
2025-04-03T23:43:04.559+09:00  INFO 45581 --- [quest-handler-3] kafka.cluster.Partition                  : [Partition __consumer_offsets-1 broker=0] Log loaded for partition __consumer_offsets-1 with initial high watermark 0
2025-04-03T23:43:04.559+09:00  INFO 45581 --- [quest-handler-3] state.change.logger                      : [Broker id=0] Leader __consumer_offsets-1 with topic id Some(Mr0xSfs8RWKjjiOhTqgPYw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [0], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1.
2025-04-03T23:43:04.569+09:00  INFO 45581 --- [quest-handler-3] kafka.log.UnifiedLog$                    : [LogLoader partition=__consumer_offsets-0, dir=/var/folders/qb/nxyw0kj549bbf87mx3ss8vgc0000gn/T/spring.kafka.d6ee9d52-45d4-418e-bc16-f6488da9fcbf7176368033373570203] Loading producer state till offset 0 with message format version 2
2025-04-03T23:43:04.570+09:00  INFO 45581 --- [quest-handler-3] kafka.log.LogManager                     : Created log for partition __consumer_offsets-0 in /var/folders/qb/nxyw0kj549bbf87mx3ss8vgc0000gn/T/spring.kafka.d6ee9d52-45d4-418e-bc16-f6488da9fcbf7176368033373570203/__consumer_offsets-0 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600}
2025-04-03T23:43:04.570+09:00  INFO 45581 --- [quest-handler-3] kafka.cluster.Partition                  : [Partition __consumer_offsets-0 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-0
2025-04-03T23:43:04.570+09:00  INFO 45581 --- [quest-handler-3] kafka.cluster.Partition                  : [Partition __consumer_offsets-0 broker=0] Log loaded for partition __consumer_offsets-0 with initial high watermark 0
2025-04-03T23:43:04.570+09:00  INFO 45581 --- [quest-handler-3] state.change.logger                      : [Broker id=0] Leader __consumer_offsets-0 with topic id Some(Mr0xSfs8RWKjjiOhTqgPYw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [0], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1.
2025-04-03T23:43:04.575+09:00  INFO 45581 --- [quest-handler-3] k.coordinator.group.GroupCoordinator     : [GroupCoordinator 0]: Elected as the group coordinator for partition 3 in epoch 0
2025-04-03T23:43:04.575+09:00  INFO 45581 --- [quest-handler-3] k.c.group.GroupMetadataManager           : [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-3 for epoch 0
2025-04-03T23:43:04.575+09:00  INFO 45581 --- [quest-handler-3] k.coordinator.group.GroupCoordinator     : [GroupCoordinator 0]: Elected as the group coordinator for partition 2 in epoch 0
2025-04-03T23:43:04.575+09:00  INFO 45581 --- [quest-handler-3] k.c.group.GroupMetadataManager           : [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-2 for epoch 0
2025-04-03T23:43:04.575+09:00  INFO 45581 --- [quest-handler-3] k.coordinator.group.GroupCoordinator     : [GroupCoordinator 0]: Elected as the group coordinator for partition 4 in epoch 0
2025-04-03T23:43:04.575+09:00  INFO 45581 --- [quest-handler-3] k.c.group.GroupMetadataManager           : [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-4 for epoch 0
2025-04-03T23:43:04.575+09:00  INFO 45581 --- [quest-handler-3] k.coordinator.group.GroupCoordinator     : [GroupCoordinator 0]: Elected as the group coordinator for partition 1 in epoch 0
2025-04-03T23:43:04.575+09:00  INFO 45581 --- [quest-handler-3] k.c.group.GroupMetadataManager           : [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-1 for epoch 0
2025-04-03T23:43:04.575+09:00  INFO 45581 --- [quest-handler-3] k.coordinator.group.GroupCoordinator     : [GroupCoordinator 0]: Elected as the group coordinator for partition 0 in epoch 0
2025-04-03T23:43:04.575+09:00  INFO 45581 --- [quest-handler-3] k.c.group.GroupMetadataManager           : [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-0 for epoch 0
2025-04-03T23:43:04.575+09:00  INFO 45581 --- [adata-manager-0] k.c.group.GroupMetadataManager           : [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-3 in 0 milliseconds for epoch 0, of which 0 milliseconds was spent in the scheduler.
2025-04-03T23:43:04.575+09:00  INFO 45581 --- [quest-handler-3] state.change.logger                      : [Broker id=0] Finished LeaderAndIsr request in 55ms correlationId 5 from controller 0 for 5 partitions
2025-04-03T23:43:04.575+09:00  INFO 45581 --- [adata-manager-0] k.c.group.GroupMetadataManager           : [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-2 in 0 milliseconds for epoch 0, of which 0 milliseconds was spent in the scheduler.
2025-04-03T23:43:04.575+09:00  INFO 45581 --- [adata-manager-0] k.c.group.GroupMetadataManager           : [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-4 in 0 milliseconds for epoch 0, of which 0 milliseconds was spent in the scheduler.
2025-04-03T23:43:04.575+09:00  INFO 45581 --- [adata-manager-0] k.c.group.GroupMetadataManager           : [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-1 in 0 milliseconds for epoch 0, of which 0 milliseconds was spent in the scheduler.
2025-04-03T23:43:04.575+09:00  INFO 45581 --- [adata-manager-0] k.c.group.GroupMetadataManager           : [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-0 in 0 milliseconds for epoch 0, of which 0 milliseconds was spent in the scheduler.
2025-04-03T23:43:04.576+09:00  INFO 45581 --- [quest-handler-5] state.change.logger                      : [Broker id=0] Add 5 partitions and deleted 0 partitions from metadata cache in response to UpdateMetadata request sent by controller 0 epoch 1 with correlation id 6
2025-04-03T23:43:04.611+09:00  INFO 45581 --- [ntainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-test-my-group-3, groupId=test-my-group] Discovered group coordinator localhost:58617 (id: 2147483647 rack: null)
2025-04-03T23:43:04.611+09:00  INFO 45581 --- [ntainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-test-my-group-3, groupId=test-my-group] (Re-)joining group
2025-04-03T23:43:04.612+09:00  INFO 45581 --- [quest-handler-1] k.coordinator.group.GroupCoordinator     : [GroupCoordinator 0]: Dynamic member with unknown member id joins group test-my-group in Empty state. Created a new member id consumer-test-my-group-3-e742400f-7734-4921-84e4-ef1925098c88 and request the member to rejoin with this id.
2025-04-03T23:43:04.613+09:00  INFO 45581 --- [ntainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-test-my-group-3, groupId=test-my-group] Request joining group due to: need to re-join with the given member-id: consumer-test-my-group-3-e742400f-7734-4921-84e4-ef1925098c88
2025-04-03T23:43:04.613+09:00  INFO 45581 --- [ntainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-test-my-group-3, groupId=test-my-group] (Re-)joining group
2025-04-03T23:43:04.613+09:00  INFO 45581 --- [ad | producer-2] o.a.k.c.p.internals.TransactionManager   : [Producer clientId=producer-2] ProducerId set to 0 with epoch 0
2025-04-03T23:43:04.613+09:00  INFO 45581 --- [quest-handler-3] k.coordinator.group.GroupCoordinator     : [GroupCoordinator 0]: Preparing to rebalance group test-my-group in state PreparingRebalance with old generation 0 (__consumer_offsets-3) (reason: Adding new member consumer-test-my-group-3-e742400f-7734-4921-84e4-ef1925098c88 with group instance id None; client reason: need to re-join with the given member-id: consumer-test-my-group-3-e742400f-7734-4921-84e4-ef1925098c88)
2025-04-03T23:43:04.613+09:00  INFO 45581 --- [cutor-Rebalance] k.coordinator.group.GroupCoordinator     : [GroupCoordinator 0]: Stabilized group test-my-group generation 1 (__consumer_offsets-3) with 1 members
2025-04-03T23:43:04.614+09:00  INFO 45581 --- [ntainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-test-my-group-3, groupId=test-my-group] Successfully joined group with generation Generation{generationId=1, memberId='consumer-test-my-group-3-e742400f-7734-4921-84e4-ef1925098c88', protocol='range'}
2025-04-03T23:43:04.614+09:00  INFO 45581 --- [ntainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-test-my-group-3, groupId=test-my-group] Finished assignment for group at generation 1: {consumer-test-my-group-3-e742400f-7734-4921-84e4-ef1925098c88=Assignment(partitions=[test-my-topic-0])}
2025-04-03T23:43:04.614+09:00  INFO 45581 --- [quest-handler-4] k.coordinator.group.GroupCoordinator     : [GroupCoordinator 0]: Assignment received from leader consumer-test-my-group-3-e742400f-7734-4921-84e4-ef1925098c88 for group test-my-group for generation 1. The group has 1 members, 0 of which are static.
2025-04-03T23:43:04.615+09:00  INFO 45581 --- [ntainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-test-my-group-3, groupId=test-my-group] Successfully synced group in generation Generation{generationId=1, memberId='consumer-test-my-group-3-e742400f-7734-4921-84e4-ef1925098c88', protocol='range'}
2025-04-03T23:43:04.615+09:00  INFO 45581 --- [ntainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-test-my-group-3, groupId=test-my-group] Notifying assignor about the new Assignment(partitions=[test-my-topic-0])
2025-04-03T23:43:04.615+09:00  INFO 45581 --- [ntainer#0-0-C-1] k.c.c.i.ConsumerRebalanceListenerInvoker : [Consumer clientId=consumer-test-my-group-3, groupId=test-my-group] Adding newly assigned partitions: test-my-topic-0
2025-04-03T23:43:04.615+09:00  INFO 45581 --- [ntainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-test-my-group-3, groupId=test-my-group] Found no committed offset for partition test-my-topic-0
2025-04-03T23:43:04.616+09:00  INFO 45581 --- [ntainer#0-0-C-1] o.a.k.c.c.internals.SubscriptionState    : [Consumer clientId=consumer-test-my-group-3, groupId=test-my-group] Resetting offset for partition test-my-topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:58617 (id: 0 rack: null)], epoch=0}}.
2025-04-03T23:43:04.616+09:00  INFO 45581 --- [ntainer#0-0-C-1] o.s.k.l.KafkaMessageListenerContainer    : test-my-group: partitions assigned: [test-my-topic-0]
>> message3
2025-04-03T23:43:04.801+09:00  INFO 45581 --- [ntainer#0-0-C-1] o.apache.kafka.common.metrics.Metrics    : Metrics scheduler closed
2025-04-03T23:43:04.801+09:00  INFO 45581 --- [ntainer#0-0-C-1] o.apache.kafka.common.metrics.Metrics    : Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-04-03T23:43:04.801+09:00  INFO 45581 --- [ntainer#0-0-C-1] o.apache.kafka.common.metrics.Metrics    : Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter
2025-04-03T23:43:04.802+09:00  INFO 45581 --- [ntainer#0-0-C-1] o.apache.kafka.common.metrics.Metrics    : Metrics reporters closed
2025-04-03T23:43:04.803+09:00  INFO 45581 --- [ntainer#0-0-C-1] o.a.kafka.common.utils.AppInfoParser     : App info kafka.consumer for consumer-my-group-2 unregistered
2025-04-03T23:43:04.803+09:00  INFO 45581 --- [ntainer#0-0-C-1] o.s.k.l.KafkaMessageListenerContainer    : my-group: Consumer stopped
2025-04-03T23:43:04.804+09:00  INFO 45581 --- [ionShutdownHook] o.s.b.w.e.tomcat.GracefulShutdown        : Commencing graceful shutdown. Waiting for active requests to complete
2025-04-03T23:43:04.809+09:00  INFO 45581 --- [tomcat-shutdown] o.s.b.w.e.tomcat.GracefulShutdown        : Graceful shutdown complete
2025-04-03T23:43:04.810+09:00  INFO 45581 --- [ionShutdownHook] o.a.k.clients.producer.KafkaProducer     : [Producer clientId=producer-1] Closing the Kafka producer with timeoutMillis = 30000 ms.
2025-04-03T23:43:04.812+09:00  INFO 45581 --- [ionShutdownHook] o.apache.kafka.common.metrics.Metrics    : Metrics scheduler closed
2025-04-03T23:43:04.812+09:00  INFO 45581 --- [ionShutdownHook] o.apache.kafka.common.metrics.Metrics    : Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-04-03T23:43:04.812+09:00  INFO 45581 --- [ionShutdownHook] o.apache.kafka.common.metrics.Metrics    : Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter
2025-04-03T23:43:04.812+09:00  INFO 45581 --- [ionShutdownHook] o.apache.kafka.common.metrics.Metrics    : Metrics reporters closed
2025-04-03T23:43:04.812+09:00  INFO 45581 --- [ionShutdownHook] o.a.kafka.common.utils.AppInfoParser     : App info kafka.producer for producer-1 unregistered
2025-04-03T23:43:04.812+09:00  INFO 45581 --- [ionShutdownHook] kafka.server.KafkaServer                 : [KafkaServer id=0] shutting down
2025-04-03T23:43:04.813+09:00  INFO 45581 --- [ionShutdownHook] icationListener$ChangeEventProcessThread : [/config/changes-event-process-thread]: Shutting down
2025-04-03T23:43:04.813+09:00  INFO 45581 --- [-process-thread] icationListener$ChangeEventProcessThread : [/config/changes-event-process-thread]: Stopped
2025-04-03T23:43:04.813+09:00  INFO 45581 --- [ionShutdownHook] icationListener$ChangeEventProcessThread : [/config/changes-event-process-thread]: Shutdown completed
2025-04-03T23:43:04.813+09:00  INFO 45581 --- [ionShutdownHook] kafka.network.SocketServer               : [SocketServer listenerType=ZK_BROKER, nodeId=0] Stopping socket server request processors
2025-04-03T23:43:04.814+09:00  INFO 45581 --- [channel-manager] org.apache.kafka.clients.NetworkClient   : [NodeToControllerChannelManager id=0 name=forwarding] Node 0 disconnected.
2025-04-03T23:43:04.815+09:00  INFO 45581 --- [ionShutdownHook] kafka.network.SocketServer               : [SocketServer listenerType=ZK_BROKER, nodeId=0] Stopped socket server request processors
2025-04-03T23:43:04.815+09:00  INFO 45581 --- [ionShutdownHook] kafka.server.KafkaRequestHandlerPool     : [data-plane Kafka Request Handler on Broker 0], shutting down
2025-04-03T23:43:04.816+09:00  INFO 45581 --- [ionShutdownHook] kafka.server.KafkaRequestHandlerPool     : [data-plane Kafka Request Handler on Broker 0], shut down completely
2025-04-03T23:43:04.816+09:00  INFO 45581 --- [ionShutdownHook] perationPurgatory$ExpiredOperationReaper : [ExpirationReaper-0-AlterAcls]: Shutting down
2025-04-03T23:43:04.816+09:00  INFO 45581 --- [per-0-AlterAcls] perationPurgatory$ExpiredOperationReaper : [ExpirationReaper-0-AlterAcls]: Stopped
2025-04-03T23:43:04.816+09:00  INFO 45581 --- [ionShutdownHook] perationPurgatory$ExpiredOperationReaper : [ExpirationReaper-0-AlterAcls]: Shutdown completed
2025-04-03T23:43:04.817+09:00  INFO 45581 --- [ionShutdownHook] kafka.server.KafkaApis                   : [KafkaApi-0] Shutdown complete.
2025-04-03T23:43:04.817+09:00  INFO 45581 --- [ionShutdownHook] perationPurgatory$ExpiredOperationReaper : [ExpirationReaper-0-topic]: Shutting down
2025-04-03T23:43:04.817+09:00  INFO 45581 --- [nReaper-0-topic] perationPurgatory$ExpiredOperationReaper : [ExpirationReaper-0-topic]: Stopped
2025-04-03T23:43:04.817+09:00  INFO 45581 --- [ionShutdownHook] perationPurgatory$ExpiredOperationReaper : [ExpirationReaper-0-topic]: Shutdown completed
2025-04-03T23:43:04.818+09:00  INFO 45581 --- [ionShutdownHook] k.c.transaction.TransactionCoordinator   : [TransactionCoordinator id=0] Shutting down.
2025-04-03T23:43:04.818+09:00  INFO 45581 --- [ionShutdownHook] k.c.transaction.TransactionStateManager  : [Transaction State Manager 0]: Shutdown complete
2025-04-03T23:43:04.818+09:00  INFO 45581 --- [ionShutdownHook] k.c.t.TransactionMarkerChannelManager    : [TxnMarkerSenderThread-0]: Shutting down
2025-04-03T23:43:04.818+09:00  INFO 45581 --- [rSenderThread-0] k.c.t.TransactionMarkerChannelManager    : [TxnMarkerSenderThread-0]: Stopped
2025-04-03T23:43:04.818+09:00  INFO 45581 --- [ionShutdownHook] k.c.t.TransactionMarkerChannelManager    : [TxnMarkerSenderThread-0]: Shutdown completed
2025-04-03T23:43:04.819+09:00  INFO 45581 --- [ionShutdownHook] k.c.transaction.TransactionCoordinator   : [TransactionCoordinator id=0] Shutdown complete.
2025-04-03T23:43:04.820+09:00  INFO 45581 --- [ionShutdownHook] k.coordinator.group.GroupCoordinator     : [GroupCoordinator 0]: Shutting down.
2025-04-03T23:43:04.820+09:00  INFO 45581 --- [ionShutdownHook] perationPurgatory$ExpiredOperationReaper : [ExpirationReaper-0-Heartbeat]: Shutting down
2025-04-03T23:43:04.820+09:00  INFO 45581 --- [per-0-Heartbeat] perationPurgatory$ExpiredOperationReaper : [ExpirationReaper-0-Heartbeat]: Stopped
2025-04-03T23:43:04.820+09:00  INFO 45581 --- [ionShutdownHook] perationPurgatory$ExpiredOperationReaper : [ExpirationReaper-0-Heartbeat]: Shutdown completed
2025-04-03T23:43:04.820+09:00  INFO 45581 --- [ionShutdownHook] perationPurgatory$ExpiredOperationReaper : [ExpirationReaper-0-Rebalance]: Shutting down
2025-04-03T23:43:04.820+09:00  INFO 45581 --- [ionShutdownHook] perationPurgatory$ExpiredOperationReaper : [ExpirationReaper-0-Rebalance]: Shutdown completed
2025-04-03T23:43:04.820+09:00  INFO 45581 --- [per-0-Rebalance] perationPurgatory$ExpiredOperationReaper : [ExpirationReaper-0-Rebalance]: Stopped
2025-04-03T23:43:04.820+09:00  INFO 45581 --- [ionShutdownHook] k.coordinator.group.GroupCoordinator     : [GroupCoordinator 0]: Shutdown complete.
2025-04-03T23:43:04.821+09:00  INFO 45581 --- [ionShutdownHook] kafka.server.ReplicaManager              : [ReplicaManager broker=0] Shutting down
2025-04-03T23:43:04.821+09:00  INFO 45581 --- [ionShutdownHook] k.s.ReplicaManager$LogDirFailureHandler  : [LogDirFailureHandler]: Shutting down
2025-04-03T23:43:04.821+09:00  INFO 45581 --- [rFailureHandler] k.s.ReplicaManager$LogDirFailureHandler  : [LogDirFailureHandler]: Stopped
2025-04-03T23:43:04.821+09:00  INFO 45581 --- [ionShutdownHook] k.s.ReplicaManager$LogDirFailureHandler  : [LogDirFailureHandler]: Shutdown completed
2025-04-03T23:43:04.821+09:00  INFO 45581 --- [ionShutdownHook] kafka.server.ReplicaFetcherManager       : [ReplicaFetcherManager on broker 0] shutting down
2025-04-03T23:43:04.821+09:00  INFO 45581 --- [ionShutdownHook] kafka.server.ReplicaFetcherManager       : [ReplicaFetcherManager on broker 0] shutdown completed
2025-04-03T23:43:04.821+09:00  INFO 45581 --- [ionShutdownHook] k.server.ReplicaAlterLogDirsManager      : [ReplicaAlterLogDirsManager on broker 0] shutting down
2025-04-03T23:43:04.821+09:00  INFO 45581 --- [ionShutdownHook] k.server.ReplicaAlterLogDirsManager      : [ReplicaAlterLogDirsManager on broker 0] shutdown completed
2025-04-03T23:43:04.821+09:00  INFO 45581 --- [ionShutdownHook] perationPurgatory$ExpiredOperationReaper : [ExpirationReaper-0-Fetch]: Shutting down
2025-04-03T23:43:04.821+09:00  INFO 45581 --- [nReaper-0-Fetch] perationPurgatory$ExpiredOperationReaper : [ExpirationReaper-0-Fetch]: Stopped
2025-04-03T23:43:04.821+09:00  INFO 45581 --- [ionShutdownHook] perationPurgatory$ExpiredOperationReaper : [ExpirationReaper-0-Fetch]: Shutdown completed
2025-04-03T23:43:04.822+09:00  INFO 45581 --- [ionShutdownHook] perationPurgatory$ExpiredOperationReaper : [ExpirationReaper-0-RemoteFetch]: Shutting down
2025-04-03T23:43:04.822+09:00  INFO 45581 --- [r-0-RemoteFetch] perationPurgatory$ExpiredOperationReaper : [ExpirationReaper-0-RemoteFetch]: Stopped
2025-04-03T23:43:04.822+09:00  INFO 45581 --- [ionShutdownHook] perationPurgatory$ExpiredOperationReaper : [ExpirationReaper-0-RemoteFetch]: Shutdown completed
2025-04-03T23:43:04.822+09:00  INFO 45581 --- [ionShutdownHook] perationPurgatory$ExpiredOperationReaper : [ExpirationReaper-0-Produce]: Shutting down
2025-04-03T23:43:04.822+09:00  INFO 45581 --- [eaper-0-Produce] perationPurgatory$ExpiredOperationReaper : [ExpirationReaper-0-Produce]: Stopped
2025-04-03T23:43:04.822+09:00  INFO 45581 --- [ionShutdownHook] perationPurgatory$ExpiredOperationReaper : [ExpirationReaper-0-Produce]: Shutdown completed
2025-04-03T23:43:04.822+09:00  INFO 45581 --- [ionShutdownHook] perationPurgatory$ExpiredOperationReaper : [ExpirationReaper-0-DeleteRecords]: Shutting down
2025-04-03T23:43:04.822+09:00  INFO 45581 --- [0-DeleteRecords] perationPurgatory$ExpiredOperationReaper : [ExpirationReaper-0-DeleteRecords]: Stopped
2025-04-03T23:43:04.822+09:00  INFO 45581 --- [ionShutdownHook] perationPurgatory$ExpiredOperationReaper : [ExpirationReaper-0-DeleteRecords]: Shutdown completed
2025-04-03T23:43:04.822+09:00  INFO 45581 --- [ionShutdownHook] perationPurgatory$ExpiredOperationReaper : [ExpirationReaper-0-ElectLeader]: Shutting down
2025-04-03T23:43:04.822+09:00  INFO 45581 --- [r-0-ElectLeader] perationPurgatory$ExpiredOperationReaper : [ExpirationReaper-0-ElectLeader]: Stopped
2025-04-03T23:43:04.822+09:00  INFO 45581 --- [ionShutdownHook] perationPurgatory$ExpiredOperationReaper : [ExpirationReaper-0-ElectLeader]: Shutdown completed
2025-04-03T23:43:04.828+09:00  INFO 45581 --- [ionShutdownHook] kafka.server.AddPartitionsToTxnManager   : [AddPartitionsToTxnSenderThread-0]: Shutting down
2025-04-03T23:43:04.829+09:00  INFO 45581 --- [ionShutdownHook] kafka.server.AddPartitionsToTxnManager   : [AddPartitionsToTxnSenderThread-0]: Shutdown completed
2025-04-03T23:43:04.829+09:00  INFO 45581 --- [nSenderThread-0] kafka.server.AddPartitionsToTxnManager   : [AddPartitionsToTxnSenderThread-0]: Stopped
2025-04-03T23:43:04.829+09:00  INFO 45581 --- [ionShutdownHook] kafka.server.ReplicaManager              : [ReplicaManager broker=0] Shut down completely
2025-04-03T23:43:04.829+09:00  INFO 45581 --- [ionShutdownHook] k.server.NodeToControllerRequestThread   : [zk-broker-0-to-controller-alter-partition-channel-manager]: Shutting down
2025-04-03T23:43:04.829+09:00  INFO 45581 --- [channel-manager] k.server.NodeToControllerRequestThread   : [zk-broker-0-to-controller-alter-partition-channel-manager]: Stopped
2025-04-03T23:43:04.829+09:00  INFO 45581 --- [ionShutdownHook] k.server.NodeToControllerRequestThread   : [zk-broker-0-to-controller-alter-partition-channel-manager]: Shutdown completed
2025-04-03T23:43:04.829+09:00  INFO 45581 --- [ionShutdownHook] k.s.NodeToControllerChannelManagerImpl   : Node to controller channel manager for alter-partition shutdown
2025-04-03T23:43:04.829+09:00  INFO 45581 --- [ionShutdownHook] k.server.NodeToControllerRequestThread   : [zk-broker-0-to-controller-forwarding-channel-manager]: Shutting down
2025-04-03T23:43:04.829+09:00  INFO 45581 --- [channel-manager] k.server.NodeToControllerRequestThread   : [zk-broker-0-to-controller-forwarding-channel-manager]: Stopped
2025-04-03T23:43:04.829+09:00  INFO 45581 --- [ionShutdownHook] k.server.NodeToControllerRequestThread   : [zk-broker-0-to-controller-forwarding-channel-manager]: Shutdown completed
2025-04-03T23:43:04.829+09:00  INFO 45581 --- [ionShutdownHook] k.s.NodeToControllerChannelManagerImpl   : Node to controller channel manager for forwarding shutdown
2025-04-03T23:43:04.830+09:00  INFO 45581 --- [ionShutdownHook] kafka.log.LogManager                     : Shutting down.
2025-04-03T23:43:04.830+09:00  INFO 45581 --- [ionShutdownHook] kafka.log.LogCleaner                     : Shutting down the log cleaner.
2025-04-03T23:43:04.830+09:00  INFO 45581 --- [ionShutdownHook] kafka.log.LogCleaner$CleanerThread       : [kafka-log-cleaner-thread-0]: Shutting down
2025-04-03T23:43:04.830+09:00  INFO 45581 --- [leaner-thread-0] kafka.log.LogCleaner$CleanerThread       : [kafka-log-cleaner-thread-0]: Stopped
2025-04-03T23:43:04.830+09:00  INFO 45581 --- [ionShutdownHook] kafka.log.LogCleaner$CleanerThread       : [kafka-log-cleaner-thread-0]: Shutdown completed
2025-04-03T23:43:04.856+09:00  INFO 45581 --- [667832673744764] o.a.k.s.i.log.ProducerStateManager       : [ProducerStateManager partition=my-topic-0] Wrote producer snapshot at offset 1 with 1 producer ids in 4 ms.
2025-04-03T23:43:04.870+09:00  INFO 45581 --- [667832673744764] o.a.k.s.i.log.ProducerStateManager       : [ProducerStateManager partition=__consumer_offsets-2] Wrote producer snapshot at offset 3 with 0 producer ids in 2 ms.
2025-04-03T23:43:04.889+09:00  INFO 45581 --- [ionShutdownHook] kafka.log.LogManager                     : Shutdown complete.
2025-04-03T23:43:04.889+09:00  INFO 45581 --- [ionShutdownHook] rollerEventManager$ControllerEventThread : [ControllerEventThread controllerId=0] Shutting down
2025-04-03T23:43:04.889+09:00  INFO 45581 --- [ionShutdownHook] rollerEventManager$ControllerEventThread : [ControllerEventThread controllerId=0] Shutdown completed
2025-04-03T23:43:04.889+09:00  INFO 45581 --- [er-event-thread] rollerEventManager$ControllerEventThread : [ControllerEventThread controllerId=0] Stopped
2025-04-03T23:43:04.889+09:00  INFO 45581 --- [ionShutdownHook] k.controller.ZkPartitionStateMachine     : [PartitionStateMachine controllerId=0] Stopped partition state machine
2025-04-03T23:43:04.889+09:00  INFO 45581 --- [ionShutdownHook] kafka.controller.ZkReplicaStateMachine   : [ReplicaStateMachine controllerId=0] Stopped replica state machine
2025-04-03T23:43:04.890+09:00  INFO 45581 --- [ionShutdownHook] kafka.controller.RequestSendThread       : [RequestSendThread controllerId=0] Shutting down
2025-04-03T23:43:04.890+09:00  INFO 45581 --- [r-0-send-thread] kafka.controller.RequestSendThread       : [RequestSendThread controllerId=0] Stopped
2025-04-03T23:43:04.890+09:00  INFO 45581 --- [ionShutdownHook] kafka.controller.RequestSendThread       : [RequestSendThread controllerId=0] Shutdown completed
2025-04-03T23:43:04.890+09:00  INFO 45581 --- [ionShutdownHook] kafka.controller.KafkaController         : [Controller id=0] Resigned
2025-04-03T23:43:04.890+09:00  INFO 45581 --- [ionShutdownHook] stener$ChangeNotificationProcessorThread : [feature-zk-node-event-process-thread]: Shutting down
2025-04-03T23:43:04.890+09:00  INFO 45581 --- [-process-thread] stener$ChangeNotificationProcessorThread : [feature-zk-node-event-process-thread]: Stopped
2025-04-03T23:43:04.890+09:00  INFO 45581 --- [ionShutdownHook] stener$ChangeNotificationProcessorThread : [feature-zk-node-event-process-thread]: Shutdown completed
2025-04-03T23:43:04.891+09:00  INFO 45581 --- [ionShutdownHook] kafka.zookeeper.ZooKeeperClient          : [ZooKeeperClient Kafka server] Closing.
2025-04-03T23:43:04.997+09:00  INFO 45581 --- [ionShutdownHook] org.apache.zookeeper.ZooKeeper           : Session: 0x100090a96600000 closed
2025-04-03T23:43:04.997+09:00  INFO 45581 --- [ker-EventThread] org.apache.zookeeper.ClientCnxn          : EventThread shut down for session: 0x100090a96600000
2025-04-03T23:43:04.998+09:00  INFO 45581 --- [ionShutdownHook] kafka.zookeeper.ZooKeeperClient          : [ZooKeeperClient Kafka server] Closed.
2025-04-03T23:43:04.998+09:00  INFO 45581 --- [ionShutdownHook] lientQuotaManager$ThrottledChannelReaper : [ThrottledChannelReaper-Fetch]: Shutting down
2025-04-03T23:43:04.999+09:00  INFO 45581 --- [nelReaper-Fetch] lientQuotaManager$ThrottledChannelReaper : [ThrottledChannelReaper-Fetch]: Stopped
2025-04-03T23:43:04.999+09:00  INFO 45581 --- [ionShutdownHook] lientQuotaManager$ThrottledChannelReaper : [ThrottledChannelReaper-Fetch]: Shutdown completed
2025-04-03T23:43:04.999+09:00  INFO 45581 --- [ionShutdownHook] lientQuotaManager$ThrottledChannelReaper : [ThrottledChannelReaper-Produce]: Shutting down
2025-04-03T23:43:04.999+09:00  INFO 45581 --- [lReaper-Produce] lientQuotaManager$ThrottledChannelReaper : [ThrottledChannelReaper-Produce]: Stopped
2025-04-03T23:43:04.999+09:00  INFO 45581 --- [ionShutdownHook] lientQuotaManager$ThrottledChannelReaper : [ThrottledChannelReaper-Produce]: Shutdown completed
2025-04-03T23:43:04.999+09:00  INFO 45581 --- [ionShutdownHook] lientQuotaManager$ThrottledChannelReaper : [ThrottledChannelReaper-Request]: Shutting down
2025-04-03T23:43:04.999+09:00  INFO 45581 --- [lReaper-Request] lientQuotaManager$ThrottledChannelReaper : [ThrottledChannelReaper-Request]: Stopped
2025-04-03T23:43:04.999+09:00  INFO 45581 --- [ionShutdownHook] lientQuotaManager$ThrottledChannelReaper : [ThrottledChannelReaper-Request]: Shutdown completed
2025-04-03T23:43:04.999+09:00  INFO 45581 --- [ionShutdownHook] lientQuotaManager$ThrottledChannelReaper : [ThrottledChannelReaper-ControllerMutation]: Shutting down
2025-04-03T23:43:04.999+09:00  INFO 45581 --- [trollerMutation] lientQuotaManager$ThrottledChannelReaper : [ThrottledChannelReaper-ControllerMutation]: Stopped
2025-04-03T23:43:04.999+09:00  INFO 45581 --- [ionShutdownHook] lientQuotaManager$ThrottledChannelReaper : [ThrottledChannelReaper-ControllerMutation]: Shutdown completed
2025-04-03T23:43:04.999+09:00  INFO 45581 --- [ionShutdownHook] kafka.network.SocketServer               : [SocketServer listenerType=ZK_BROKER, nodeId=0] Shutting down socket server
2025-04-03T23:43:05.004+09:00  INFO 45581 --- [ionShutdownHook] kafka.network.SocketServer               : [SocketServer listenerType=ZK_BROKER, nodeId=0] Shutdown completed
2025-04-03T23:43:05.004+09:00  INFO 45581 --- [ionShutdownHook] o.apache.kafka.common.metrics.Metrics    : Metrics scheduler closed
2025-04-03T23:43:05.004+09:00  INFO 45581 --- [ionShutdownHook] o.apache.kafka.common.metrics.Metrics    : Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-04-03T23:43:05.004+09:00  INFO 45581 --- [ionShutdownHook] o.apache.kafka.common.metrics.Metrics    : Metrics reporters closed
2025-04-03T23:43:05.005+09:00  INFO 45581 --- [ionShutdownHook] kafka.server.BrokerTopicStats            : Broker and topic stats closed
2025-04-03T23:43:05.005+09:00  INFO 45581 --- [ionShutdownHook] o.a.kafka.common.utils.AppInfoParser     : App info kafka.server for 0 unregistered
2025-04-03T23:43:05.005+09:00  INFO 45581 --- [ionShutdownHook] kafka.server.KafkaServer                 : [KafkaServer id=0] shut down completed
2025-04-03T23:43:05.020+09:00  INFO 45581 --- [nnectionExpirer] o.a.z.server.NIOServerCnxnFactory        : ConnnectionExpirerThread interrupted
2025-04-03T23:43:05.021+09:00  INFO 45581 --- [ad:/127.0.0.1:0] o.a.z.server.NIOServerCnxnFactory        : accept thread exitted run method
2025-04-03T23:43:05.021+09:00  INFO 45581 --- [electorThread-1] o.a.z.server.NIOServerCnxnFactory        : selector thread exitted run method
2025-04-03T23:43:05.021+09:00  INFO 45581 --- [electorThread-0] o.a.z.server.NIOServerCnxnFactory        : selector thread exitted run method
2025-04-03T23:43:05.021+09:00  INFO 45581 --- [ionShutdownHook] o.a.zookeeper.server.ZooKeeperServer     : shutting down
2025-04-03T23:43:05.021+09:00  INFO 45581 --- [ionShutdownHook] o.a.zookeeper.server.RequestThrottler    : Shutting down
2025-04-03T23:43:05.021+09:00  INFO 45581 --- [equestThrottler] o.a.zookeeper.server.RequestThrottler    : Draining request throttler queue
2025-04-03T23:43:05.021+09:00  INFO 45581 --- [equestThrottler] o.a.zookeeper.server.RequestThrottler    : RequestThrottler shutdown. Dropped 0 requests
2025-04-03T23:43:05.021+09:00  INFO 45581 --- [ionShutdownHook] o.a.zookeeper.server.SessionTrackerImpl  : Shutting down
2025-04-03T23:43:05.021+09:00  INFO 45581 --- [ionShutdownHook] o.a.z.server.PrepRequestProcessor        : Shutting down
2025-04-03T23:43:05.021+09:00  INFO 45581 --- [ionShutdownHook] o.a.z.server.SyncRequestProcessor        : Shutting down
2025-04-03T23:43:05.021+09:00  INFO 45581 --- [0 cport:58604):] o.a.z.server.PrepRequestProcessor        : PrepRequestProcessor exited loop!
2025-04-03T23:43:05.021+09:00  INFO 45581 --- [   SyncThread:0] o.a.z.server.SyncRequestProcessor        : SyncRequestProcessor exited!
2025-04-03T23:43:05.022+09:00  INFO 45581 --- [ionShutdownHook] o.a.z.server.FinalRequestProcessor       : shutdown of request processor complete
2025-04-03T23:43:05.026+09:00  INFO 45581 --- [ntainer#0-0-C-1] k.c.c.i.ConsumerRebalanceListenerInvoker : [Consumer clientId=consumer-test-my-group-3, groupId=test-my-group] Revoke previously assigned partitions test-my-topic-0
2025-04-03T23:43:05.026+09:00  INFO 45581 --- [ntainer#0-0-C-1] o.s.k.l.KafkaMessageListenerContainer    : test-my-group: partitions revoked: [test-my-topic-0]
2025-04-03T23:43:05.026+09:00  INFO 45581 --- [ntainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-test-my-group-3, groupId=test-my-group] Member consumer-test-my-group-3-e742400f-7734-4921-84e4-ef1925098c88 sending LeaveGroup request to coordinator localhost:58617 (id: 2147483647 rack: null) due to the consumer unsubscribed from all topics
2025-04-03T23:43:05.027+09:00  INFO 45581 --- [ntainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-test-my-group-3, groupId=test-my-group] Resetting generation and member id due to: consumer pro-actively leaving the group
2025-04-03T23:43:05.027+09:00  INFO 45581 --- [ntainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-test-my-group-3, groupId=test-my-group] Request joining group due to: consumer pro-actively leaving the group
2025-04-03T23:43:05.027+09:00  INFO 45581 --- [ntainer#0-0-C-1] o.a.k.c.c.internals.LegacyKafkaConsumer  : [Consumer clientId=consumer-test-my-group-3, groupId=test-my-group] Unsubscribed all topics or patterns and assigned partitions
2025-04-03T23:43:05.027+09:00  INFO 45581 --- [ntainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-test-my-group-3, groupId=test-my-group] Resetting generation and member id due to: consumer pro-actively leaving the group
2025-04-03T23:43:05.027+09:00  INFO 45581 --- [ntainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-test-my-group-3, groupId=test-my-group] Request joining group due to: consumer pro-actively leaving the group
2025-04-03T23:43:05.027+09:00  INFO 45581 --- [quest-handler-5] k.coordinator.group.GroupCoordinator     : [GroupCoordinator 0]: Preparing to rebalance group test-my-group in state PreparingRebalance with old generation 1 (__consumer_offsets-3) (reason: Removing member consumer-test-my-group-3-e742400f-7734-4921-84e4-ef1925098c88 on LeaveGroup; client reason: the consumer unsubscribed from all topics)
2025-04-03T23:43:05.027+09:00  INFO 45581 --- [quest-handler-5] k.coordinator.group.GroupCoordinator     : [GroupCoordinator 0]: Group test-my-group with generation 2 is now empty (__consumer_offsets-3)
2025-04-03T23:43:05.027+09:00  INFO 45581 --- [quest-handler-5] k.coordinator.group.GroupCoordinator     : [GroupCoordinator 0]: Member MemberMetadata(memberId=consumer-test-my-group-3-e742400f-7734-4921-84e4-ef1925098c88, groupInstanceId=None, clientId=consumer-test-my-group-3, clientHost=/127.0.0.1, sessionTimeoutMs=45000, rebalanceTimeoutMs=300000, supportedProtocols=List(range, cooperative-sticky)) has left group test-my-group through explicit `LeaveGroup`; client reason: the consumer unsubscribed from all topics
2025-04-03T23:43:05.121+09:00  INFO 45581 --- [ntainer#0-0-C-1] o.apache.kafka.common.metrics.Metrics    : Metrics scheduler closed
2025-04-03T23:43:05.121+09:00  INFO 45581 --- [ntainer#0-0-C-1] o.apache.kafka.common.metrics.Metrics    : Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-04-03T23:43:05.121+09:00  INFO 45581 --- [ntainer#0-0-C-1] o.apache.kafka.common.metrics.Metrics    : Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter
2025-04-03T23:43:05.121+09:00  INFO 45581 --- [ntainer#0-0-C-1] o.apache.kafka.common.metrics.Metrics    : Metrics reporters closed
2025-04-03T23:43:05.122+09:00  INFO 45581 --- [ntainer#0-0-C-1] o.a.kafka.common.utils.AppInfoParser     : App info kafka.consumer for consumer-test-my-group-3 unregistered
2025-04-03T23:43:05.122+09:00  INFO 45581 --- [ntainer#0-0-C-1] o.s.k.l.KafkaMessageListenerContainer    : test-my-group: Consumer stopped
2025-04-03T23:43:05.122+09:00  INFO 45581 --- [ionShutdownHook] o.s.b.w.e.tomcat.GracefulShutdown        : Commencing graceful shutdown. Waiting for active requests to complete
2025-04-03T23:43:05.124+09:00  INFO 45581 --- [tomcat-shutdown] o.s.b.w.e.tomcat.GracefulShutdown        : Graceful shutdown complete
2025-04-03T23:43:05.125+09:00  INFO 45581 --- [ionShutdownHook] o.a.k.clients.producer.KafkaProducer     : [Producer clientId=producer-2] Closing the Kafka producer with timeoutMillis = 30000 ms.
2025-04-03T23:43:05.126+09:00  INFO 45581 --- [ionShutdownHook] o.apache.kafka.common.metrics.Metrics    : Metrics scheduler closed
2025-04-03T23:43:05.126+09:00  INFO 45581 --- [ionShutdownHook] o.apache.kafka.common.metrics.Metrics    : Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-04-03T23:43:05.126+09:00  INFO 45581 --- [ionShutdownHook] o.apache.kafka.common.metrics.Metrics    : Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter
2025-04-03T23:43:05.126+09:00  INFO 45581 --- [ionShutdownHook] o.apache.kafka.common.metrics.Metrics    : Metrics reporters closed
2025-04-03T23:43:05.126+09:00  INFO 45581 --- [ionShutdownHook] o.a.kafka.common.utils.AppInfoParser     : App info kafka.producer for producer-2 unregistered
2025-04-03T23:43:05.126+09:00  INFO 45581 --- [ionShutdownHook] kafka.server.KafkaServer                 : [KafkaServer id=0] shutting down
2025-04-03T23:43:05.126+09:00  INFO 45581 --- [ionShutdownHook] icationListener$ChangeEventProcessThread : [/config/changes-event-process-thread]: Shutting down
2025-04-03T23:43:05.126+09:00  INFO 45581 --- [-process-thread] icationListener$ChangeEventProcessThread : [/config/changes-event-process-thread]: Stopped
2025-04-03T23:43:05.126+09:00  INFO 45581 --- [ionShutdownHook] icationListener$ChangeEventProcessThread : [/config/changes-event-process-thread]: Shutdown completed
2025-04-03T23:43:05.126+09:00  INFO 45581 --- [ionShutdownHook] kafka.network.SocketServer               : [SocketServer listenerType=ZK_BROKER, nodeId=0] Stopping socket server request processors
2025-04-03T23:43:05.127+09:00  INFO 45581 --- [channel-manager] org.apache.kafka.clients.NetworkClient   : [NodeToControllerChannelManager id=0 name=forwarding] Node 0 disconnected.
2025-04-03T23:43:05.127+09:00  INFO 45581 --- [ionShutdownHook] kafka.network.SocketServer               : [SocketServer listenerType=ZK_BROKER, nodeId=0] Stopped socket server request processors
2025-04-03T23:43:05.127+09:00  INFO 45581 --- [ionShutdownHook] kafka.server.KafkaRequestHandlerPool     : [data-plane Kafka Request Handler on Broker 0], shutting down
2025-04-03T23:43:05.127+09:00  INFO 45581 --- [ionShutdownHook] kafka.server.KafkaRequestHandlerPool     : [data-plane Kafka Request Handler on Broker 0], shut down completely
2025-04-03T23:43:05.127+09:00  INFO 45581 --- [ionShutdownHook] perationPurgatory$ExpiredOperationReaper : [ExpirationReaper-0-AlterAcls]: Shutting down
2025-04-03T23:43:05.128+09:00  INFO 45581 --- [per-0-AlterAcls] perationPurgatory$ExpiredOperationReaper : [ExpirationReaper-0-AlterAcls]: Stopped
2025-04-03T23:43:05.128+09:00  INFO 45581 --- [ionShutdownHook] perationPurgatory$ExpiredOperationReaper : [ExpirationReaper-0-AlterAcls]: Shutdown completed
2025-04-03T23:43:05.128+09:00  INFO 45581 --- [ionShutdownHook] kafka.server.KafkaApis                   : [KafkaApi-0] Shutdown complete.
2025-04-03T23:43:05.128+09:00  INFO 45581 --- [ionShutdownHook] perationPurgatory$ExpiredOperationReaper : [ExpirationReaper-0-topic]: Shutting down
2025-04-03T23:43:05.128+09:00  INFO 45581 --- [nReaper-0-topic] perationPurgatory$ExpiredOperationReaper : [ExpirationReaper-0-topic]: Stopped
2025-04-03T23:43:05.128+09:00  INFO 45581 --- [ionShutdownHook] perationPurgatory$ExpiredOperationReaper : [ExpirationReaper-0-topic]: Shutdown completed
2025-04-03T23:43:05.128+09:00  INFO 45581 --- [ionShutdownHook] k.c.transaction.TransactionCoordinator   : [TransactionCoordinator id=0] Shutting down.
2025-04-03T23:43:05.128+09:00  INFO 45581 --- [ionShutdownHook] k.c.transaction.TransactionStateManager  : [Transaction State Manager 0]: Shutdown complete
2025-04-03T23:43:05.128+09:00  INFO 45581 --- [ionShutdownHook] k.c.t.TransactionMarkerChannelManager    : [TxnMarkerSenderThread-0]: Shutting down
2025-04-03T23:43:05.128+09:00  INFO 45581 --- [rSenderThread-0] k.c.t.TransactionMarkerChannelManager    : [TxnMarkerSenderThread-0]: Stopped
2025-04-03T23:43:05.128+09:00  INFO 45581 --- [ionShutdownHook] k.c.t.TransactionMarkerChannelManager    : [TxnMarkerSenderThread-0]: Shutdown completed
2025-04-03T23:43:05.128+09:00  INFO 45581 --- [ionShutdownHook] k.c.transaction.TransactionCoordinator   : [TransactionCoordinator id=0] Shutdown complete.
2025-04-03T23:43:05.128+09:00  INFO 45581 --- [ionShutdownHook] k.coordinator.group.GroupCoordinator     : [GroupCoordinator 0]: Shutting down.
2025-04-03T23:43:05.128+09:00  INFO 45581 --- [ionShutdownHook] perationPurgatory$ExpiredOperationReaper : [ExpirationReaper-0-Heartbeat]: Shutting down
2025-04-03T23:43:05.128+09:00  INFO 45581 --- [per-0-Heartbeat] perationPurgatory$ExpiredOperationReaper : [ExpirationReaper-0-Heartbeat]: Stopped
2025-04-03T23:43:05.128+09:00  INFO 45581 --- [ionShutdownHook] perationPurgatory$ExpiredOperationReaper : [ExpirationReaper-0-Heartbeat]: Shutdown completed
2025-04-03T23:43:05.129+09:00  INFO 45581 --- [ionShutdownHook] perationPurgatory$ExpiredOperationReaper : [ExpirationReaper-0-Rebalance]: Shutting down
2025-04-03T23:43:05.129+09:00  INFO 45581 --- [per-0-Rebalance] perationPurgatory$ExpiredOperationReaper : [ExpirationReaper-0-Rebalance]: Stopped
2025-04-03T23:43:05.129+09:00  INFO 45581 --- [ionShutdownHook] perationPurgatory$ExpiredOperationReaper : [ExpirationReaper-0-Rebalance]: Shutdown completed
2025-04-03T23:43:05.129+09:00  INFO 45581 --- [ionShutdownHook] k.coordinator.group.GroupCoordinator     : [GroupCoordinator 0]: Shutdown complete.
2025-04-03T23:43:05.129+09:00  INFO 45581 --- [ionShutdownHook] kafka.server.ReplicaManager              : [ReplicaManager broker=0] Shutting down
2025-04-03T23:43:05.129+09:00  INFO 45581 --- [ionShutdownHook] k.s.ReplicaManager$LogDirFailureHandler  : [LogDirFailureHandler]: Shutting down
2025-04-03T23:43:05.129+09:00  INFO 45581 --- [rFailureHandler] k.s.ReplicaManager$LogDirFailureHandler  : [LogDirFailureHandler]: Stopped
2025-04-03T23:43:05.129+09:00  INFO 45581 --- [ionShutdownHook] k.s.ReplicaManager$LogDirFailureHandler  : [LogDirFailureHandler]: Shutdown completed
2025-04-03T23:43:05.129+09:00  INFO 45581 --- [ionShutdownHook] kafka.server.ReplicaFetcherManager       : [ReplicaFetcherManager on broker 0] shutting down
2025-04-03T23:43:05.129+09:00  INFO 45581 --- [ionShutdownHook] kafka.server.ReplicaFetcherManager       : [ReplicaFetcherManager on broker 0] shutdown completed
2025-04-03T23:43:05.129+09:00  INFO 45581 --- [ionShutdownHook] k.server.ReplicaAlterLogDirsManager      : [ReplicaAlterLogDirsManager on broker 0] shutting down
2025-04-03T23:43:05.129+09:00  INFO 45581 --- [ionShutdownHook] k.server.ReplicaAlterLogDirsManager      : [ReplicaAlterLogDirsManager on broker 0] shutdown completed
2025-04-03T23:43:05.129+09:00  INFO 45581 --- [ionShutdownHook] perationPurgatory$ExpiredOperationReaper : [ExpirationReaper-0-Fetch]: Shutting down
2025-04-03T23:43:05.129+09:00  INFO 45581 --- [nReaper-0-Fetch] perationPurgatory$ExpiredOperationReaper : [ExpirationReaper-0-Fetch]: Stopped
2025-04-03T23:43:05.129+09:00  INFO 45581 --- [ionShutdownHook] perationPurgatory$ExpiredOperationReaper : [ExpirationReaper-0-Fetch]: Shutdown completed
2025-04-03T23:43:05.129+09:00  INFO 45581 --- [ionShutdownHook] perationPurgatory$ExpiredOperationReaper : [ExpirationReaper-0-RemoteFetch]: Shutting down
2025-04-03T23:43:05.129+09:00  INFO 45581 --- [r-0-RemoteFetch] perationPurgatory$ExpiredOperationReaper : [ExpirationReaper-0-RemoteFetch]: Stopped
2025-04-03T23:43:05.129+09:00  INFO 45581 --- [ionShutdownHook] perationPurgatory$ExpiredOperationReaper : [ExpirationReaper-0-RemoteFetch]: Shutdown completed
2025-04-03T23:43:05.129+09:00  INFO 45581 --- [ionShutdownHook] perationPurgatory$ExpiredOperationReaper : [ExpirationReaper-0-Produce]: Shutting down
2025-04-03T23:43:05.129+09:00  INFO 45581 --- [eaper-0-Produce] perationPurgatory$ExpiredOperationReaper : [ExpirationReaper-0-Produce]: Stopped
2025-04-03T23:43:05.129+09:00  INFO 45581 --- [ionShutdownHook] perationPurgatory$ExpiredOperationReaper : [ExpirationReaper-0-Produce]: Shutdown completed
2025-04-03T23:43:05.129+09:00  INFO 45581 --- [ionShutdownHook] perationPurgatory$ExpiredOperationReaper : [ExpirationReaper-0-DeleteRecords]: Shutting down
2025-04-03T23:43:05.129+09:00  INFO 45581 --- [0-DeleteRecords] perationPurgatory$ExpiredOperationReaper : [ExpirationReaper-0-DeleteRecords]: Stopped
2025-04-03T23:43:05.129+09:00  INFO 45581 --- [ionShutdownHook] perationPurgatory$ExpiredOperationReaper : [ExpirationReaper-0-DeleteRecords]: Shutdown completed
2025-04-03T23:43:05.129+09:00  INFO 45581 --- [ionShutdownHook] perationPurgatory$ExpiredOperationReaper : [ExpirationReaper-0-ElectLeader]: Shutting down
2025-04-03T23:43:05.129+09:00  INFO 45581 --- [r-0-ElectLeader] perationPurgatory$ExpiredOperationReaper : [ExpirationReaper-0-ElectLeader]: Stopped
2025-04-03T23:43:05.129+09:00  INFO 45581 --- [ionShutdownHook] perationPurgatory$ExpiredOperationReaper : [ExpirationReaper-0-ElectLeader]: Shutdown completed
2025-04-03T23:43:05.141+09:00  INFO 45581 --- [ionShutdownHook] kafka.server.AddPartitionsToTxnManager   : [AddPartitionsToTxnSenderThread-0]: Shutting down
2025-04-03T23:43:05.141+09:00  INFO 45581 --- [nSenderThread-0] kafka.server.AddPartitionsToTxnManager   : [AddPartitionsToTxnSenderThread-0]: Stopped
2025-04-03T23:43:05.141+09:00  INFO 45581 --- [ionShutdownHook] kafka.server.AddPartitionsToTxnManager   : [AddPartitionsToTxnSenderThread-0]: Shutdown completed
2025-04-03T23:43:05.141+09:00  INFO 45581 --- [ionShutdownHook] kafka.server.ReplicaManager              : [ReplicaManager broker=0] Shut down completely
2025-04-03T23:43:05.141+09:00  INFO 45581 --- [ionShutdownHook] k.server.NodeToControllerRequestThread   : [zk-broker-0-to-controller-alter-partition-channel-manager]: Shutting down
2025-04-03T23:43:05.141+09:00  INFO 45581 --- [channel-manager] k.server.NodeToControllerRequestThread   : [zk-broker-0-to-controller-alter-partition-channel-manager]: Stopped
2025-04-03T23:43:05.141+09:00  INFO 45581 --- [ionShutdownHook] k.server.NodeToControllerRequestThread   : [zk-broker-0-to-controller-alter-partition-channel-manager]: Shutdown completed
2025-04-03T23:43:05.141+09:00  INFO 45581 --- [ionShutdownHook] k.s.NodeToControllerChannelManagerImpl   : Node to controller channel manager for alter-partition shutdown
2025-04-03T23:43:05.141+09:00  INFO 45581 --- [ionShutdownHook] k.server.NodeToControllerRequestThread   : [zk-broker-0-to-controller-forwarding-channel-manager]: Shutting down
2025-04-03T23:43:05.141+09:00  INFO 45581 --- [channel-manager] k.server.NodeToControllerRequestThread   : [zk-broker-0-to-controller-forwarding-channel-manager]: Stopped
2025-04-03T23:43:05.141+09:00  INFO 45581 --- [ionShutdownHook] k.server.NodeToControllerRequestThread   : [zk-broker-0-to-controller-forwarding-channel-manager]: Shutdown completed
2025-04-03T23:43:05.141+09:00  INFO 45581 --- [ionShutdownHook] k.s.NodeToControllerChannelManagerImpl   : Node to controller channel manager for forwarding shutdown
2025-04-03T23:43:05.141+09:00  INFO 45581 --- [ionShutdownHook] kafka.log.LogManager                     : Shutting down.
2025-04-03T23:43:05.141+09:00  INFO 45581 --- [ionShutdownHook] kafka.log.LogCleaner                     : Shutting down the log cleaner.
2025-04-03T23:43:05.141+09:00  INFO 45581 --- [ionShutdownHook] kafka.log.LogCleaner$CleanerThread       : [kafka-log-cleaner-thread-0]: Shutting down
2025-04-03T23:43:05.141+09:00  INFO 45581 --- [leaner-thread-0] kafka.log.LogCleaner$CleanerThread       : [kafka-log-cleaner-thread-0]: Stopped
2025-04-03T23:43:05.141+09:00  INFO 45581 --- [ionShutdownHook] kafka.log.LogCleaner$CleanerThread       : [kafka-log-cleaner-thread-0]: Shutdown completed
2025-04-03T23:43:05.146+09:00  INFO 45581 --- [368033373570203] o.a.k.s.i.log.ProducerStateManager       : [ProducerStateManager partition=__consumer_offsets-3] Wrote producer snapshot at offset 3 with 0 producer ids in 4 ms.
2025-04-03T23:43:05.164+09:00  INFO 45581 --- [368033373570203] o.a.k.s.i.log.ProducerStateManager       : [ProducerStateManager partition=test-my-topic-0] Wrote producer snapshot at offset 1 with 1 producer ids in 4 ms.
2025-04-03T23:43:05.181+09:00  INFO 45581 --- [ionShutdownHook] kafka.log.LogManager                     : Shutdown complete.
2025-04-03T23:43:05.181+09:00  INFO 45581 --- [ionShutdownHook] rollerEventManager$ControllerEventThread : [ControllerEventThread controllerId=0] Shutting down
2025-04-03T23:43:05.181+09:00  INFO 45581 --- [ionShutdownHook] rollerEventManager$ControllerEventThread : [ControllerEventThread controllerId=0] Shutdown completed
2025-04-03T23:43:05.181+09:00  INFO 45581 --- [er-event-thread] rollerEventManager$ControllerEventThread : [ControllerEventThread controllerId=0] Stopped
2025-04-03T23:43:05.181+09:00  INFO 45581 --- [ionShutdownHook] k.controller.ZkPartitionStateMachine     : [PartitionStateMachine controllerId=0] Stopped partition state machine
2025-04-03T23:43:05.181+09:00  INFO 45581 --- [ionShutdownHook] kafka.controller.ZkReplicaStateMachine   : [ReplicaStateMachine controllerId=0] Stopped replica state machine
2025-04-03T23:43:05.181+09:00  INFO 45581 --- [ionShutdownHook] kafka.controller.RequestSendThread       : [RequestSendThread controllerId=0] Shutting down
2025-04-03T23:43:05.181+09:00  INFO 45581 --- [r-0-send-thread] kafka.controller.RequestSendThread       : [RequestSendThread controllerId=0] Stopped
2025-04-03T23:43:05.181+09:00  INFO 45581 --- [ionShutdownHook] kafka.controller.RequestSendThread       : [RequestSendThread controllerId=0] Shutdown completed
2025-04-03T23:43:05.181+09:00  INFO 45581 --- [ionShutdownHook] kafka.controller.KafkaController         : [Controller id=0] Resigned
2025-04-03T23:43:05.182+09:00  INFO 45581 --- [ionShutdownHook] stener$ChangeNotificationProcessorThread : [feature-zk-node-event-process-thread]: Shutting down
2025-04-03T23:43:05.182+09:00  INFO 45581 --- [-process-thread] stener$ChangeNotificationProcessorThread : [feature-zk-node-event-process-thread]: Stopped
2025-04-03T23:43:05.182+09:00  INFO 45581 --- [ionShutdownHook] stener$ChangeNotificationProcessorThread : [feature-zk-node-event-process-thread]: Shutdown completed
2025-04-03T23:43:05.182+09:00  INFO 45581 --- [ionShutdownHook] kafka.zookeeper.ZooKeeperClient          : [ZooKeeperClient Kafka server] Closing.
2025-04-03T23:43:05.288+09:00  INFO 45581 --- [ionShutdownHook] org.apache.zookeeper.ZooKeeper           : Session: 0x100090a9a1d0000 closed
2025-04-03T23:43:05.288+09:00  INFO 45581 --- [ker-EventThread] org.apache.zookeeper.ClientCnxn          : EventThread shut down for session: 0x100090a9a1d0000
2025-04-03T23:43:05.288+09:00  INFO 45581 --- [ionShutdownHook] kafka.zookeeper.ZooKeeperClient          : [ZooKeeperClient Kafka server] Closed.
2025-04-03T23:43:05.288+09:00  INFO 45581 --- [ionShutdownHook] lientQuotaManager$ThrottledChannelReaper : [ThrottledChannelReaper-Fetch]: Shutting down
2025-04-03T23:43:05.288+09:00  INFO 45581 --- [nelReaper-Fetch] lientQuotaManager$ThrottledChannelReaper : [ThrottledChannelReaper-Fetch]: Stopped
2025-04-03T23:43:05.288+09:00  INFO 45581 --- [ionShutdownHook] lientQuotaManager$ThrottledChannelReaper : [ThrottledChannelReaper-Fetch]: Shutdown completed
2025-04-03T23:43:05.288+09:00  INFO 45581 --- [ionShutdownHook] lientQuotaManager$ThrottledChannelReaper : [ThrottledChannelReaper-Produce]: Shutting down
2025-04-03T23:43:05.288+09:00  INFO 45581 --- [lReaper-Produce] lientQuotaManager$ThrottledChannelReaper : [ThrottledChannelReaper-Produce]: Stopped
2025-04-03T23:43:05.288+09:00  INFO 45581 --- [ionShutdownHook] lientQuotaManager$ThrottledChannelReaper : [ThrottledChannelReaper-Produce]: Shutdown completed
2025-04-03T23:43:05.288+09:00  INFO 45581 --- [ionShutdownHook] lientQuotaManager$ThrottledChannelReaper : [ThrottledChannelReaper-Request]: Shutting down
2025-04-03T23:43:05.288+09:00  INFO 45581 --- [lReaper-Request] lientQuotaManager$ThrottledChannelReaper : [ThrottledChannelReaper-Request]: Stopped
2025-04-03T23:43:05.288+09:00  INFO 45581 --- [ionShutdownHook] lientQuotaManager$ThrottledChannelReaper : [ThrottledChannelReaper-Request]: Shutdown completed
2025-04-03T23:43:05.288+09:00  INFO 45581 --- [ionShutdownHook] lientQuotaManager$ThrottledChannelReaper : [ThrottledChannelReaper-ControllerMutation]: Shutting down
2025-04-03T23:43:05.288+09:00  INFO 45581 --- [trollerMutation] lientQuotaManager$ThrottledChannelReaper : [ThrottledChannelReaper-ControllerMutation]: Stopped
2025-04-03T23:43:05.288+09:00  INFO 45581 --- [ionShutdownHook] lientQuotaManager$ThrottledChannelReaper : [ThrottledChannelReaper-ControllerMutation]: Shutdown completed
2025-04-03T23:43:05.288+09:00  INFO 45581 --- [ionShutdownHook] kafka.network.SocketServer               : [SocketServer listenerType=ZK_BROKER, nodeId=0] Shutting down socket server
2025-04-03T23:43:05.292+09:00  INFO 45581 --- [ionShutdownHook] kafka.network.SocketServer               : [SocketServer listenerType=ZK_BROKER, nodeId=0] Shutdown completed
2025-04-03T23:43:05.292+09:00  INFO 45581 --- [ionShutdownHook] o.apache.kafka.common.metrics.Metrics    : Metrics scheduler closed
2025-04-03T23:43:05.292+09:00  INFO 45581 --- [ionShutdownHook] o.apache.kafka.common.metrics.Metrics    : Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-04-03T23:43:05.292+09:00  INFO 45581 --- [ionShutdownHook] o.apache.kafka.common.metrics.Metrics    : Metrics reporters closed
2025-04-03T23:43:05.292+09:00  INFO 45581 --- [ionShutdownHook] kafka.server.BrokerTopicStats            : Broker and topic stats closed
2025-04-03T23:43:05.292+09:00  INFO 45581 --- [ionShutdownHook] o.a.kafka.common.utils.AppInfoParser     : App info kafka.server for 0 unregistered
2025-04-03T23:43:05.292+09:00  INFO 45581 --- [ionShutdownHook] kafka.server.KafkaServer                 : [KafkaServer id=0] shut down completed
2025-04-03T23:43:05.313+09:00  INFO 45581 --- [nnectionExpirer] o.a.z.server.NIOServerCnxnFactory        : ConnnectionExpirerThread interrupted
2025-04-03T23:43:05.313+09:00  INFO 45581 --- [ad:/127.0.0.1:0] o.a.z.server.NIOServerCnxnFactory        : accept thread exitted run method
2025-04-03T23:43:05.313+09:00  INFO 45581 --- [electorThread-1] o.a.z.server.NIOServerCnxnFactory        : selector thread exitted run method
2025-04-03T23:43:05.313+09:00  INFO 45581 --- [electorThread-0] o.a.z.server.NIOServerCnxnFactory        : selector thread exitted run method
2025-04-03T23:43:05.313+09:00  INFO 45581 --- [ionShutdownHook] o.a.zookeeper.server.ZooKeeperServer     : shutting down
2025-04-03T23:43:05.313+09:00  INFO 45581 --- [ionShutdownHook] o.a.zookeeper.server.RequestThrottler    : Shutting down
2025-04-03T23:43:05.313+09:00  INFO 45581 --- [equestThrottler] o.a.zookeeper.server.RequestThrottler    : Draining request throttler queue
2025-04-03T23:43:05.313+09:00  INFO 45581 --- [equestThrottler] o.a.zookeeper.server.RequestThrottler    : RequestThrottler shutdown. Dropped 0 requests
2025-04-03T23:43:05.313+09:00  INFO 45581 --- [ionShutdownHook] o.a.zookeeper.server.SessionTrackerImpl  : Shutting down
2025-04-03T23:43:05.314+09:00  INFO 45581 --- [ionShutdownHook] o.a.z.server.PrepRequestProcessor        : Shutting down
2025-04-03T23:43:05.314+09:00  INFO 45581 --- [ionShutdownHook] o.a.z.server.SyncRequestProcessor        : Shutting down
2025-04-03T23:43:05.314+09:00  INFO 45581 --- [   SyncThread:0] o.a.z.server.SyncRequestProcessor        : SyncRequestProcessor exited!
2025-04-03T23:43:05.314+09:00  INFO 45581 --- [ionShutdownHook] o.a.z.server.FinalRequestProcessor       : shutdown of request processor complete
2025-04-03T23:43:05.314+09:00  INFO 45581 --- [0 cport:58615):] o.a.z.server.PrepRequestProcessor        : PrepRequestProcessor exited loop!
> Task :test
BUILD SUCCESSFUL in 4s
5 actionable tasks: 2 executed, 3 up-to-date
23:43:05: Execution finished ':test'.
